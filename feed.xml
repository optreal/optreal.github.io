<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://optreal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://optreal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-21T11:46:23+00:00</updated><id>https://optreal.github.io/feed.xml</id><title type="html">Optreal</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Batch RL vs. Offline RL</title><link href="https://optreal.github.io/blog/2025/batch_RL_vs_offline_RL/" rel="alternate" type="text/html" title="Batch RL vs. Offline RL"/><published>2025-01-30T00:00:00+00:00</published><updated>2025-01-30T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/batch_RL_vs_offline_RL</id><content type="html" xml:base="https://optreal.github.io/blog/2025/batch_RL_vs_offline_RL/"><![CDATA[<h3 id="batch-reinforcement-learning-vs-offline-reinforcement-learning"><strong>Batch Reinforcement Learning vs. Offline Reinforcement Learning</strong></h3> <p>Both <strong>Batch Reinforcement Learning</strong> and <strong>Offline Reinforcement Learning (Offline RL)</strong> involve training an RL agent using pre-collected experience rather than interacting with the environment in real time. However, they differ in scope and methodology.</p> <hr/> <h3 id="1ï¸âƒ£-batch-reinforcement-learning"><strong>1ï¸âƒ£ Batch Reinforcement Learning</strong></h3> <h4 id="-definition">ğŸ“Œ <strong>Definition</strong></h4> <ul> <li>Batch RL refers to <strong>training an RL agent from a fixed batch of data</strong> without additional data collection.</li> <li>The dataset consists of <strong>state-action-reward-next state</strong> tuples collected from past interactions.</li> </ul> <h4 id="-key-characteristics">ğŸ“Œ <strong>Key Characteristics</strong></h4> <ul> <li><strong>Single fixed dataset</strong>: The agent learns only from <strong>limited</strong> data.</li> <li><strong>No environment interaction</strong>: No further exploration is allowed during training.</li> <li><strong>Challenges</strong>: <ul> <li><strong>Extrapolation error</strong>: The agent may <strong>misestimate Q-values</strong> for unseen actions.</li> <li><strong>Distribution shift</strong>: Learned policies may propose actions <strong>outside the dataset</strong>, leading to unreliable performance.</li> </ul> </li> </ul> <h4 id="-example-algorithms">ğŸ“Œ <strong>Example Algorithms</strong></h4> <ul> <li><strong>Batch-Constrained RL (BCQ)</strong>: Restricts the agent to remain close to the observed data distribution.</li> <li><strong>Bootstrapping Error Accumulation Reduction (BEAR)</strong>: Limits policy divergence from the batch data.</li> <li><strong>Conservative Q-Learning (CQL)</strong>: Regularizes Q-values to prevent overestimation of unseen actions.</li> </ul> <hr/> <h3 id="2ï¸âƒ£-offline-reinforcement-learning"><strong>2ï¸âƒ£ Offline Reinforcement Learning</strong></h3> <h4 id="-definition-1">ğŸ“Œ <strong>Definition</strong></h4> <ul> <li>Offline RL is a <strong>more general</strong> framework that includes Batch RL but <strong>extends beyond it</strong>.</li> <li>It aims to train RL agents <strong>entirely from pre-collected datasets</strong>, often from <strong>various sources</strong>.</li> </ul> <h4 id="-key-characteristics-1">ğŸ“Œ <strong>Key Characteristics</strong></h4> <ul> <li><strong>Diverse datasets</strong>: Offline RL assumes access to <strong>large-scale, heterogeneous datasets</strong> collected from different policies.</li> <li><strong>Generalization beyond training data</strong>: Unlike Batch RL, Offline RL aims to <strong>infer optimal policies</strong> even for unseen states.</li> <li><strong>Common use cases</strong>: <ul> <li>Learning from <strong>human demonstrations</strong> (e.g., robotics, healthcare)</li> <li>Training from <strong>logged data</strong> in recommendation systems, finance</li> <li>Leveraging past <strong>simulation data</strong> for decision-making</li> </ul> </li> </ul> <h4 id="-example-algorithms-1">ğŸ“Œ <strong>Example Algorithms</strong></h4> <ul> <li><strong>Implicit Q-Learning (IQL)</strong>: Avoids explicit policy estimation and focuses on value-based learning.</li> <li><strong>Advantage-Weighted Regression (AWR)</strong>: Uses advantage weighting to update policies from offline data.</li> <li><strong>Decision Transformer (DT)</strong>: Uses sequence modeling (Transformers) for RL without explicit value function learning.</li> </ul> <hr/> <h3 id="-key-differences-between-batch-rl-and-offline-rl"><strong>ğŸ” Key Differences Between Batch RL and Offline RL</strong></h3> <style>table{width:100%;border-collapse:collapse}th,td{border:1px solid #ddd;padding:8px;text-align:center}th{background-color:#f4f4f4}</style> <table> <thead> <tr> <th>Feature</th> <th><strong>Batch RL</strong></th> <th><strong>Offline RL</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Definition</strong></td> <td>Learning from a <strong>single</strong> fixed dataset</td> <td>Learning from <strong>large-scale, diverse</strong> datasets</td> </tr> <tr> <td><strong>Dataset Scope</strong></td> <td>Limited, usually from <strong>one policy</strong></td> <td>Data from <strong>multiple sources or policies</strong></td> </tr> <tr> <td><strong>Goal</strong></td> <td>Learn optimal policy <strong>constrained by batch data</strong></td> <td><strong>Generalize beyond</strong> training data</td> </tr> <tr> <td><strong>Challenges</strong></td> <td>Extrapolation error, distributional shift</td> <td>More complex distribution shift issues</td> </tr> <tr> <td><strong>Algorithms</strong></td> <td>BCQ, BEAR, CQL</td> <td>IQL, AWR, Decision Transformer</td> </tr> </tbody> </table> <hr/> <h3 id="-summary"><strong>ğŸ”¹ Summary</strong></h3> <p>âœ… <strong>Batch RL</strong> is a subset of <strong>Offline RL</strong>, focusing on a <strong>single batch of data</strong> with constraints to prevent extrapolation errors.<br/> âœ… <strong>Offline RL</strong> is broader, leveraging <strong>diverse datasets</strong> to <strong>generalize beyond the given data</strong>.<br/> âœ… <strong>Both methods avoid online interactions</strong>, but Offline RL aims to <strong>develop policies that work well in unseen scenarios</strong>.</p> <hr/> <h3 id="-when-to-use-each"><strong>ğŸ’¡ When to Use Each?</strong></h3> <ul> <li><strong>Use Batch RL</strong> if working with <strong>a single, limited dataset</strong> (e.g., training a robot from one past log).</li> <li><strong>Use Offline RL</strong> if leveraging <strong>large, diverse datasets</strong> (e.g., training a recommendation system from multiple user interactions).</li> </ul>]]></content><author><name></name></author><category term="RL"/><category term="Imitation"/><category term="Learning,"/><category term="Offline"/><category term="Learning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Batch RL vs. Offline RL]]></summary></entry><entry><title type="html">Offline RL vs. Offline IL</title><link href="https://optreal.github.io/blog/2025/offline_RL_IL/" rel="alternate" type="text/html" title="Offline RL vs. Offline IL"/><published>2025-01-29T00:00:00+00:00</published><updated>2025-01-29T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/offline_RL_IL</id><content type="html" xml:base="https://optreal.github.io/blog/2025/offline_RL_IL/"><![CDATA[<h3 id="0-offline-reinforcement-learning-offline-rl-vs-offline-imitation-learning-offline-il"><strong>0. Offline Reinforcement Learning (Offline RL) vs. Offline Imitation Learning (Offline IL)</strong></h3> <p><strong>Offline RL</strong>ê³¼ <strong>Offline IL</strong>ì€ ëª¨ë‘ <strong>í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© ì—†ì´, ì‚¬ì „ì— ìˆ˜ì§‘ëœ ì •ì ì¸ ë°ì´í„°ë§Œì„ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê¸°ë²•</strong>ì´ë‹¤.<br/> ê·¸ëŸ¬ë‚˜, ë‘ ì ‘ê·¼ë²•ì€ <strong>í•™ìŠµ ëª©í‘œ, ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš© ì—¬ë¶€, ë°ì´í„° í™œìš© ë°©ì‹ì—ì„œ ì°¨ì´ì </strong>ì´ ì¡´ì¬í•œë‹¤.</p> <hr/> <h3 id="1-ê°œë…-ì •ì˜-definition"><strong>1. ê°œë… ì •ì˜ (Definition)</strong></h3> <table> <thead> <tr> <th>í•™ìŠµ ë°©ë²•</th> <th>ì •ì˜</th> </tr> </thead> <tbody> <tr> <td><strong>Offline RL</strong></td> <td><strong>ë³´ìƒ í•¨ìˆ˜(Reward Function)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ê°•í™” í•™ìŠµ(RL) ë°©ë²•</strong>.</td> </tr> <tr> <td><strong>Offline IL</strong></td> <td><strong>ì „ë¬¸ê°€ ì‹œì—°(Expert Demonstrations)ì„ ë³µì œí•˜ì—¬ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ëª¨ë°© í•™ìŠµ(IL) ë°©ë²•</strong>.</td> </tr> </tbody> </table> <p>âœ… <strong>í•µì‹¬ ì°¨ì´ì </strong>:</p> <ul> <li><strong>Offline RL</strong>ì€ <strong>ë³´ìƒ í•¨ìˆ˜(Reward Function)</strong>ë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµ.</li> <li><strong>Offline IL</strong>ì€ <strong>ì „ë¬¸ê°€ì˜ í–‰ë™(Demonstrations)</strong>ì„ ê·¸ëŒ€ë¡œ ëª¨ë°©í•˜ì—¬ ì •ì±…ì„ í•™ìŠµ.</li> </ul> <hr/> <h3 id="2-offline-rl-vs-offline-il-ë¹„êµ"><strong>2. Offline RL vs. Offline IL ë¹„êµ</strong></h3> <table> <thead> <tr> <th>ë¹„êµ í•­ëª©</th> <th><strong>Offline RL</strong></th> <th><strong>Offline IL</strong></th> </tr> </thead> <tbody> <tr> <td><strong>í•™ìŠµ ëª©í‘œ (Objective)</strong></td> <td>ìµœì ì˜ ë³´ìƒ(Reward)ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì •ì±… í•™ìŠµ</td> <td>ì „ë¬¸ê°€ í–‰ë™ì„ ëª¨ë°©í•˜ëŠ” ì •ì±… í•™ìŠµ</td> </tr> <tr> <td><strong>ë³´ìƒ í•¨ìˆ˜ (Reward Function)</strong></td> <td>âœ… í•„ìš”í•¨ (í™˜ê²½ ë‚´ì—ì„œ ë³´ìƒ ì œê³µ)</td> <td>âŒ ë¶ˆí•„ìš”í•¨ (ë³´ìƒ ì—†ì´ ì „ë¬¸ê°€ í–‰ë™ì„ ë³µì œ)</td> </tr> <tr> <td><strong>ë°ì´í„° ì¶œì²˜ (Data Source)</strong></td> <td>ê³¼ê±°ì˜ ë‹¤ì–‘í•œ í–‰ë™ ë°ì´í„° (ì „ë¬¸ê°€ + ë¹„ì „ë¬¸ê°€)</td> <td>ì „ë¬¸ê°€ ì‹œì—° ë°ì´í„°ë§Œ í™œìš©</td> </tr> <tr> <td><strong>í•™ìŠµ ë°©ì‹ (Learning Approach)</strong></td> <td>ê°•í™” í•™ìŠµ (RL) ê¸°ë°˜</td> <td>ì§€ë„ í•™ìŠµ (Supervised Learning) ê¸°ë°˜</td> </tr> <tr> <td><strong>ì •ì±…ì˜ ì¼ë°˜í™” (Generalization)</strong></td> <td>ìƒˆë¡œìš´ ìƒí™©ì—ì„œë„ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµ ê°€ëŠ¥</td> <td>ì „ë¬¸ê°€ê°€ ë°©ë¬¸í•œ ìƒíƒœì—ì„œë§Œ ì¼ë°˜í™” ê°€ëŠ¥</td> </tr> <tr> <td><strong>í™œìš© ì‚¬ë¡€ (Applications)</strong></td> <td>ììœ¨ì£¼í–‰, ë¡œë´‡ ì œì–´, ê¸ˆìœµ AI</td> <td>ì˜ë£Œ AI, ììœ¨ì£¼í–‰, ë¡œë´‡ ëª¨ë°© í•™ìŠµ</td> </tr> </tbody> </table> <p>âœ… <strong>Offline RLì€ ë³´ìƒ í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ â€œìµœì  í–‰ë™â€ì„ í•™ìŠµí•˜ëŠ” ë°˜ë©´, Offline ILì€ â€œì „ë¬¸ê°€ í–‰ë™ì„ ê·¸ëŒ€ë¡œ ë³µì œâ€í•˜ëŠ” ë°©ì‹</strong>.</p> <hr/> <h3 id="3-offline-rlê³¼-offline-ilì˜-ì¥ë‹¨ì -pros--cons"><strong>3. Offline RLê³¼ Offline ILì˜ ì¥ë‹¨ì  (Pros &amp; Cons)</strong></h3> <table> <thead> <tr> <th><strong>ë¹„êµ í•­ëª©</strong></th> <th><strong>Offline RL</strong></th> <th><strong>Offline IL</strong></th> </tr> </thead> <tbody> <tr> <td><strong>âœ… ì¥ì </strong></td> <td>- ë³´ìƒ ê¸°ë°˜ í•™ìŠµì´ë¯€ë¡œ ìƒˆë¡œìš´ í™˜ê²½ì—ì„œë„ ì¼ë°˜í™” ê°€ëŠ¥ <br/> - ì „ë¬¸ê°€ ë°ì´í„° ì—†ì´ë„ í•™ìŠµ ê°€ëŠ¥</td> <td>- ë³´ìƒì´ í•„ìš” ì—†ìœ¼ë©°, ì‰½ê²Œ í•™ìŠµ ê°€ëŠ¥ <br/> - í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© ì—†ì´ ì•ˆì „í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥</td> </tr> <tr> <td><strong>âš  ë‹¨ì </strong></td> <td>- ë³´ìƒ ì„¤ê³„ê°€ ì–´ë ¤ì›€ (Sparse Reward ë¬¸ì œ) <br/> - í–‰ë™ í’ˆì§ˆì´ ë‚®ì€ ë°ì´í„°ê°€ í¬í•¨ë  ê²½ìš° í•™ìŠµì´ ì–´ë ¤ì›€</td> <td>- ì „ë¬¸ê°€ ì‹œì—° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í•™ìŠµ ë¶ˆê°€ëŠ¥ <br/> - ê³µë³€ëŸ‰ ì´ë™(Covariate Shift) ë¬¸ì œ ë°œìƒ</td> </tr> </tbody> </table> <p>âœ… <strong>Offline RLì€ ë³´ìƒì„ ìµœì í™”í•  ìˆ˜ ìˆì§€ë§Œ, ë³´ìƒ ì„¤ê³„ê°€ ì–´ë ¤ìš´ ë°˜ë©´, Offline ILì€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš© ì—†ì´ ì „ë¬¸ê°€ í–‰ë™ì„ ì‰½ê²Œ ë³µì œí•  ìˆ˜ ìˆìŒ</strong>.</p> <hr/> <h3 id="4-ì—°êµ¬-ë°©í–¥-ë°-ê²°ë¡ "><strong>4. ì—°êµ¬ ë°©í–¥ ë° ê²°ë¡ </strong></h3> <ul> <li><strong>Offline RL</strong>ì€ <strong>ë³´ìƒ ê¸°ë°˜ ìµœì  í–‰ë™ í•™ìŠµ</strong>ì— ê°•ì ì„ ê°€ì§€ì§€ë§Œ, <strong>ë³´ìƒ ì„¤ê³„ ë¬¸ì œ ë° ìƒ˜í”Œ íš¨ìœ¨ì„± ë¬¸ì œ í•´ê²°ì´ í•„ìš”</strong>.</li> <li><strong>Offline IL</strong>ì€ <strong>ì „ë¬¸ê°€ ë°ì´í„°ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥</strong>í•˜ì§€ë§Œ, <strong>ê³µë³€ëŸ‰ ì´ë™ ë¬¸ì œ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ í•„ìš”</strong>.</li> <li><strong>ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” Offline RLê³¼ Offline ILì„ ê²°í•©í•˜ì—¬, ì „ë¬¸ê°€ ì‹œì—°ì„ í™œìš©í•œ RL í•™ìŠµ(IL-RL Hybrid Approaches)ì´ ì§„í–‰ë¨</strong>.</li> </ul> <p>âœ… <strong>Offline RL vs. Offline ILì˜ ì„ íƒì€ â€œë³´ìƒ í•¨ìˆ˜ê°€ ì£¼ì–´ì§€ëŠ”ì§€ ì—¬ë¶€â€ì™€ â€œì¼ë°˜í™” ì„±ëŠ¥ ìš”êµ¬ ìˆ˜ì¤€â€ì— ë”°ë¼ ê²°ì •ë¨</strong>.<br/> âœ… <strong>ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ë‘ ë°©ë²•ì„ ì¡°í•©í•˜ì—¬ í™œìš©í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ</strong>.</p>]]></content><author><name></name></author><category term="RL"/><category term="Imitation"/><category term="Learning,"/><category term="Offline"/><category term="Learning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Offline RL vs. Offline IL]]></summary></entry><entry><title type="html">Imitation Learning (ëª¨ë°© í•™ìŠµ)</title><link href="https://optreal.github.io/blog/2025/imitation_learning/" rel="alternate" type="text/html" title="Imitation Learning (ëª¨ë°© í•™ìŠµ)"/><published>2025-01-28T00:00:00+00:00</published><updated>2025-01-28T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/imitation_learning</id><content type="html" xml:base="https://optreal.github.io/blog/2025/imitation_learning/"><![CDATA[<h3 id="-0-imitation-learning-ì¢…ë¥˜"><strong>ğŸ“Œ 0. Imitation Learning ì¢…ë¥˜</strong></h3> <ul> <li>Behavioral Cloning</li> <li>Inverse Reinforcement Learning</li> <li>Adversarial Imitation Learning</li> <li>Imitation from Observation)**</li> </ul> <hr/> <h3 id="-1-behavioral-cloning-bc"><strong>ğŸ“Œ 1. Behavioral Cloning (BC)</strong></h3> <p>BC(í–‰ë™ ë³µì œ)ëŠ” <strong>ê°ë… í•™ìŠµ(Supervised Learning) ê¸°ë°˜ì˜ ëª¨ë°© í•™ìŠµ(IL) ë°©ë²•</strong>ìœ¼ë¡œ, ì „ë¬¸ê°€ì˜ ì‹œì—°ì„ ì§ì ‘ í•™ìŠµí•˜ì—¬ <strong>ìƒíƒœ(State) â†’ í–‰ë™(Action) ë§¤í•‘ì„ í•™ìŠµ</strong>í•˜ëŠ” ë°©ì‹.</p> <h4 id="11-bcì˜-ê°œë…-ë°-íŠ¹ì§•"><strong>1.1 BCì˜ ê°œë… ë° íŠ¹ì§•</strong></h4> <ul> <li><strong>ì „ë¬¸ê°€ì˜ ì‹œì—° ë°ì´í„°(ìƒíƒœ-í–‰ë™ ìŒ)ë¥¼ ì§ì ‘ í•™ìŠµ</strong>í•˜ì—¬ ì •ì±…ì„ êµ¬ì„±.</li> <li><strong>í™˜ê²½ê³¼ì˜ ì¶”ê°€ì ì¸ ìƒí˜¸ì‘ìš© ì—†ì´ ì „ë¬¸ê°€ì˜ ì •ì±…ì„ ë³µì œí•  ìˆ˜ ìˆìŒ</strong>.</li> <li><strong>ì»´í“¨íŒ… íš¨ìœ¨ì„±ì´ ë†’ê³ , í•™ìŠµì´ ë¹ ë¦„</strong>.</li> <li><strong>í™˜ê²½ì˜ ë™ì—­í•™(Dynamics)ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ</strong>.</li> </ul> <p>âœ… <strong>ì¥ì </strong>: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµ ê°€ëŠ¥.<br/> âš  <strong>ë‹¨ì </strong>: ê³µë³€ëŸ‰ ì´ë™(Covariate Shift) ë¬¸ì œë¡œ ì¸í•´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŒ.</p> <hr/> <h4 id="12-bcì˜-ì£¼ìš”-í•œê³„-ê³µë³€ëŸ‰-ì‰¬í”„íŠ¸covariate-shift-ë¬¸ì œ"><strong>1.2 BCì˜ ì£¼ìš” í•œê³„: ê³µë³€ëŸ‰ ì‰¬í”„íŠ¸(Covariate Shift) ë¬¸ì œ</strong></h4> <ul> <li><strong>í›ˆë ¨ ì¤‘ì—ëŠ” ì „ë¬¸ê°€ ë°ì´í„°ì—ì„œ í•™ìŠµí•˜ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì—ì´ì „íŠ¸ê°€ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œ ë°œìƒ</strong>.</li> <li>ì˜ëª»ëœ í–‰ë™ì„ ìˆ˜í–‰í•˜ë©´ <strong>ì „ë¬¸ê°€ì˜ ì‹œì—° ë°ì´í„°ì—ì„œ ë²—ì–´ë‚˜ë©°, ë³µêµ¬ ë°©ë²•ì„ í•™ìŠµí•˜ì§€ ëª»í•¨</strong>.</li> </ul> <p><strong>â¡ í•´ê²° ë°©ë²•</strong>:</p> <ol> <li><strong>ëŒ€í™”í˜• ëª¨ë°© í•™ìŠµ(Interactive IL)</strong> â†’ ì „ë¬¸ê°€ ê°œì…ì„ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ì±… ë³´ì • (ì˜ˆ: DAgger, SafeDAgger).</li> <li><strong>ì „ë¬¸ê°€ ì •ì±… ë¶„í¬ ì¶”ì • ê¸°ë°˜ ë³´ìƒ í•™ìŠµ</strong> â†’ ì „ë¬¸ê°€ ì •ì±…ì„ ë”°ë¥´ë„ë¡ ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ í›„ RLë¡œ ìµœì í™” (ì˜ˆ: SQIL).</li> <li><strong>ì •ì±…ì„ ì œí•œí•˜ì—¬ ì „ë¬¸ê°€ ìƒíƒœ ê³µê°„ì—ì„œë§Œ ì‘ë™í•˜ë„ë¡ í•™ìŠµ</strong> â†’ ì „ë¬¸ê°€ ê¶¤ì ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ (ì˜ˆ: ììœ¨ì£¼í–‰ ì—°êµ¬).</li> </ol> <hr/> <h4 id="13-bcì˜-ì¶”ê°€ì ì¸-ë¬¸ì œ-ì¸ê³¼-ê´€ê³„-ì˜¤ì¸causal-misidentification-ë¬¸ì œ"><strong>1.3 BCì˜ ì¶”ê°€ì ì¸ ë¬¸ì œ: ì¸ê³¼ ê´€ê³„ ì˜¤ì¸(Causal Misidentification) ë¬¸ì œ</strong></h4> <ul> <li>BCëŠ” ì „ë¬¸ê°€ í–‰ë™ì„ ë‹¨ìˆœ ë³µì œí•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ, <strong>í–‰ë™ì˜ ì›ì¸ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° ë°œìƒ</strong>.</li> <li>íŠ¹íˆ, <strong>Copycat Problem</strong>ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ â†’ ì—ì´ì „íŠ¸ê°€ ì „ë¬¸ê°€ì˜ ì´ì „ í–‰ë™ì„ ë‹¨ìˆœíˆ ë”°ë¼ í•˜ëŠ” í˜„ìƒ.</li> </ul> <p><strong>â¡ í•´ê²° ë°©ë²•</strong>:</p> <ol> <li><strong>Residual Action Prediction</strong> â†’ ê³¼ê±° í–‰ë™ì„ ì§ì ‘ ì˜ˆì¸¡í•˜ì§€ ì•Šê³ , â€œì˜ˆì¸¡ ë³´ì •(residual prediction)â€ ìˆ˜í–‰.</li> <li><strong>Memory Extraction Module</strong> â†’ ê³¼ê±° í–‰ë™ ì •ë³´ë¥¼ ì œê±°í•˜ê³  í˜„ì¬ ìƒíƒœë§Œì„ ì´ìš©í•œ í–‰ë™ ì˜ˆì¸¡ ìˆ˜í–‰.</li> </ol> <p>âœ… <strong>BCì˜ ì£¼ìš” ì—°êµ¬ ë°©í–¥</strong>: ì „ë¬¸ê°€ ê°œì…ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²• ê°œë°œ.</p> <hr/> <h3 id="-2-inverse-reinforcement-learning-irl"><strong>ğŸ“Œ 2. Inverse Reinforcement Learning (IRL)</strong></h3> <p>IRL(ì—­ê°•í™”í•™ìŠµ)ì€ <strong>ì „ë¬¸ê°€ì˜ í–‰ë™ì—ì„œ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹</strong>.</p> <h4 id="21-irlì˜-ê°œë…-ë°-íŠ¹ì§•"><strong>2.1 IRLì˜ ê°œë… ë° íŠ¹ì§•</strong></h4> <ul> <li><strong>ì „ë¬¸ê°€ê°€ ìµœì  ì •ì±…ì„ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³ , ë³´ìƒ í•¨ìˆ˜(Reward Function)ë¥¼ ì¶”ì •</strong>.</li> <li><strong>ì¶”ì •ëœ ë³´ìƒ í•¨ìˆ˜ë¥¼ RLì„ í†µí•´ ìµœì í™”í•˜ì—¬ ìµœì  ì •ì±… í•™ìŠµ</strong>.</li> <li><strong>í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í•„ìš”ë¡œ í•˜ë©°, ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ë‚®ìŒ</strong>.</li> </ul> <p>âœ… <strong>ì¥ì </strong>: í™˜ê²½ì˜ ë™ì—­í•™ì„ í™œìš©í•˜ì—¬ ë” ì¼ë°˜í™”ëœ ì •ì±… í•™ìŠµ ê°€ëŠ¥.<br/> âš  <strong>ë‹¨ì </strong>: ë³´ìƒ í•¨ìˆ˜ í•™ìŠµ ê³¼ì •ì´ ê³„ì‚°ì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì´ë©°, ì•ˆì „ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.</p> <hr/> <h4 id="22-irlì˜-ì£¼ìš”-í•œê³„-ë°-í•´ê²°-ë°©ë²•"><strong>2.2 IRLì˜ ì£¼ìš” í•œê³„ ë° í•´ê²° ë°©ë²•</strong></h4> <ol> <li><strong>ìƒ˜í”Œ íš¨ìœ¨ì„± ë¬¸ì œ â†’ ë³€ë¶„ ì¶”ë¡ (Variational Inference) í™œìš©í•˜ì—¬ ê°œì„  (ì˜ˆ: AVRIL)</strong></li> <li><strong>ë³´ìƒ í•¨ìˆ˜ì™€ ì •ì±… ê°„ì˜ ëª¨í˜¸ì„± í•´ê²° â†’ ìµœëŒ€ ë§ˆì§„ ê¸°ë²•(Maximum Margin Methods), ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ IRL(MaxEntIRL) ì ìš©</strong></li> </ol> <p>âœ… <strong>IRLì˜ ì£¼ìš” ì—°êµ¬ ë°©í–¥</strong>: ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê³ , ë³´ìƒ í•¨ìˆ˜ ì¶”ì •ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì´ëŠ” ì—°êµ¬ ì§„í–‰.</p> <hr/> <h3 id="-3-adversarial-imitation-learning-ail"><strong>ğŸ“Œ 3. Adversarial Imitation Learning (AIL)</strong></h3> <p>AIL(ì ëŒ€ì  ëª¨ë°© í•™ìŠµ)ì€ <strong>IRLì˜ ê³„ì‚° ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì ëŒ€ì  í•™ìŠµ(Adversarial Training)ì„ ì ìš©í•œ ë°©ì‹</strong>.</p> <h4 id="31-ailì˜-ê°œë…-ë°-íŠ¹ì§•"><strong>3.1 AILì˜ ê°œë… ë° íŠ¹ì§•</strong></h4> <ul> <li><strong>GANê³¼ ìœ ì‚¬í•œ êµ¬ì¡°ì˜ 2ì¸ ê²Œì„(ì—ì´ì „íŠ¸ vs. íŒë³„ê¸°) í™œìš©</strong>.</li> <li><strong>íŒë³„ê¸°(Discriminator)ëŠ” ì „ë¬¸ê°€ì™€ ì—ì´ì „íŠ¸ì˜ ìƒíƒœ-í–‰ë™ ë¶„í¬ ì°¨ì´ë¥¼ í•™ìŠµí•˜ê³ , ì´ë¥¼ ë³´ìƒìœ¼ë¡œ ë³€í™˜</strong>.</li> <li><strong>ê°•í™” í•™ìŠµ(RL)ì„ í†µí•´ ì—ì´ì „íŠ¸ê°€ ì „ë¬¸ê°€ì˜ í–‰ë™ì„ ë”°ë¼ê°€ë„ë¡ í•™ìŠµ</strong>.</li> </ul> <p>âœ… <strong>ì¥ì </strong>: IRLë³´ë‹¤ ë” ë¹ ë¥´ê³  ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ë†’ìŒ.<br/> âš  <strong>ë‹¨ì </strong>: í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìœ¼ë©°, ìµœì  ì •ì±…ì´ ìˆ˜ë ´í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ.</p> <hr/> <h4 id="32-ailì˜-ëŒ€í‘œì ì¸-ë°©ë²•ë“¤"><strong>3.2 AILì˜ ëŒ€í‘œì ì¸ ë°©ë²•ë“¤</strong></h4> <ol> <li><strong>GAIL (Generative Adversarial Imitation Learning)</strong> â†’ íŒë³„ê¸°ì˜ í˜¼ë€ë„ë¥¼ ë³´ìƒìœ¼ë¡œ í™œìš©í•˜ì—¬ ì „ë¬¸ê°€ ì •ì±…ì„ ë³µì œ.</li> <li><strong>Wasserstein AIL (PWIL)</strong> â†’ Wasserstein ê±°ë¦¬ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± ê°œì„ .</li> </ol> <p>âœ… <strong>AILì˜ ì£¼ìš” ì—°êµ¬ ë°©í–¥</strong>: í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ê³ , GAN ê¸°ë°˜ ìµœì í™”ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²• ì—°êµ¬.</p> <hr/> <h3 id="-4-imitation-from-observation-ifo"><strong>ğŸ“Œ 4. Imitation from Observation (IfO)</strong></h3> <p>IfO(ê´€ì°°ì„ í†µí•œ ëª¨ë°© í•™ìŠµ)ëŠ” <strong>í–‰ë™ ì •ë³´ ì—†ì´ ìƒíƒœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê¸°ë²•</strong>.</p> <h4 id="41-ifoì˜-ê°œë…-ë°-íŠ¹ì§•"><strong>4.1 IfOì˜ ê°œë… ë° íŠ¹ì§•</strong></h4> <ul> <li><strong>ì „ë¬¸ê°€ì˜ í–‰ë™(Action) ì •ë³´ ì—†ì´, ìƒíƒœ(State) ì •ë³´ë§Œì„ ì´ìš©í•˜ì—¬ ëª¨ë°© í•™ìŠµ ìˆ˜í–‰</strong>.</li> <li><strong>ì¸í„°ë„· ë¹„ë””ì˜¤(YouTube)ì™€ ê°™ì€ ëŒ€ê·œëª¨ ë¹„ì •í˜• ë°ì´í„° í™œìš© ê°€ëŠ¥</strong>.</li> </ul> <p>âœ… <strong>ì¥ì </strong>: ê¸°ì¡´ ILë³´ë‹¤ ë°ì´í„° ìˆ˜ì§‘ì´ ìš©ì´í•˜ë©°, ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ ëª¨ë°© í•™ìŠµ ê°€ëŠ¥.<br/> âš  <strong>ë‹¨ì </strong>: í–‰ë™ì„ ì§ì ‘ ê´€ì°°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í–‰ë™ì„ ë³µì›í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.</p> <hr/> <h4 id="42-ifoì˜-ëŒ€í‘œì ì¸-ë°©ë²•ë“¤"><strong>4.2 IfOì˜ ëŒ€í‘œì ì¸ ë°©ë²•ë“¤</strong></h4> <ol> <li><strong>BCO (Behavior Cloning from Observation)</strong> â†’ í–‰ë™ì„ ì—­ì¶”ë¡ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.</li> <li><strong>GAIfO (Generative Adversarial Imitation from Observation)</strong> â†’ GAIL êµ¬ì¡°ë¥¼ IfOì— ì ìš©í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± ê°œì„ .</li> <li><strong>Time-Contrastive Networks (TCN)</strong> â†’ ì¹´ë©”ë¼ ì‹œì  ì°¨ì´ë¥¼ ê·¹ë³µí•˜ëŠ” í•™ìŠµ ê¸°ë²•.</li> </ol> <p>âœ… <strong>IfOì˜ ì£¼ìš” ì—°êµ¬ ë°©í–¥</strong>: í–‰ë™ì„ ì§ì ‘ ê´€ì°°í•  í•„ìš” ì—†ì´, ìƒíƒœ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì •ë°€í•˜ê²Œ ì„¤ê³„í•˜ëŠ” ì—°êµ¬ ì§„í–‰.</p> <hr/> <h3 id="-ì¢…í•©-ê²°ë¡ "><strong>ğŸ“Œ ì¢…í•© ê²°ë¡ </strong></h3> <ul> <li><strong>BC â†’ IRL â†’ AIL â†’ IfOë¡œ ì—°êµ¬ê°€ ë°œì „í•˜ë©´ì„œ, ìƒ˜í”Œ íš¨ìœ¨ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì´ ê°œì„ ë˜ê³  ìˆìŒ</strong>.</li> <li><strong>AILê³¼ IfOëŠ” ê¸°ì¡´ ILì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ì´ë©°, í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ë„ë©”ì¸ ê°„ ì „ì´ í•™ìŠµê³¼ í–‰ë™ ë³µì› ì„±ëŠ¥ ê°œì„ ì´ í•µì‹¬ ê³¼ì œê°€ ë  ê²ƒ</strong>.</li> </ul>]]></content><author><name></name></author><category term="RL"/><category term="Imitation"/><category term="Learning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Imitation Learning (ëª¨ë°© í•™ìŠµ)]]></summary></entry><entry><title type="html">Copycat ë¬¸ì œ</title><link href="https://optreal.github.io/blog/2025/copycat/" rel="alternate" type="text/html" title="Copycat ë¬¸ì œ"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/copycat</id><content type="html" xml:base="https://optreal.github.io/blog/2025/copycat/"><![CDATA[<h3 id="copycat-ë¬¸ì œë€"><strong>Copycat ë¬¸ì œë€?</strong></h3> <p>Copycat ë¬¸ì œëŠ” <strong>â€œì†ì„ìˆ˜ í•™ìŠµâ€</strong> ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.</p> <ul> <li>ì–´ë–¤ í•™ìƒì´ ì„ ìƒë‹˜ì˜ ì„¤ëª…ì„ ë“£ê³  ë”°ë¼ í•´ì•¼ í•˜ëŠ” ìˆ™ì œê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì. ì´ í•™ìƒì´ ì„ ìƒë‹˜ì˜ ë§ì„ ì§„ì •ìœ¼ë¡œ ì´í•´í•˜ê³  ë”°ë¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, <strong>ë‹¨ìˆœíˆ ì§ì „ì— í•œ ë§ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ í•˜ê¸°ë§Œ í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?</strong></li> <li>ì´ëŸ¬í•œ ë°©ì‹ì€ ì¦‰ê°ì ì¸ ë°˜ì‘ì„ ë³´ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, <strong>ì™œ ê·¸ë ‡ê²Œ ë§í•´ì•¼ í•˜ëŠ”ì§€, ì–´ë–¤ ìƒí™©ì—ì„œ ê·¸ ë§ì„ í•´ì•¼ í•˜ëŠ”ì§€ëŠ” ì „í˜€ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤.</strong></li> <li>Copycat ë¬¸ì œëŠ” <strong>ëª¨ë°© í•™ìŠµ(Imitation Learning, IL)</strong>ì—ì„œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ <strong>ì¸ê³¼ì  í˜¼ë™(Causal Confusion)</strong>ë¬¸ì œ ì¤‘ í•˜ë‚˜ì´ë‹¤.</li> <li>ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ì˜ ë™ì—­í•™(dynamics)ì„ ì´í•´í•˜ë ¤ í•˜ì§€ ì•Šê³ , ë‹¨ìˆœíˆ ì „ë¬¸ê°€ì˜ ê³¼ê±° í–‰ë™ì„ ë³µì‚¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.</li> <li>ì´ëŸ¬í•œ ë¬¸ì œëŠ” ë³µì¡í•œ í™˜ê²½ì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ë¿ë§Œ ì•„ë‹ˆë¼, ì•ˆì „ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ì‘ì—…ì—ì„œë„ ì˜¤ë¥˜ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆë‹¤.</li> </ul> <hr/> <h3 id="ì˜ˆì œ-1-ìˆ˜í•™-ë¬¸ì œ-í’€ê¸°"><strong>ì˜ˆì œ 1: ìˆ˜í•™ ë¬¸ì œ í’€ê¸°</strong></h3> <p>ì„ ìƒë‹˜ì´ ì¹ íŒì— ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ë©´ì„œ <strong>â€œ3 + 5 = 8â€</strong>ì´ë¼ê³  ì„¤ëª…í–ˆë‹¤ê³  ê°€ì •í•˜ì.<br/> ë§Œì•½ í•™ìƒì´ <strong>ì§„ì •ìœ¼ë¡œ ìˆ˜í•™ì˜ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‹¤ë©´</strong>, ê°™ì€ ë°©ë²•ì„ í™œìš©í•˜ì—¬ <strong>4 + 6</strong>ê³¼ ê°™ì€ ìƒˆë¡œìš´ ë¬¸ì œë„ í•´ê²°í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.<br/> ê·¸ëŸ¬ë‚˜ í•™ìƒì´ <strong>ë‹¨ìˆœíˆ ì¹ íŒì— ì íŒ ë‹µë§Œì„ ì™¸ì›Œì„œ â€œ3 + 5ëŠ” ë¬´ì¡°ê±´ 8â€ì´ë¼ê³ ë§Œ ê¸°ì–µí•œë‹¤ë©´</strong>, ìƒˆë¡œìš´ ë¬¸ì œê°€ ì œì‹œë˜ì—ˆì„ ë•Œ í•´ê²°í•  ìˆ˜ ì—†ê²Œ ëœë‹¤.</p> <p>ì´ê²ƒì´ ë°”ë¡œ <strong>Copycat ë¬¸ì œ</strong>ì´ë‹¤.</p> <blockquote> <p><strong>ì¦‰, ì›ë¦¬ë¥¼ ì´í•´í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ë”°ë¼ í•˜ê¸°ë§Œ í•˜ë©´, ìƒˆë¡œìš´ ìƒí™©ì—ì„œ ì´ë¥¼ ì ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ì ì´ ë¬¸ì œë¡œ ì‘ìš©í•œë‹¤.</strong></p> </blockquote> <hr/> <h3 id="ì˜ˆì œ-2-ììœ¨ì£¼í–‰-ìë™ì°¨"><strong>ì˜ˆì œ 2: ììœ¨ì£¼í–‰ ìë™ì°¨</strong></h3> <p>ììœ¨ì£¼í–‰ ìë™ì°¨ê°€ <strong>ìš´ì „ìì˜ í–‰ë™ì„ ëª¨ë°© í•™ìŠµí•œë‹¤ê³  ê°€ì •í•´ë³´ì.</strong></p> <ul> <li>ìš´ì „ìê°€ <strong>í•­ìƒ ì‹ í˜¸ë“±ì´ ì´ˆë¡ìƒ‰ì¼ ë•Œë§Œ ì¶œë°œí–ˆë‹¤</strong>ê³  í•˜ì.</li> <li>ê·¸ëŸ¬ë‚˜ ììœ¨ì£¼í–‰ ìë™ì°¨ê°€ <strong>ì‹ í˜¸ë“±ì˜ ìƒ‰ì„ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë‹¨ìˆœíˆ â€œìš´ì „ìê°€ ì–¸ì œ ì¶œë°œí–ˆëŠ”ì§€â€ë§Œ í•™ìŠµí–ˆë‹¤ë©´?</strong></li> <li>ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ìë™ì°¨ëŠ” <strong>ìƒˆë¡œìš´ ë„ë¡œ ìƒí™©ì´ë‚˜ ë‹¤ë¥¸ ì‹ í˜¸ë“± ì•ì—ì„œëŠ” ì˜¬ë°”ë¥´ê²Œ ì¶œë°œí•´ì•¼ í• ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.</strong></li> <li>ë˜í•œ ë‹¤ë¥¸ ì˜ˆì œë¡œì„œ, ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì—ì„œ ì „ë¬¸ê°€ê°€ ì´ì „ì— ì¢ŒíšŒì „ì„ í–ˆê³ , í˜„ì¬ë„ ì¢ŒíšŒì „í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²½ìš°, ì—ì´ì „íŠ¸ëŠ” ì£¼ë³€ í™˜ê²½ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ê·¸ì € ì´ì „ í–‰ë™ì„ ë”°ë¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ì±…ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li> </ul> <p>ì´ê²ƒë„ <strong>Copycat ë¬¸ì œ</strong>ì´ë‹¤.</p> <blockquote> <p><strong>ì¦‰, ìƒí™©ì„ ë³´ê³  ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë‹¨ìˆœíˆ ê³¼ê±° í–‰ë™ì„ ê¸°ê³„ì ìœ¼ë¡œ ë”°ë¼ í•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¬¸ì œì´ë‹¤.</strong></p> </blockquote> <hr/> <h3 id="copycat-ë¬¸ì œë¥¼-í•´ê²°í•˜ëŠ”-ë°©ë²•ì˜-ê¸°ë³¸-ì•„ì´ë””ì–´"><strong>Copycat ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´</strong></h3> <p>Copycat ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” <strong>ë‹¨ìˆœíˆ ë”°ë¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í–‰ë™ì˜ ì›ì¸ì„ ì´í•´í•˜ëŠ” í•™ìŠµ ë°©ì‹ì´ í•„ìš”í•˜ë‹¤.</strong></p> <h4 id="1-ì˜ëª»ëœ-í•™ìŠµ-ìŠµê´€ì„-ì œê±°í•˜ê¸°"><strong>1. ì˜ëª»ëœ í•™ìŠµ ìŠµê´€ì„ ì œê±°í•˜ê¸°</strong></h4> <ul> <li>ë‹¨ìˆœíˆ ë”°ë¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, <strong>ì™œ í•´ë‹¹ í–‰ë™ì´ í•„ìš”í•œì§€</strong>ì— ëŒ€í•œ ë¶„ì„ì´ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤.</li> </ul> <h4 id="2-í•„ìš”í•œ-ì •ë³´ë§Œ-í•™ìŠµí•˜ê¸°"><strong>2. í•„ìš”í•œ ì •ë³´ë§Œ í•™ìŠµí•˜ê¸°</strong></h4> <ul> <li>ììœ¨ì£¼í–‰ ìë™ì°¨ì˜ ì˜ˆì‹œì—ì„œ ë‹¨ìˆœíˆ <strong>â€œìš´ì „ìê°€ ì¶œë°œí–ˆë‹¤â€</strong>ëŠ” ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼,</li> <li><strong>â€œì‹ í˜¸ë“±ì´ ì´ˆë¡ìƒ‰ì¼ ë•Œ ì¶œë°œí•´ì•¼ í•œë‹¤â€</strong>ëŠ” ê·œì¹™ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•´ì•¼ í•œë‹¤.</li> </ul> <h4 id="3-ì‹¤ì œ-í™˜ê²½ì—ì„œ-ì—°ìŠµí•˜ê¸°"><strong>3. ì‹¤ì œ í™˜ê²½ì—ì„œ ì—°ìŠµí•˜ê¸°</strong></h4> <ul> <li>ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì§ì ‘ ê²½í—˜í•˜ë„ë¡ í•˜ì—¬, ìƒˆë¡œìš´ ë¬¸ì œì—ì„œë„ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì„ ìœ ë„í•´ì•¼ í•œë‹¤.</li> </ul> <hr/> <h3 id="copycat-ë¬¸ì œë¥¼-í•´ê²°í•˜ëŠ”-ì „ë¬¸ì ì¸-ë°©ë²•"><strong>Copycat ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì „ë¬¸ì ì¸ ë°©ë²•</strong></h3> <p>Copycat ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì—°êµ¬ìë“¤ì€ <strong>ì „ë¬¸ê°€ í–‰ë™ì„ ë‹¨ìˆœ ë³µì‚¬í•˜ì§€ ì•Šê³ , í™˜ê²½ì„ ì´í•´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ê¸°ë²•</strong>ì„ ê°œë°œí–ˆë‹¤.</p> <h4 id="1-ì ëŒ€ì -í•™ìŠµ-ê¸°ë°˜-í”¼ì²˜-í•™ìŠµ-adversarial-feature-learning"><strong>1. ì ëŒ€ì  í•™ìŠµ ê¸°ë°˜ í”¼ì²˜ í•™ìŠµ (Adversarial Feature Learning)</strong></h4> <ul> <li>Wen et al. [1]ì€ Copycat ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>ì ëŒ€ì  í•™ìŠµ(adversarial learning)</strong>ì„ í™œìš©í•˜ì˜€ë‹¤.</li> <li><strong>í•µì‹¬ ì•„ì´ë””ì–´</strong>: <ul> <li>íŠ¹ì • íŠ¹ì§•(feature representation)ì„ í•™ìŠµí•  ë•Œ, <strong>ì „ë¬¸ê°€ì˜ ê³¼ê±° í–‰ë™(previous actions)ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê±°</strong>í•˜ê³ ,</li> <li><strong>ì˜¬ë°”ë¥¸ ë‹¤ìŒ í–‰ë™(next action)ì„ ì˜ˆì¸¡í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë§Œ ìœ ì§€</strong>í•˜ë„ë¡ í•¨.</li> </ul> </li> <li>ì´ë¥¼ í†µí•´, <strong>ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì„ ì§ì ‘ ë¶„ì„í•˜ê³  íŒë‹¨í•˜ëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ë„ë¡ ìœ ë„</strong>í•˜ì˜€ë‹¤.</li> </ul> <h4 id="2-ë©”ëª¨ë¦¬-ê¸°ë°˜-ì¸ê³¼-êµ¬ì¡°-í•™ìŠµ-memory-based-causal-structure-learning"><strong>2. ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¸ê³¼ êµ¬ì¡° í•™ìŠµ (Memory-based Causal Structure Learning)</strong></h4> <ul> <li>Chuang et al. [2]ì€ Copycat ë¬¸ì œ í•´ê²° ë°©ë²•ì„ <strong>ê³ ì°¨ì› ì´ë¯¸ì§€ ê´€ì°°(high-dimensional image observations)ìœ¼ë¡œ í™•ì¥</strong>í•˜ì˜€ë‹¤.</li> <li><strong>ë©”ëª¨ë¦¬ ì¶”ì¶œ ëª¨ë“ˆ(memory extraction module)</strong>ì„ ì‚¬ìš©í•˜ì—¬: <ul> <li><strong>ê³¼ê±° ê´€ì°°(observation history)ì—ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ</strong>í•˜ê³ ,</li> <li><strong>ì´ì „ í–‰ë™(previous actions)ê³¼ ê´€ë ¨ëœ ë¶ˆí•„ìš”í•œ ì •ë³´(nuisance correlates)ë¥¼ ì œê±°</strong>í•˜ì˜€ë‹¤.</li> </ul> </li> <li>ì´ë¥¼ í†µí•´, ì—ì´ì „íŠ¸ê°€ <strong>ë‹¨ìˆœí•œ ë³µì‚¬ê°€ ì•„ë‹Œ, ì‹¤ì œë¡œ í™˜ê²½ì„ ì´í•´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ì±…ì„ í•™ìŠµ</strong>í•˜ë„ë¡ ìœ ë„í•˜ì˜€ë‹¤.</li> </ul> <h4 id="3-í™˜ê²½-ë™ì—­í•™-ëª¨ë¸-í™œìš©-dynamics-model-based-training"><strong>3. í™˜ê²½ ë™ì—­í•™ ëª¨ë¸ í™œìš© (Dynamics Model-Based Training)</strong></h4> <ul> <li>Copycat ë¬¸ì œë¥¼ ë°©ì§€í•˜ë ¤ë©´ <strong>ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì˜ ë™ì—­í•™ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ í¬í•¨</strong>ë˜ì–´ì•¼ í•œë‹¤.</li> <li>ì´ë¥¼ ìœ„í•´: <ul> <li><strong>ê°•í™” í•™ìŠµ(Reinforcement Learning, RL) ê¸°ë°˜ ì ‘ê·¼ë²•</strong>ì„ ì ìš©í•˜ì—¬ <strong>í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ í•™ìŠµ</strong>í•˜ë„ë¡ í•¨.</li> <li><strong>ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„(Reward Function Design)</strong>ë¥¼ í†µí•´, ì „ë¬¸ê°€ í–‰ë™ì„ ë‹¨ìˆœíˆ ë”°ë¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, <strong>í™˜ê²½ì— ì ì‘í•˜ëŠ” í–‰ë™ì„ ì¥ë ¤</strong>í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìœ ë„í•  ìˆ˜ ìˆìŒ.</li> </ul> </li> </ul> <hr/> <h3 id="ê²°ë¡ "><strong>ê²°ë¡ </strong></h3> <p>Copycat ë¬¸ì œëŠ” <strong>ì§„ì •í•œ ì›ë¦¬ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ , ê³¼ê±° í–‰ë™ì„ ë‹¨ìˆœíˆ ë³µì‚¬í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ</strong>ì´ë‹¤.<br/> ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” <strong>ê³¼ê±° í–‰ë™ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í–‰ë™ì˜ ì›ì¸ê³¼ ê²°ê³¼(ì¸ê³¼ ê´€ê³„)ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ í•„ìš”í•˜ë‹¤.</strong></p>]]></content><author><name></name></author><category term="RL"/><category term="Imitation"/><category term="Learning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Copycat ë¬¸ì œ]]></summary></entry><entry><title type="html">Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments</title><link href="https://optreal.github.io/blog/2025/traj_planning/" rel="alternate" type="text/html" title="Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments"/><published>2025-01-14T00:00:00+00:00</published><updated>2025-01-14T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/traj_planning</id><content type="html" xml:base="https://optreal.github.io/blog/2025/traj_planning/"><![CDATA[<div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/authors-480.webp 480w,/assets/img/traj_planning/authors-800.webp 800w,/assets/img/traj_planning/authors-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/authors.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="abstract">Abstract</h2> <ul> <li>Motivation <ul> <li>Increase necessary to swiftly evade obstacles and adapt trajectories under hard real-time constraints. <ul> <li>Generate viable paths that prevent collisions while maintaining high speeds with minimal tracking errors.</li> </ul> </li> </ul> </li> <li>Method <ul> <li>The proposed method combines a supervised learning approach, as teacher policy, with deep reinforcement learning (DRL), as student policy. <ol> <li>Train the teacher policy using a path planning algorithm that prioritizes safety while minimizing jerk and flight time.</li> <li>Use this policy to guide the learning of the student policy in various unknown environments.</li> </ol> </li> </ul> </li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="introduction">Introduction</h3> <ul> <li>Previous worksâ€™ limitation <ul> <li>Require <ul> <li>Known environments.</li> <li>Extensive information for reliable outcomes, which is seldom the case in real-world missions.</li> </ul> </li> </ul> </li> <li>Proposed Method <ul> <li>A trajectory planning method for generating agile flight trajectories in unknown environments solely based on data from onboard sensors.</li> <li>The framework integrates two neural networks: <ul> <li>The teacher policy <ul> <li>Supervised learning.</li> <li>Incorporates a geometry-based trajectory planning strategy enriched with a heuristic to optimize flight time and enhance safety.</li> </ul> </li> <li>The student policy <ul> <li>DQN-PER.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="methodology">Methodology</h2> <ul> <li>The primary aim <ul> <li>The primary aim of our proposed approach is to facilitate the safe and agile navigation of UAVs in <strong>unknown environments</strong>, leveraging <strong>3D LiDAR</strong> for environmental perception.</li> <li>Our strategy involves establishing the UAV trajectory with a minimal flight time based on real-time sensory data while prioritizing safety.</li> </ul> </li> <li>The proposed privileged reinforcement learning framework <ul> <li>The teacher policy <ul> <li>Deep Feedforward Neural Network (DFNN).</li> <li>Trains using an expert algorithm proficient.</li> <li>Provides optimal action insights across diverse environments.</li> <li>Evaluates the student policy <strong>(DRL reward)</strong>.</li> </ul> </li> <li>The student policy <ul> <li>DQN-PER</li> <li>Operates based on data identifiable by the UAVâ€™s 3D Lidar around its current pose.</li> </ul> </li> <li>Both networks output the new ideal waypoints to be followed (\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> <li>The distilled knowledge from the teacher policy is integrated into the student policy, functioning without privileged information.</li> </ul> </li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="problem-definition">Problem Definition</h4> <ul> <li>Our focus is on formulating a practical and reliable trajectory planning strategy, aiming to optimize three key objectives: <ul> <li>Minimizing jerk.</li> <li>Reducing flight time.</li> <li>Enhancing safety by maximizing the distance to the obstacles in the environment.</li> </ul> </li> <li>The goal is to minimize equations (1), (2), and (3) while maximizing (4) to enhance safety and ensure collision free trajectories for high-speed flights. <ol> <li>Goal Distance: \(D_{\text{goal}} = \sum_{i=0}^n \sqrt{(\mathbf{p}_{\text{goal}} - \mathbf{p}_i)^2}\)</li> <li>Next Step Distance: \(D_{\text{next}} = \sum_{i=0}^{n-1} \sqrt{(\mathbf{p}_{i+1} - \mathbf{p}_i)^2}\)</li> <li>Jerk Cost: \(D_{\text{jerk}} = \int \left( \frac{d^3 \mathbf{p}}{dt^3} \right)^2 dt\)</li> <li>Obstacle Distance: \(D_{\text{obs}} = \max \sum_{j=0}^k \sqrt{(\mathbf{p}_i - \mathbf{O}_j)^2}\)</li> </ol> </li> </ul> <h3 id="expert">Expert</h3> <ul> <li>Bidirectional A* algorithm <ul> <li>We integrate objective equations as heuristics for trajectory generation: \(f(i) = (D_{\text{goal}} + D_{\text{obs}} + D_{\text{next}}) \times D_{\text{jerk}}\)</li> </ul> </li> <li>Input: <ul> <li>Entire environment.</li> <li>Global position.</li> <li>Goal Node.</li> </ul> </li> <li>Output: <ul> <li>The ideal next waypoint for each waypoint.</li> </ul> </li> </ul> <h3 id="teacher-policy">Teacher Policy</h3> <ul> <li>The policy is trained across multiple randomly generated scenarios. <ul> <li>Rainforests.</li> <li>Mazes.</li> <li>Disaster Areas.</li> </ul> </li> <li>To ensure precision, a distinct model is learned for each scenario, resulting in 30 models trained across different scenarios, with 10 models for each environment.</li> <li>Input: <ul> <li>Goal node.</li> <li>Global position.</li> <li>Orientation.</li> <li>The environment around within a 5 Ã— 5 Ã— 2 meter radius range.</li> </ul> </li> <li>Output: <ul> <li>(\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> <li>The subsequent ideal action determined by our expert.</li> </ul> </li> </ul> <h3 id="student-policy">Student Policy</h3> <ul> <li>The student policy efficiently produces real-time, collision-free trajectories for agile flights, relying solely on onboard sensor measurements.</li> <li>Input: <ul> <li>obstacle positions within a 5 Ã— 5 Ã— 2 meter radius obtained from the 3D Lidar.</li> <li>UAV position</li> <li>Orientation.</li> <li>Goal node.</li> </ul> </li> <li>Ouput: <ul> <li>(\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> </ul> </li> <li>During flight, the UAV records obstacle positions, triggering policy execution upon identifying new obstacles.</li> <li>The policy generates a new trajectory using obstacle positions providing a single waypoint per iteration.</li> <li>Multiple policy runs are conducted to create the final trajectory.</li> <li>Upon generating a trajectory, it utilizes BÃ©zier curves to transform the anticipated trajectory into a comprehensive state representation.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="results">Results</h2> <ul> <li>The proposed method is evaluated based on: <ul> <li>Flight time (seconds)</li> <li>Processing time (seconds)</li> <li>Representing the time the UAV is at least at 80% of the maximum speed</li> <li>RMSE from the guidance trajectory (meters)</li> <li>Success rate (%)</li> </ul> </li> <li>Baseline: <ul> <li>the standard bidirectional A* in an unknown environment</li> <li>our expert operating in an unknown environment</li> <li>The original DQN-PER</li> <li>Our teacher policy</li> </ul> </li> <li>Testing in simulation demonstrates noteworthy advancements, including an 80% reduction in tracking error, a 31% decrease in flight time, a 19% increase in high-speed duration, and a success rate improvement from 50% to 100%, as compared to baseline methods.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/results-480.webp 480w,/assets/img/traj_planning/results-800.webp 800w,/assets/img/traj_planning/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="Drone"/><category term="Trajectory-Planning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments]]></summary></entry><entry><title type="html">RMA: Rapid Motor Adaptation for Legged Robots</title><link href="https://optreal.github.io/blog/2024/rma/" rel="alternate" type="text/html" title="RMA: Rapid Motor Adaptation for Legged Robots"/><published>2024-12-30T16:00:00+00:00</published><updated>2024-12-30T16:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/rma</id><content type="html" xml:base="https://optreal.github.io/blog/2024/rma/"><![CDATA[<div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/title-480.webp 480w,/assets/img/rma/title-800.webp 800w,/assets/img/rma/title-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/title.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="i-introduction">I Introduction</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig1-480.webp 480w,/assets/img/rma/fig1-800.webp 800w,/assets/img/rma/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig2-480.webp 480w,/assets/img/rma/fig2-800.webp 800w,/assets/img/rma/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="1-legged-roboticsì˜-ë°œì „">1. Legged Roboticsì˜ ë°œì „</h3> <ul> <li>ë¬¼ë¦¬ì  ì—­í•™ ëª¨ë¸ë§ ë° ì œì–´ ì´ë¡  ë„êµ¬ í™œìš©</li> <li>ì¸ê°„ ì„¤ê³„ìì˜ ì „ë¬¸ì„± ìš”êµ¬</li> <li>ê°•í™” í•™ìŠµ ë° ëª¨ë°© í•™ìŠµ ê¸°ë²• ì ìš© <ul> <li>ì„¤ê³„ ë¶€ë‹´ ì™„í™”</li> <li>ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì„±</li> </ul> </li> </ul> <hr/> <h3 id="2-ê°•í™”-í•™ìŠµ-ê¸°ë°˜-ì»¨íŠ¸ë¡¤ëŸ¬ì˜-í‘œì¤€-íŒ¨ëŸ¬ë‹¤ì„">2. ê°•í™” í•™ìŠµ ê¸°ë°˜ ì»¨íŠ¸ë¡¤ëŸ¬ì˜ í‘œì¤€ íŒ¨ëŸ¬ë‹¤ì„</h3> <ul> <li>ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ RL ê¸°ë°˜ ì»¨íŠ¸ë¡¤ëŸ¬ í›ˆë ¨</li> <li>ì‹œë®¬ë ˆì´ì…˜-í˜„ì‹¤ ê°„ ì „ì´ (sim-to-real) ê¸°ìˆ  ì ìš© <ul> <li>ë¬¼ë¦¬ ë¡œë´‡ê³¼ ì‹œë®¬ë ˆì´í„° ëª¨ë¸ì˜ ì°¨ì´</li> <li>í˜„ì‹¤ ì„¸ê³„ ì§€í˜•ì˜ ë‹¤ì–‘ì„±</li> <li>ì‹œë®¬ë ˆì´í„°ì˜ ë¬¼ë¦¬ì  í•œê³„ <ul> <li>ì ‘ì´‰ë ¥, ë³€í˜• ê°€ëŠ¥í•œ í‘œë©´ ë“± ë³µì¡í•œ ë¬¼ë¦¬ í˜„ìƒ</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="3-quadruped-locomotion-challenge-solving-approach">3. Quadruped Locomotion Challenge Solving Approach</h3> <ul> <li><strong>A1 ë¡œë´‡ (Unitree)ì˜ ì‹¤í—˜ í”Œë«í¼ ì‚¬ìš©</strong></li> <li><strong>ì‹¤ì œ ì¸ê°„ ë³´í–‰ì˜ íŠ¹ì§•</strong> <ul> <li>ë‹¤ì–‘í•œ ì§€í˜• ì ì‘ (í™, ì–¸ë•, í•˜ì¤‘ ë“±)</li> <li>í”¼ë¡œ ë° ë¶€ìƒ ëŒ€ì‘ ëŠ¥ë ¥</li> </ul> </li> <li><strong>RMA (Rapid Motor Adaptation)</strong> <ul> <li>ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ì ì‘ í•„ìš” (ì´ˆ ë‹¨ìœ„ ì ì‘)</li> <li>ë¬¼ë¦¬ ì„¸ê³„ì—ì„œ ë‹¤ì¤‘ ì‹¤í—˜ ë° ìµœì í™” ë¶ˆê°€ëŠ¥</li> <li>ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ë°ì´í„° ìˆ˜ì§‘ ì–´ë ¤ì›€ <ul> <li>3-5ë¶„ ë°ì´í„° ìˆ˜ì§‘ì¡°ì°¨ ë¹„í˜„ì‹¤ì </li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="4-ì „ëµ">4. ì „ëµ</h3> <ul> <li>ê¸°ë³¸ ë³´í–‰ ì •ì±… ë° RMAë¥¼ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í›ˆë ¨</li> <li>ì§ì ‘ í˜„ì‹¤ ì„¸ê³„ì— ë°°í¬</li> </ul> <hr/> <h3 id="5-rma-rapid-motor-adaptation">5. RMA (Rapid Motor Adaptation)</h3> <ul> <li><strong>ë‘ ê°€ì§€ í•˜ìœ„ ì‹œìŠ¤í…œ</strong> <ul> <li>ê¸°ë³¸ ì •ì±… (Base Policy, \(\pi\))</li> <li>ì ì‘ ëª¨ë“ˆ (Adaptation Module, \(\phi\))</li> </ul> </li> <li><strong>ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ì ì‘ ì§€ì›</strong> <ul> <li>ë‹¤ì–‘í•œ í™˜ê²½ êµ¬ì„±ì— ì ì‘</li> </ul> </li> </ul> <hr/> <h4 id="51-base-policy-pi">5.1 Base Policy, \(\pi\)</h4> <ul> <li><strong>í›ˆë ¨ ë°©ì‹</strong> <ul> <li>ê°•í™” í•™ìŠµ(RL)ì„ í†µí•œ ì‹œë®¬ë ˆì´ì…˜ í›ˆë ¨</li> <li>í™˜ê²½ êµ¬ì„± ë²¡í„°(\(e_t\))ì˜ privileged information í™œìš© (ë§ˆì°°, í•˜ì¤‘ ë“±)</li> </ul> </li> <li><strong>í™˜ê²½ êµ¬ì„± ë²¡í„°(\(e_t\)) í™œìš© ê³¼ì •</strong> <ul> <li>ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬(\(\mu\))ë¥¼ í†µí•´ ì ì¬ íŠ¹ì§• ê³µê°„(\(z_t\))ìœ¼ë¡œ ì¸ì½”ë”©</li> <li>ì ì¬ ë²¡í„°(\(z_t\)) â†’ ê¸°ë³¸ ì •ì±…(\(Ï€\)) ì…ë ¥</li> <li>ë¡œë´‡ì˜ í˜„ì¬ ìƒíƒœ(\(x_t\)) ë° ì´ì „ í–‰ë™(\(a_{t-1}\))ê³¼ í•¨ê»˜ ì‚¬ìš©</li> <li>ëª©í‘œ ê´€ì ˆ ìœ„ì¹˜(\(a_t\)) ì˜ˆì¸¡</li> </ul> </li> <li><strong>End-to-End Training</strong> <ul> <li>ì •ì±…(\(Ï€\))ê³¼ ì¸ì½”ë”(\(\mu\))ì˜ End-to-End ê°•í™” í•™ìŠµ</li> </ul> </li> </ul> <hr/> <h4 id="52-adaptation-module-phi">5.2 Adaptation Module, \(\phi\)</h4> <ul> <li><strong>ëª©í‘œ</strong> <ul> <li>ì‹¤ì‹œê°„ìœ¼ë¡œ ì ì¬ ë²¡í„°(\(z_t\)) ì¶”ì •</li> <li>Privileged information(\(e_t\)) ì—†ì´ ìƒíƒœ ë° í–‰ë™ historyë¥¼ í†µí•´ ì¶”ì •</li> </ul> </li> <li><strong>ì›ë¦¬</strong> <ul> <li>ë¡œë´‡ ê´€ì ˆì˜ ëª…ë ¹ëœ ì›€ì§ì„ê³¼ ì‹¤ì œ ì›€ì§ì„ì˜ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™˜ê²½ íŠ¹ì„± ì¶”ì •</li> <li>ì¹¼ë§Œ í•„í„°ì™€ ìœ ì‚¬í•œ ì ‘ê·¼ë²•</li> </ul> </li> <li><strong>í›ˆë ¨ ê³¼ì •</strong> <ul> <li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ê°ë… í•™ìŠµìœ¼ë¡œ í›ˆë ¨</li> <li>ìƒíƒœ ì´ë ¥ê³¼ ì ì¬ ë²¡í„°(\(z_t\)) ê³„ì‚° ê°€ëŠ¥</li> </ul> </li> <li><strong>ì‹¤í–‰ ì‹œ ë™ì‘</strong> <ul> <li>ì‹¤ì‹œê°„ ì¶”ì •ìœ¼ë¡œ ì •ì±…(\(Ï€\))ì— í™˜ê²½ íŠ¹ì„± ë²¡í„°(\(z_t\)) ì œê³µ</li> <li>ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰ <ul> <li>Base policy(\(Ï€\)): 100Hz ì‹¤í–‰</li> <li>Adaptation module(\(Ï†\)): 10Hz ì‹¤í–‰</li> <li>ì¤‘ì•™ í´ëŸ­ ì—†ì´ ë…ë¦½ ì‹¤í–‰</li> </ul> </li> <li>ì ì¬ ë²¡í„°(\(z_t\))ë¥¼ ê¸°ë³¸ ì •ì±…ì— ì „ë‹¬í•˜ì—¬ í–‰ë™(\(a_t\)) ì˜ˆì¸¡ ì§€ì›</li> </ul> </li> </ul> <hr/> <h3 id="6-ê¸°ì¡´-ì ‘ê·¼ë²•ê³¼ì˜-ë¹„êµ">6. ê¸°ì¡´ ì ‘ê·¼ë²•ê³¼ì˜ ë¹„êµ</h3> <ul> <li><strong>ê¸°ì¡´ ì ‘ê·¼ë²•</strong> <ul> <li>ìƒˆë¡œìš´ í™˜ê²½ì—ì„œ ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìˆ˜ì§‘ í›„ ì •ì±… ì ì‘</li> <li>ë¬¼ë¦¬ì  ë§¤ê°œë³€ìˆ˜ (ë§ˆì°° ë“±) ë˜ëŠ” ì ì¬ ì¸ì½”ë”© í™œìš©</li> </ul> </li> <li><strong>ê¸°ì¡´ ì ‘ê·¼ë²•ì˜ ë¬¸ì œì </strong> <ul> <li>ì´ˆê¸° ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì¤‘ ë‚™ìƒ ë° ë¡œë´‡ ì†ìƒ ìœ„í—˜</li> </ul> </li> <li><strong>RMA ì ‘ê·¼ë²•ì˜ ì¥ì </strong> <ul> <li>ì ì¬ ë²¡í„°(\(z_t\))ì˜ ë¹ ë¥¸ ì¶”ì •</li> <li>ì¦‰ê°ì  ì •ì±… ì ì‘ì„ í†µí•´ ë‚™ìƒ ë°©ì§€</li> </ul> </li> </ul> <hr/> <h3 id="7-rmaì˜-ì£¼ìš”-íŠ¹ì§•-ë°-í•µì‹¬-ì›ë¦¬">7. RMAì˜ ì£¼ìš” íŠ¹ì§• ë° í•µì‹¬ ì›ë¦¬</h3> <hr/> <h4 id="71-base-policy">7.1 Base Policy</h4> <ul> <li><strong>ê¸°ì¡´ ì ‘ê·¼ë²•ê³¼ì˜ ìœ ì‚¬ì </strong> <ul> <li>í™˜ê²½ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ê°€ ì¸ìë¡œ í™œìš©í•œ ê°•í™” í•™ìŠµ(RL) í›ˆë ¨</li> </ul> </li> <li><strong>ìƒˆë¡œìš´ ì¸¡ë©´</strong> <ul> <li>ë‹¤ì–‘í•œ ì§€í˜• ìƒì„±ê¸° ì‚¬ìš©</li> <li>ìƒì²´ì—ë„ˆì§€í•™ì—ì„œ ì˜ê°ì„ ë°›ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©</li> <li>Reference demonstrations ì—†ì´ ë³´í–‰ ì •ì±… í•™ìŠµ ê°€ëŠ¥</li> </ul> </li> </ul> <hr/> <h4 id="72-adaptation-moduleì˜-ì‘ë™-ì›ë¦¬">7.2 Adaptation Moduleì˜ ì‘ë™ ì›ë¦¬</h4> <ul> <li><strong>System Identificationì˜ ì›ë¦¬ í™œìš©</strong> <ul> <li>ìµœì í™” ë¬¸ì œë¡œì„œì˜ ì‹œìŠ¤í…œ ì‹ë³„ ì ‘ê·¼</li> <li>ì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ì…ë ¥-ì¶œë ¥ ê´€ê³„ ê·¼ì‚¬</li> </ul> </li> <li><strong>ì™„ë²½í•œ ì‹œìŠ¤í…œ ì‹ë³„ì˜ ë¶ˆí•„ìš”ì„±</strong> <ul> <li>Extrinsics Vector(\(z_t\)): í™˜ê²½ ë§¤ê°œë³€ìˆ˜ì˜ ì €ì°¨ì› ë¹„ì„ í˜• íˆ¬ì˜</li> <li>ì •í™•í•œ â€˜ì •ë‹µâ€™ ë²¡í„°ê°€ ì•„ë‹Œ â€˜ì˜¬ë°”ë¥¸ í–‰ë™â€™ì„ ìœ ë„í•˜ëŠ” ë²¡í„°ë¡œ ìµœì í™”</li> </ul> </li> <li><strong>ë‹¤ì–‘í•œ í›ˆë ¨ ìƒí™© ì œê³µ</strong> <ul> <li>í”„ë™íƒˆ ì§€í˜• ìƒì„±ê¸° ì‚¬ìš©</li> <li>ì§ˆëŸ‰, ë§ˆì°° ë“± ë§¤ê°œë³€ìˆ˜ ë¬´ì‘ìœ„í™”</li> <li>ë‹¤ì–‘í•œ ë¬¼ë¦¬ì  ë§¥ë½ì—ì„œì˜ ë³´í–‰ ë°˜ì‘ í›ˆë ¨</li> </ul> </li> </ul> <hr/> <h3 id="8-ì‹¤ì œ-í™˜ê²½ì—ì„œì˜-ì„±ëŠ¥-í‰ê°€">8. ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ í‰ê°€</h3> <ul> <li><strong>ë‹¤ì–‘í•œ ì§€í˜•ì—ì„œì˜ ê²€ì¦</strong> <ul> <li>ë¯¸ë„ëŸ¬ìš´ í‘œë©´</li> <li>ë¶ˆê·œì¹™í•œ ì§€í˜•</li> <li>ë³€í˜• ê°€ëŠ¥í•œ í‘œë©´ (ìŠ¤í€ì§€, ë§¤íŠ¸ë¦¬ìŠ¤ ë“±)</li> <li>ìì—° ì§€í˜• (ì”ë””, ê¸´ ì‹ë¬¼, ì½˜í¬ë¦¬íŠ¸, ìê°ˆ, ë°”ìœ„, ëª¨ë˜ ë“±)</li> </ul> </li> </ul> <hr/> <h3 id="9-rmaì˜-ì£¼ìš”-ê¸°ì—¬">9. RMAì˜ ì£¼ìš” ê¸°ì—¬</h3> <ul> <li>ì‹¤ì‹œê°„ ì ì‘ ëª¨ë“ˆì„ í†µí•œ í™˜ê²½ ë³€í™” ëŒ€ì‘</li> <li>ë°ì´í„°ì…‹ ì˜ì¡´ ì—†ì´ ì¦‰ê°ì ì¸ ì •ì±… ì¡°ì •</li> <li>ê°•ê±´í•œ ë³´í–‰ ì •ì±…ì˜ ë‹¤ì–‘í•œ í™˜ê²½ ì ìš© ê°€ëŠ¥ì„±</li> </ul> <hr/> <h2 id="ii-related-work">II Related Work</h2> <hr/> <h3 id="1-ì „í†µì ì¸-ì œì–´-ê¸°ë°˜-ì ‘ê·¼ë²•-control-based-methods">1. ì „í†µì ì¸ ì œì–´ ê¸°ë°˜ ì ‘ê·¼ë²• (Control-Based Methods)</h3> <ul> <li><strong>ì£¼ìš” ë¡œë´‡ ì‚¬ë¡€</strong> <ul> <li>MIT Cheetah 3 <ul> <li>ì •ê·œí™”ëœ <strong>ëª¨ë¸ ì˜ˆì¸¡ ì œì–´ (MPC)</strong> ì‚¬ìš©</li> <li>ë‹¨ìˆœí™”ëœ ë™ì—­í•™ í™œìš©</li> <li>ê³ ì† ì´ë™ ë° ì¥ì• ë¬¼ ì í”„ ê°€ëŠ¥</li> </ul> </li> <li>ANYmal ë¡œë´‡ <ul> <li>ë§¤ê°œë³€ìˆ˜í™”ëœ ì œì–´ê¸° ìµœì í™”</li> <li>ì—­ì§„ì ëª¨ë¸ ê¸°ë°˜ ê³„íš ìˆ˜í–‰</li> </ul> </li> </ul> </li> <li><strong>í•œê³„ì </strong> <ul> <li>ì •í™•í•œ ì‹¤ì œ ë™ì—­í•™ ëª¨ë¸ ìš”êµ¬</li> <li>ë¡œë´‡ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ í•„ìš”</li> <li>ë³´í–‰ ë° í–‰ë™ ìˆ˜ë™ íŠœë‹ ìš”êµ¬</li> </ul> </li> <li><strong>ê°œì„  ë°©ì•ˆ</strong> <ul> <li>ì œì–´ê¸° ìµœì í™” ë° MPC ê²°í•© <ul> <li>ë¬¸ì œ ì¼ë¶€ ì™„í™”</li> <li>ì—¬ì „íˆ ê³¼ì œë³„ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•„ìš”</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="2-í•™ìŠµ-ê¸°ë°˜-ë³´í–‰-ì ‘ê·¼ë²•-learning-for-legged-locomotion">2. í•™ìŠµ ê¸°ë°˜ ë³´í–‰ ì ‘ê·¼ë²• (Learning for Legged Locomotion)</h3> <ul> <li><strong>ì´ˆê¸° ì‹œë„</strong> <ul> <li>DARPA Learning Locomotion Program</li> </ul> </li> <li><strong>ìµœê·¼ ë™í–¥</strong> <ul> <li>ì‹¬ì¸µ ê°•í™” í•™ìŠµ(RL)ì˜ ë„ì… <ul> <li>ì¸ê°„ ì „ë¬¸ì„± ì˜ì¡´ë„ ê°ì†Œ</li> <li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼ ë„ì¶œ</li> </ul> </li> </ul> </li> <li><strong>í•œê³„ì </strong> <ul> <li>ì •ì±…ì˜ í˜„ì‹¤ ì„¸ê³„ ì „ì´ ì–´ë ¤ì›€</li> </ul> </li> <li><strong>í•´ê²°ì±…</strong> <ul> <li>í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì§ì ‘ í›ˆë ¨ <ul> <li>ë‹¨ìˆœí•œ í™˜ê²½ì— í•œì •ë¨</li> <li>ë³µì¡í•œ í™˜ê²½ì—ì„œëŠ” ë¹„íš¨ìœ¨ì  ë° ì•ˆì „ ë¬¸ì œ ë°œìƒ</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="3-sim-to-real-ê°•í™”-í•™ìŠµ-sim-to-real-reinforcement-learning">3. Sim-to-Real ê°•í™” í•™ìŠµ (Sim-to-Real Reinforcement Learning)</h3> <ul> <li><strong>Domain Randomization</strong> <ul> <li>ë‹¤ì–‘í•œ í™˜ê²½ ë§¤ê°œë³€ìˆ˜ ë° ì„¼ì„œ ë…¸ì´ì¦ˆ ì‚¬ìš©</li> <li>ê°•ê±´í•œ ì •ì±… í•™ìŠµ ê°€ëŠ¥</li> <li>ìµœì ì„±(Optimality) í¬ìƒ â†’ ê³¼ë„í•˜ê²Œ ë³´ìˆ˜ì ì¸ ì •ì±… ìƒì„±</li> </ul> </li> <li><strong>Simulation Accuracy Improvement</strong> <ul> <li>ëª¨í„° ëª¨ë¸ ê°œì„  <ul> <li>Tan et al.: ì„ í˜• í•¨ìˆ˜ë¡œ ëª¨í„° ë°ì´í„° í”¼íŒ…</li> <li>Hwangbo et al.: ì‹ ê²½ë§ìœ¼ë¡œ ì•¡ì¶”ì—ì´í„° ëª¨ë¸ ë§¤ê°œë³€ìˆ˜í™”</li> </ul> </li> </ul> </li> <li><strong>Limitations</strong> <ul> <li>ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ í•„ìš”</li> <li>ìƒˆë¡œìš´ í™˜ê²½ë§ˆë‹¤ ì¬ì¡°ì • í•„ìš”</li> </ul> </li> </ul> <hr/> <h3 id="4-ì‹œìŠ¤í…œ-ì‹ë³„-ë°-ì ì‘-system-identification-and-adaptation">4. ì‹œìŠ¤í…œ ì‹ë³„ ë° ì ì‘ (System Identification and Adaptation)</h3> <ul> <li><strong>Online System Identification</strong> <ul> <li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í›ˆë ¨ëœ ëª¨ë“ˆì„ í†µí•´ ë§¤ê°œë³€ìˆ˜ ì¶”ì •</li> <li>ì§„í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ì§ì ‘ ìµœì í™”</li> </ul> </li> <li><strong>Latent Embedding í™œìš©</strong> <ul> <li>ì €ì°¨ì› ì ì¬ ì„ë² ë”©ì„ í†µí•´ ì‹œìŠ¤í…œ ë§¤ê°œë³€ìˆ˜ í‘œí˜„</li> <li>ì‹¤ì œ í™˜ê²½ ë¡¤ì•„ì›ƒ ê¸°ë°˜ ìµœì í™” <ul> <li>ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ë°©ë²• ì‚¬ìš©</li> <li>ë² ì´ì§€ì•ˆ ìµœì í™” ì ìš©</li> <li>ë¬´ì‘ìœ„ íƒìƒ‰ í™œìš©</li> </ul> </li> </ul> </li> <li><strong>Meta-Learning</strong> <ul> <li>ë¹ ë¥¸ ì˜¨ë¼ì¸ ì ì‘ì„ ìœ„í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” í•™ìŠµ</li> </ul> </li> <li><strong>Limitations</strong> <ul> <li>ì—¬ëŸ¬ ì‹¤ì œ í™˜ê²½ ë¡¤ì•„ì›ƒ í•„ìš”</li> <li>í˜„ì‹¤ ì„¸ê³„ì—ì„œì˜ ìµœì í™” ë¹„ìš© ë° ì‹œê°„ ë¬¸ì œ</li> </ul> </li> </ul> <hr/> <h3 id="5-ìš”ì•½-ë°-ë¹„êµ">5. ìš”ì•½ ë° ë¹„êµ</h3> <ul> <li><strong>ì „í†µì  ì œì–´ ì ‘ê·¼ë²•:</strong> ì •í™•í•œ ëª¨ë¸ê³¼ ìˆ˜ì‘ì—… íŠœë‹ ìš”êµ¬</li> <li><strong>í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²•:</strong> ì‹œë®¬ë ˆì´ì…˜ ì„±ëŠ¥ ìš°ìˆ˜, í˜„ì‹¤ ì „ì´ ì–´ë ¤ì›€</li> <li><strong>Sim-to-Real ì ‘ê·¼ë²•:</strong> ë„ë©”ì¸ ë¬´ì‘ìœ„í™”ì™€ ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ í–¥ìƒ</li> <li><strong>ì‹œìŠ¤í…œ ì‹ë³„ ë° ì ì‘:</strong> ì €ì°¨ì› ì ì¬ ì„ë² ë”©ê³¼ ë©”íƒ€ í•™ìŠµ í™œìš©</li> </ul> <p><strong>ê²°ë¡ :</strong> RMAëŠ” ê¸°ì¡´ ì ‘ê·¼ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ë©° ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ê°•ê±´í•œ ë³´í–‰ ì •ì±…ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì ì‘ ê°€ëŠ¥í•˜ê²Œ í•¨.</p> <hr/> <h2 id="iii-rapid-motor-adaptation">III Rapid Motor Adaptation</h2> <hr/> <h3 id="1-base-policy">1. Base Policy</h3> <hr/> <h4 id="11-base-policy">1.1 Base Policy</h4> \[a_t = \pi(x_t, a_{t-1}, z_t)\] <ul> <li><strong>Input</strong> <ul> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> <li>extrinsics vector: \(z_t \in \mathbb{R}^8\) <ul> <li>form: \(z_t=\mu(e_t)\)</li> <li>environment vector \(e_t \in \mathbb{R}^{17}\)</li> <li>environment factor endoder: \(\mu\)</li> </ul> </li> </ul> </li> <li><strong>Ouput</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> <li>\(a_t\)ëŠ” 12ê°œì˜ ë¡œë´‡ ê´€ì ˆì´ ì›í•˜ëŠ” ìœ„ì¹˜ë¡œ ì„¤ì •ëœ ê°’ìœ¼ë¡œ, PD ì œì–´ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë¡œ ë³€í™˜</li> </ul> </li> </ul> <hr/> <h4 id="12-ì •ì±…-í•™ìŠµ">1.2 ì •ì±… í•™ìŠµ</h4> <ul> <li><strong>ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°:</strong> <ul> <li>ì •ì±… ë„¤íŠ¸ì›Œí¬(\(\pi\)) ë° ì¸ì½”ë”(\(\mu\)): ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ìœ¼ë¡œ êµ¬í˜„</li> </ul> </li> <li><strong>End-to-End í›ˆë ¨:</strong> Model-free RL</li> <li><strong>ëª©í‘œ:</strong> ì •ì±…ì˜ ê¸°ëŒ€ ë°˜í™˜(\(J(\pi)\)) ìµœëŒ€í™”</li> </ul> \[J(\pi) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[\sum_{t=0}^{T-1} \gamma^t r_t \right]\] <ul> <li><strong>ê²½ë¡œ(\(\tau\)):</strong> ì •ì±…(\(\pi\))ì„ ë”°ë¥´ëŠ” ì—ì´ì „íŠ¸ì˜ ìƒíƒœ, í–‰ë™, ë³´ìƒ ì‹œí€€ìŠ¤</li> </ul> <hr/> <h4 id="13-ìì—°ì -ì œì•½ì„-í†µí•œ-ì•ˆì •ì ì¸-ë³´í–‰-stable-gait-through-natural-constraints">1.3 ìì—°ì  ì œì•½ì„ í†µí•œ ì•ˆì •ì ì¸ ë³´í–‰ (Stable Gait through Natural Constraints)</h4> <ul> <li><strong>ìì—°ì  ì œì•½ í™œìš©:</strong> <ul> <li><strong>ìƒì²´ì—ë„ˆì§€í•™ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜:</strong> ì‘ì—… ìµœì†Œí™”, ì§€ë©´ ì¶©ê²© ê°ì†Œ ëª©í‘œ</li> <li><strong>ê³ ë¥´ì§€ ì•Šì€ ì§€í˜•ì—ì„œ í›ˆë ¨:</strong> ì¶”ê°€ ë³´ìƒ ì—†ì´ ë³´í–‰ ê°•ê±´ì„± í™•ë³´</li> </ul> </li> <li><strong>í•™ìŠµ í™˜ê²½:</strong> <ul> <li>ì¸ê³µì ì¸ ì‹œë®¬ë ˆì´ì…˜ ë…¸ì´ì¦ˆ ëŒ€ì‹  ìì—°ì  ì œì•½ ì ìš©</li> </ul> </li> <li><strong>ì •ì±… ì „ì´:</strong> <ul> <li>ë‹¨ìˆœ í™˜ê²½(ì½˜í¬ë¦¬íŠ¸, ë‚˜ë¬´ ë°”ë‹¥)ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì „ì´</li> <li>ì¶”ê°€ì ì¸ ë¯¸ì„¸ ì¡°ì •(finetuning) ë¶ˆí•„ìš”</li> </ul> </li> </ul> <hr/> <h4 id="14-ê¸°ì¡´-sim-to-real-ì ‘ê·¼ë²•ê³¼ì˜-ì°¨ì´ì ">1.4 ê¸°ì¡´ Sim-to-Real ì ‘ê·¼ë²•ê³¼ì˜ ì°¨ì´ì </h4> <ul> <li><strong>ê¸°ì¡´ ì ‘ê·¼ë²•:</strong> <ul> <li>ì‹œë®¬ë ˆì´ì…˜ê³¼ í˜„ì‹¤ ê°„ ë³´ì •(calibration) ìˆ˜í–‰ [51, 23]</li> <li>í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì •ì±… ë¯¸ì„¸ ì¡°ì •(finetuning) í•„ìš” [41]</li> </ul> </li> <li><strong>RMA ì ‘ê·¼ë²•:</strong> <ul> <li>ì ì‘ ëª¨ë“ˆ(Adaptation Module)ì„ í†µí•´ ë‹¨ìˆœ í™˜ê²½ì—ì„œ ë³µì¡í•œ ì§€í˜•ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥</li> </ul> </li> </ul> <hr/> <h4 id="15-ê°•í™”-í•™ìŠµ-ë³´ìƒ-rl-rewards">1.5 ê°•í™” í•™ìŠµ ë³´ìƒ (RL Rewards)</h4> <p>ë³´ìƒ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ê°€ ìµœëŒ€ 0.35 m/s ì†ë„ë¡œ ì „ì§„í•˜ë„ë¡ ì¥ë ¤ ë° ë¶ˆê·œì¹™í•˜ê³  ë¹„íš¨ìœ¨ì ì¸ ì›€ì§ì„ì— íŒ¨ë„í‹° ë¶€ì—¬.</p> <ul> <li><strong>ì„ í˜• ì†ë„: \(\mathbf{v}\)</strong></li> <li><strong>ìì„¸: \(\mathbf{\theta}\)</strong></li> <li><strong>ê°ì†ë„: \(\omega\)</strong></li> <li><strong>ê´€ì ˆ ê°ë„: \(\mathbf{q}\)</strong></li> <li><strong>ê´€ì ˆ ì†ë„: \(\dot{\mathbf{q}}\)</strong></li> <li><strong>ê´€ì ˆ í† í¬: \(\tau\)</strong></li> <li><strong>ë°œì˜ ì§€ë©´ ë°˜ë ¥: \(\mathbf{f}\)</strong></li> <li><strong>ë°œì˜ ì†ë„: \(\mathbf{v}_\mathbf{f}\)</strong></li> <li><strong>ë°œ ì ‘ì´‰ ì´ì§„ ë²¡í„°: \(\mathbf{g}\)</strong></li> </ul> <p>ì‹œê°„ \(t\)ì—ì„œì˜ ë³´ìƒì€ ë‹¤ìŒ ìš”ì†Œë“¤ì˜ í•©ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤:</p> <ol> <li><strong>ì „ì§„:</strong> \(\operatorname{min}(v^t_x, 0.35)\)</li> <li><strong>ì¸¡ë©´ ì´ë™ ë° íšŒì „:</strong> \(- \| v_{y}^{t} \|^2 - \| \omega_{\text{yaw}}^{t} \|^2\)</li> <li><strong>ì‘ì—…:</strong> \(- \| \boldsymbol{\tau}^{T} \cdot (\mathbf{q}^{t} - \mathbf{q}^{t-1}) \|\)</li> <li><strong>ì§€ë©´ ì¶©ê²©:</strong> \(- \| \mathbf{f}^{t} - \mathbf{f}^{t-1} \|^2\)</li> <li><strong>ë¶€ë“œëŸ¬ì›€:</strong> \(- \| \mathbf{\tau}^{t} - \mathbf{\tau}^{t-1} \|^2\)</li> <li><strong>í–‰ë™ í¬ê¸°:</strong> \(- \| \mathbf{a}^{t} \|^2\)</li> <li><strong>ê´€ì ˆ ì†ë„:</strong> \(- \| \dot{\mathbf{q}}^{t} \|^2\)</li> <li><strong>ìì„¸ ì•ˆì •ì„±:</strong> \(- \| \boldsymbol{\theta}^{t}_{\text{roll, pitch}} \|^2\)</li> <li><strong>Zì¶• ê°€ì†ë„:</strong> \(- \| v^t_z \|^2\)</li> <li><strong>ë°œ ë¯¸ë„ëŸ¬ì§:</strong> \(- \| \operatorname{diag}(\mathbf{g}^{t}) \cdot \mathbf{v}_{\mathbf{f}}^{t} \|^2\)</li> </ol> <p>ê° ë³´ìƒ í•­ëª©ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜: 20, 21, 0.002, 0.02, 0.001, 0.07, 0.002, 1.5, 2.0, 0.8.</p> <hr/> <h4 id="16-training-curriculum">1.6 Training Curriculum</h4> <p>ìœ„ ë³´ìƒ í•¨ìˆ˜ë¡œ í•™ìŠµ ì‹œ, ì›€ì§ì§ì— ëŒ€í•œ íŒ¨ë„í‹°ë¡œ ì¸í•´ ì œìë¦¬ì—ì„œ ë¨¸ë¬´ë¥´ëŠ” í˜„ìƒ ë°œìƒ ê°€ëŠ¥. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì „ëµ ì‚¬ìš©:</p> <ol> <li><strong>íŒ¨ë„í‹° ê³„ìˆ˜:</strong> ì´ˆê¸°ì—ëŠ” ë§¤ìš° ì‘ì€ íŒ¨ë„í‹° ê³„ìˆ˜ë¡œ ì‹œì‘, í›ˆë ¨ì´ ì§„í–‰ë¨ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ê³„ìˆ˜ ì¦ê°€.</li> <li><strong>í™˜ê²½ ë³€í™”:</strong> ì§ˆëŸ‰, ë§ˆì°°, ëª¨í„° í˜ ë“±ì˜ ë‚œì´ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€.</li> <li><strong>ì§€í˜• ë‚œì´ë„:</strong> ì§€í˜•ì— ëŒ€í•œ ë³„ë„ì˜ ì»¤ë¦¬í˜ëŸ¼ ì—†ìŒ, ê³ ì •ëœ ë‚œì´ë„ì—ì„œ ë¬´ì‘ìœ„ ì§€í˜• í”„ë¡œí•„ ìƒ˜í”Œë§.</li> </ol> <hr/> <h3 id="2-adaptation-module">2. Adaptation Module</h3> <hr/> <h4 id="21-adaptation-module">2.1 Adaptation Module</h4> \[\hat{z_t} = \phi(x_{t-k:t-1}, a_{t-k:t-1})\] <ul> <li><strong>ì—­í• </strong> <ul> <li>ì‹¤ì œ í™˜ê²½ì—ì„œ \(z_t\)ë¥¼ ì˜¨ë¼ì¸ ì¶”ì •</li> <li>privileged environment configuration, <strong>\(e_t\)</strong> ì—†ì´ ì‘ë™</li> </ul> </li> <li><strong>Input</strong> <ul> <li>history of robotâ€™s states: \(x_{t-k:t-1}\)</li> <li>history of robotâ€™s actions(\(a_{t-k:t-1}\))</li> </ul> </li> <li><strong>Output</strong> <ul> <li>predicted extrinsics vector: \(\hat{z_t}\)</li> </ul> </li> <li><strong>Hyperparameter</strong> <ul> <li>k=50 ì‚¬ìš© (ì•½ 0.5ì´ˆì— í•´ë‹¹)</li> </ul> </li> </ul> <hr/> <h4 id="22-ëª¨ë¸-í•™ìŠµ">2.2 ëª¨ë¸ í•™ìŠµ</h4> \[\operatorname{MSE}(\hat{z}_{t}, z_{t}) = \| \hat{z}_{t} - z_{t} \|^2\] <ul> <li><strong>ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°</strong>: <ul> <li><strong>1D CNN</strong> ì‚¬ìš©: ì‹œê°„ì  ìƒê´€ê´€ê³„ í¬ì°©</li> </ul> </li> </ul> <hr/> <h4 id="23-ë°ì´í„°-ìˆ˜ì§‘-ë°©ì‹">2.3 ë°ì´í„° ìˆ˜ì§‘ ë°©ì‹</h4> <ul> <li><strong>ë¬¸ì œì </strong> <ul> <li>Base policy(\(\pi\))ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°ì´í„°ì…‹ì€ ìµœì  ê²½ë¡œë§Œ í¬í•¨</li> <li>ì‹¤ì œ ë°°í¬ ì‹œ ë°œìƒí•  í¸ì°¨ë¥¼ ì¶©ë¶„íˆ ë‹¤ë£¨ì§€ ëª»í•¨</li> </ul> </li> <li><strong>í•´ê²°ì±…: On-Policy ë°ì´í„° ì‚¬ìš©</strong> <ul> <li>ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ <strong>Adaptation module(\(\phi\))</strong> ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘</li> <li>ìƒíƒœ-í–‰ë™ ì´ë ¥ê³¼ ëª©í‘œ ì ì¬ ë²¡í„°(\(z_t\)) ìŒìœ¼ë¡œ í•™ìŠµ ì§„í–‰</li> </ul> </li> <li><strong>í›ˆë ¨ ì ˆì°¨</strong> <ul> <li>Base policy(\(\pi\))ì„ \(\hat{z_t}\)ë¡œ ë¡¤ì•„ì›ƒ</li> <li>state action historyê³¼ ground truth \(z_t\) ìƒì„±</li> <li>ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ìˆ˜ë ´ ìœ ë„</li> </ul> </li> <li><strong>ê°•ê±´ì„± í™•ë³´ ë©”ì»¤ë‹ˆì¦˜</strong> <ul> <li>ëœë¤ ì´ˆê¸°í™”ëœ <strong>Adaptation module</strong>(\(\phi\)) ì‚¬ìš©</li> <li><strong>imperfect prediction</strong>(\(\hat{z_t}\)) ìˆ˜ìš©</li> <li>í›ˆë ¨ ì¤‘ ì¶©ë¶„í•œ íƒìƒ‰ ê²½ë¡œ í™•ë³´</li> </ul> </li> </ul> <hr/> <h3 id="3-asynchronous-deployment">3. Asynchronous Deployment</h3> <hr/> <h4 id="31-ë¹„ë™ê¸°ì‹-ë°°í¬">3.1 ë¹„ë™ê¸°ì‹ ë°°í¬</h4> <ul> <li><strong>í›ˆë ¨ ë° ë°°í¬</strong> <ul> <li>ì™„ì „í•œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í›ˆë ¨</li> <li>í˜„ì‹¤ ì„¸ê³„ë¡œ ì§ì ‘ ë°°í¬ (ìˆ˜ì • ë° ë¯¸ì„¸ ì¡°ì • ë¶ˆí•„ìš”)</li> </ul> </li> <li><strong>ë¹„ë™ê¸°ì‹ ì‹¤í–‰ êµ¬ì¡°</strong> <ul> <li>ë‘ í•˜ìœ„ ì‹œìŠ¤í…œì˜ ë¹„ë™ê¸°ì  ì‹¤í–‰</li> <li>ì„œë¡œ ë‹¤ë¥¸ ì£¼ê¸°ë¡œ ì‘ë™ â†’ ì˜¨ë³´ë“œ ì»´í“¨íŒ… ë¶€ë‹´ ìµœì†Œí™”</li> </ul> </li> </ul> <hr/> <h4 id="32-adaptation-module">3.2 Adaptation Module</h4> <ul> <li><strong>ì‘ë™ ì£¼ê¸°:</strong> 10Hz</li> <li><strong>ì…ë ¥:</strong> ìµœê·¼ 50 íƒ€ì„ ìŠ¤í…ì˜ ìƒíƒœ ë° í–‰ë™ ì´ë ¥</li> <li><strong>ì¶œë ¥:</strong> Extrinsics Vector, \(\hat{z_t}\)</li> <li><strong>íŠ¹ì§•:</strong> ë¹„êµì  ëŠë¦¬ê²Œ ì—…ë°ì´íŠ¸ë˜ë‚˜ ì„±ëŠ¥ì— ì˜í–¥ ì—†ìŒ</li> </ul> <hr/> <h4 id="33-base-policy">3.3 Base Policy</h4> <ul> <li><strong>ì‘ë™ ì£¼ê¸°:</strong> 100Hz</li> <li><strong>ì…ë ¥:</strong> <ul> <li>the most recent \(\hat{z_t}\) generated by the adaptation module</li> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> </ul> </li> <li><strong>Output</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> </ul> </li> <li><strong>íŠ¹ì§•:</strong> <ul> <li>ë¹ ë¥¸ ì£¼ê¸°ë¡œ ì‘ë™</li> <li>ì ì‘ ëª¨ë“ˆì˜ ë¹„ë™ê¸°ì  ì—…ë°ì´íŠ¸ ìˆ˜ìš© ê°€ëŠ¥</li> </ul> </li> </ul> <hr/> <h4 id="34-ë‹¨ì¼-ì •ì±…-ì„¤ê³„ì˜-í•œê³„ì ">3.4 ë‹¨ì¼ ì •ì±… ì„¤ê³„ì˜ í•œê³„ì </h4> <ul> <li><strong>ìƒíƒœ ë° í–‰ë™ ì´ë ¥ì„ ì§ì ‘ ì…ë ¥ë°›ëŠ” ë‹¨ì¼ ì •ì±… ì ‘ê·¼ë²•ì˜ ë¬¸ì œì </strong> <ul> <li><strong>ë¹„ìì—°ìŠ¤ëŸ¬ìš´ ë³´í–‰ íŒ¨í„´ ë°œìƒ</strong></li> <li><strong>ì‹œë®¬ë ˆì´ì…˜ ì„±ëŠ¥ ì €í•˜</strong></li> <li><strong>ì˜¨ë³´ë“œ ì»´í“¨íŒ… í•œê³„:</strong> 10Hzì—ì„œë§Œ ì‘ë™ ê°€ëŠ¥</li> <li><strong>ë¹„ë™ê¸°ì  ì„¤ê³„ ë¶ˆê°€ëŠ¥:</strong> í•˜ìœ„ ì‹œìŠ¤í…œ ê°„ ë™ê¸°í™” ë° ë³´ì • í•„ìš”</li> </ul> </li> </ul> <hr/> <h3 id="35-ë¹„ë™ê¸°-ì„¤ê³„ì˜-ì¥ì ">3.5 ë¹„ë™ê¸° ì„¤ê³„ì˜ ì¥ì </h3> <ul> <li>ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¬ê²Œ ë³€í•˜ëŠ” Extrinsics Vector(\(\hat{z_t}\))ì™€ ë¹ ë¥´ê²Œ ë³€í•˜ëŠ” ë¡œë´‡ ìƒíƒœ(\(x_t\))ì˜ ë¶„ë¦¬</li> <li>íš¨ìœ¨ì ì¸ ì˜¨ë³´ë“œ ì»´í“¨íŒ… ìì› í™œìš© ê°€ëŠ¥</li> <li>ë™ê¸°í™” ë° ì¶”ê°€ ë³´ì • ë¶ˆí•„ìš” â†’ ì›í™œí•œ Real-world ë°°í¬ ê°€ëŠ¥</li> </ul> <hr/> <h2 id="iv-experimental-setup">IV Experimental Setup</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table1-480.webp 480w,/assets/img/rma/table1-800.webp 800w,/assets/img/rma/table1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/table1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h3 id="1-environment-details">1. Environment Details</h3> <hr/> <h4 id="11-hardware-details">1.1 Hardware Details</h4> <ul> <li><strong>ë¡œë´‡ í”Œë«í¼:</strong> Unitreeì˜ A1 ë¡œë´‡</li> <li><strong>íŠ¹ì§•:</strong> <ul> <li>ì¤‘í˜• í¬ê¸°, ì €ë¹„ìš© ì‚¬ì¡± ë³´í–‰ ë¡œë´‡</li> <li><strong>ììœ ë„:</strong> ì´ 18ììœ ë„ (12ììœ ë„ëŠ” ì•¡ì¶”ì—ì´í„° ì‘ë™, ë‹¤ë¦¬ë‹¹ 3ê°œ ëª¨í„°)</li> <li><strong>ë¬´ê²Œ:</strong> ì•½ 12kg</li> </ul> </li> <li><strong>ì„¼ì„œ ì…ë ¥:</strong> <ul> <li><strong>ëª¨í„° ì¸ì½”ë”:</strong> ê´€ì ˆ ìœ„ì¹˜ ë° ì†ë„ ì¸¡ì •</li> <li><strong>IMU ì„¼ì„œ:</strong> ë¡¤(Roll), í”¼ì¹˜(Pitch) ì¸¡ì •</li> <li><strong>ë°œ ì„¼ì„œ:</strong> ì´ì§„í™”ëœ ë°œ ì ‘ì´‰ ìƒíƒœ</li> </ul> </li> <li><strong>ì œì–´ ë°©ì‹:</strong> <ul> <li><strong>ê´€ì ˆ ìœ„ì¹˜ ì œì–´ ì‚¬ìš©</strong></li> <li><strong>PD ì»¨íŠ¸ë¡¤ëŸ¬ë¡œ í† í¬ ë³€í™˜</strong> <ul> <li>Gain: \(K_p = 55, K_d = 0.8\)</li> </ul> </li> </ul> </li> </ul> <hr/> <h4 id="12-simulation-setup">1.2 Simulation Setup</h4> <ul> <li><strong>ì‹œë®¬ë ˆì´í„°:</strong> RaiSim ì‚¬ìš©</li> <li><strong>ë¡œë´‡ ëª¨ë¸:</strong> Unitree A1 URDF íŒŒì¼ ì‚¬ìš©</li> <li><strong>ì§€í˜• ìƒì„±ê¸°:</strong> í”„ë™íƒˆ ì§€í˜• ìƒì„±ê¸° <ul> <li>fractal octaves: 2</li> <li>fractal lacunarity: 2.0</li> <li>fractal gain: 0.25</li> <li>Z-scale: 0.27</li> </ul> </li> <li><strong>ì—í”¼ì†Œë“œ ê¸¸ì´:</strong> ìµœëŒ€ 1000 steps <ul> <li>ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´: <ul> <li>ë†’ì´ &lt; 0.28m</li> <li>ë¡¤ ê°ë„ &gt; 0.4 radians</li> <li>í”¼ì¹˜ ê°ë„ &gt; 0.2 radians</li> </ul> </li> </ul> </li> <li><strong>ì œì–´ ì£¼íŒŒìˆ˜:</strong> 100 Hz</li> <li><strong>ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ ê°„ê²©:</strong> 0.025s = 1/40</li> </ul> <hr/> <h4 id="13-state-action-space">1.3 State-Action Space</h4> <ul> <li><strong>ìƒíƒœ ë²¡í„°(</strong>\(x_t \in \mathbb{R}^{30}\)<strong>)</strong> <ul> <li><strong>ê´€ì ˆ ìœ„ì¹˜:</strong> 12ê°œ ê°’</li> <li><strong>ê´€ì ˆ ì†ë„:</strong> 12ê°œ ê°’</li> <li><strong>ëª¸ì²´ ë¡¤ ë° í”¼ì¹˜:</strong> 2ê°œ ê°’</li> <li><strong>ë°œ ì ‘ì´‰ ìƒíƒœ:</strong> 4ê°œ ê°’</li> </ul> </li> <li><strong>í–‰ë™ ë²¡í„°(</strong>\(a \in \mathbb{R}^{12}\)<strong>)</strong> <ul> <li><strong>ê´€ì ˆ ëª©í‘œ ìœ„ì¹˜ ì˜ˆì¸¡:</strong> \(a = \hat{q} \in \mathbb{R}^{12}\)</li> <li><strong>í† í¬ ë³€í™˜:</strong> PD ì»¨íŠ¸ë¡¤ëŸ¬ ì‚¬ìš© <ul> <li>ìˆ˜ì‹: \(\tau = K_p (\hat{q} - q) + K_d (\hat{\dot{q}} - \dot{q})\)</li> </ul> </li> <li><strong>ì´ë“ ê°’:</strong> \(K_p\) ë° \(K_d\) (ìˆ˜ë™ ì„¤ì •)</li> <li><strong>ëª©í‘œ ê´€ì ˆ ì†ë„:</strong> \(\hat{\dot{q}} = 0\)</li> </ul> </li> </ul> <hr/> <h4 id="14-environmental-variations">1.4 Environmental Variations</h4> <ul> <li><strong>í™˜ê²½ ë²¡í„°(\(e_t \in \mathbb{R}^{17}\))</strong> <ul> <li><strong>ì§ˆëŸ‰ ë° ë¡œë´‡ ë‚´ ìœ„ì¹˜:</strong> 3ì°¨ì›</li> <li><strong>ëª¨í„° ê°•ë„:</strong> 12ì°¨ì›</li> <li><strong>ë§ˆì°° ê³„ìˆ˜:</strong> ìŠ¤ì¹¼ë¼ ê°’</li> <li><strong>ì§€í˜• ë†’ì´:</strong> ìŠ¤ì¹¼ë¼ ê°’</li> </ul> </li> <li><strong>ì§€í˜• ë†’ì´ ì¸¡ì • ë°©ë²•:</strong> <ul> <li>ê° ë°œ ì•„ë˜ ì§€í˜• ë†’ì´ ê°’ì„ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ë¡œ ì´ì‚°í™”</li> <li>ë„¤ ë°œ ì¤‘ ìµœëŒ€ê°’ ì‚¬ìš©</li> </ul> </li> <li><strong>ì§€í˜• í”„ë¡œíŒŒì¼ íŠ¹ì„±:</strong> <ul> <li>ê³ ì •ëœ ë‚œì´ë„</li> <li>ë¡œì»¬ ì§€í˜• ë†’ì´ì˜ ë™ì  ë³€í™”</li> </ul> </li> </ul> <hr/> <h3 id="2-training-details">2. Training Details</h3> <hr/> <h4 id="21-base-policy-and-environment-factor-encoder-architecture">2.1 Base Policy and Environment Factor Encoder Architecture</h4> <ul> <li><strong>Base Policy</strong> <ul> <li><strong>êµ¬ì¡°:</strong> 3-layer MLP</li> <li><strong>Hidden layer sizes:</strong> 128</li> </ul> </li> <li><strong>Environment Factor Encoder</strong> <ul> <li><strong>êµ¬ì¡°:</strong> 3ì¸µ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)</li> <li><strong>Hidden layer sizes:</strong> 256, 128</li> </ul> </li> </ul> <hr/> <h4 id="22-adaptation-module-architecture">2.2 Adaptation Module Architecture</h4> <ul> <li><strong>1ë‹¨ê³„: ìƒíƒœ ë° í–‰ë™ ì„ë² ë”©</strong> <ul> <li><strong>êµ¬ì¡°:</strong> 2-layer MLP</li> </ul> </li> <li><strong>2ë‹¨ê³„: ì‹œê°„ì  ìƒê´€ê´€ê³„ í•™ìŠµ</strong> <ul> <li><strong>êµ¬ì¡°:</strong> 3-layer 1D CNN</li> <li><strong>ì…ë ¥-ì¶œë ¥ íŠ¹ì„±:</strong> <ul> <li><strong>1ì¸µ:</strong> ì…ë ¥ ì±„ë„: 32, ì¶œë ¥ ì±„ë„: 32, ì»¤ë„ í¬ê¸°: 8, ìŠ¤íŠ¸ë¼ì´ë“œ: 4</li> <li><strong>2ì¸µ:</strong> ì…ë ¥ ì±„ë„: 32, ì¶œë ¥ ì±„ë„: 32, ì»¤ë„ í¬ê¸°: 5, ìŠ¤íŠ¸ë¼ì´ë“œ: 1</li> <li><strong>3ì¸µ:</strong> ì…ë ¥ ì±„ë„: 32, ì¶œë ¥ ì±„ë„: 32, ì»¤ë„ í¬ê¸°: 5, ìŠ¤íŠ¸ë¼ì´ë“œ: 1</li> </ul> </li> </ul> </li> <li><strong>3ë‹¨ê³„: ìµœì¢… ì˜ˆì¸¡</strong> <ul> <li><strong>êµ¬ì¡°</strong>: Linear projection</li> </ul> </li> </ul> <hr/> <h2 id="v-results-and-analysis">V Results and Analysis</h2> <p><strong>ë¹„êµ ëŒ€ìƒ:</strong></p> <ul> <li><strong>ì‹œë®¬ë ˆì´ì…˜:</strong> ì—¬ëŸ¬ ê¸°ì¤€ ëª¨ë¸(Table II)</li> <li><strong>í˜„ì‹¤ í™˜ê²½:</strong> ì œì¡°ì‚¬ ì œê³µ A1 ì»¨íŠ¸ë¡¤ëŸ¬(Figure 3)</li> <li><strong>ë‹¤ì–‘í•œ ì•¼ì™¸ ì§€í˜•:</strong> ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ RMA í…ŒìŠ¤íŠ¸(Figure 1)</li> </ul> <hr/> <h3 id="1-baselines">1. Baselines</h3> <ol> <li><strong>A1 ì»¨íŠ¸ë¡¤ëŸ¬ (A1 Controller)</strong> <ul> <li>ì œì¡°ì‚¬ ê¸°ë³¸ ì»¨íŠ¸ë¡¤ëŸ¬</li> <li>í˜ ê¸°ë°˜ ì œì–´ ë° ëª¨ë¸ ì˜ˆì¸¡ ì œì–´(MPC) ì‚¬ìš©</li> </ul> </li> <li><strong>Robustness through Domain Randomization</strong> <ul> <li>Extrinsics vector(\(z_t\)) ì—†ì´ í•™ìŠµëœ ê¸°ë³¸ ì •ì±…</li> <li>í›ˆë ¨ ë²”ìœ„ ë‚´ ë³€í™”ë¥¼ ê²¬ë””ë„ë¡ ì„¤ê³„</li> </ul> </li> <li><strong>Expert Adaptation Policy</strong> <ul> <li>ì‹œë®¬ë ˆì´ì…˜ì—ì„œ true extrinsics vector(\(z_t\)) ì‚¬ìš©</li> <li>RMAì˜ ì´ë¡ ì  ìƒí•œ ì„±ëŠ¥ ì œê³µ</li> </ul> </li> <li><strong>RMA w/o Adaptation</strong> <ul> <li>Adaptation module ì—†ì´ base policy ë‹¨ë… í‰ê°€</li> <li>Adaptation moduleì˜ ì¤‘ìš”ì„± ë¶„ì„</li> </ul> </li> <li><strong>System Identification</strong> <ul> <li>Extrinsics vector(\(\hat{z_t}\)) ëŒ€ì‹  system parameters \(\hat{e_t}\) ì§ì ‘ ì˜ˆì¸¡</li> </ul> </li> <li><strong>AWR (Advantage Weighted Regression for Domain Adaptation)</strong> <ul> <li>ì˜¤í”„ë¼ì¸ìœ¼ë¡œ Extrinsics vector(\(\hat{z_t}\)) ìµœì í™”</li> <li>ì‹¤ì œ í™˜ê²½ ë¡¤ì•„ì›ƒì„ ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì ì‘</li> </ul> </li> </ol> <p><strong>í•™ìŠµ ì¡°ê±´ í†µì¼:</strong></p> <ul> <li>ë™ì¼í•œ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ì‚¬ìš©</li> <li>ë™ì¼í•œ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©</li> <li>ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©</li> </ul> <hr/> <h3 id="2-metrics">2. Metrics</h3> <ol> <li><strong>Time-to-Fall (TTF):</strong> <ul> <li>ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´ë¡œ ë‚˜ëˆˆ <strong>ì‹œê°„ë‹¹ ì¶”ë½ ë¹„ìœ¨</strong></li> <li>0~1 ë²”ìœ„ ì •ê·œí™”</li> </ul> </li> <li><strong>í‰ê·  ì „ì§„ ë³´ìƒ (Average Forward Reward):</strong></li> <li><strong>ì„±ê³µë¥  (Success Rate):</strong></li> <li><strong>ì´ë™ ê±°ë¦¬ (Distance Covered):</strong></li> <li><strong>ì ì‘ì— í•„ìš”í•œ íƒìƒ‰ ìƒ˜í”Œ ìˆ˜ (Exploration Samples Needed for Adaptation):</strong></li> <li><strong>í† í¬ ì‚¬ìš©ëŸ‰ (Torque Applied):</strong></li> <li><strong>ë¶€ë“œëŸ¬ì›€ (Smoothness):</strong> í† í¬ ë¯¸ë¶„ ê°’</li> <li><strong>ì§€ë©´ ì¶©ê²© (Ground Impact):</strong> ë°œì´ ì§€ë©´ì— ë¯¸ì¹˜ëŠ” ì¶©ê²© ìˆ˜ì¤€</li> </ol> <hr/> <h3 id="3-indoor-experiments">3. Indoor Experiments</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig3-480.webp 480w,/assets/img/rma/fig3-800.webp 800w,/assets/img/rma/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>ë¹„êµ ëŒ€ìƒ:</strong> <ul> <li><strong>RMA (Rapid Motor Adaptation)</strong></li> <li><strong>A1 ê¸°ë³¸ ì»¨íŠ¸ë¡¤ëŸ¬</strong></li> <li><strong>ì ì‘ ëª¨ë“ˆ ì œê±°ëœ RMA (RMA w/o Adaptation)</strong></li> </ul> </li> <li><strong>ë¹„êµ ëª©ì :</strong> ë¡œë´‡ í•˜ë“œì›¨ì–´ ì†ìƒ ìµœì†Œí™”</li> <li><strong>ì‹¤í—˜ ì¡°ê±´:</strong> <ul> <li>ê° ë°©ë²•ë‹¹ 5íšŒ ì‹¤í—˜</li> <li>ì‹¬ê°í•œ ì‹¤íŒ¨ ë°œìƒ ì‹œ 2íšŒë§Œ ì§„í–‰ í›„ ì‹¤íŒ¨ë¡œ ê¸°ë¡</li> </ul> </li> <li><strong>í‰ê°€ ì§€í‘œ:</strong> <ul> <li>ì„±ê³µë¥  (Success Rate)</li> <li>ë‚™í•˜ ì‹œê°„ (TTF: Time-to-Fall)</li> <li>ì´ë™ ê±°ë¦¬ (Distance Covered)</li> </ul> </li> </ul> <hr/> <h4 id="31-setups">3.1 Setups</h4> <ol> <li><strong>n-kg í˜ì´ë¡œë“œ (n-kg Payload)</strong> <ul> <li><strong>ëª©í‘œ:</strong> n-kg í•˜ì¤‘ì„ ì‹£ê³  300cm ê±·ê¸°</li> </ul> </li> <li><strong>StepUp-n</strong> <ul> <li><strong>ëª©í‘œ:</strong> n-cm ë†’ì´ì˜ ê³„ë‹¨ ì˜¤ë¥´ê¸°</li> <li><strong>í‰ê°€ ì§€í‘œ:</strong> ì„±ê³µë¥ ë§Œ ê¸°ë¡</li> </ul> </li> <li><strong>Uneven Foam</strong> <ul> <li><strong>ëª©í‘œ:</strong> ì¤‘ì•™ì´ ì†Ÿì•„ìˆëŠ” ìŠ¤í€ì§€ ìœ„ 180cm ê±·ê¸°</li> </ul> </li> <li><strong>Mattress</strong> <ul> <li><strong>ëª©í‘œ:</strong> ë©”ëª¨ë¦¬í¼ ë§¤íŠ¸ë¦¬ìŠ¤ ìœ„ 60cm ê±·ê¸°</li> </ul> </li> <li><strong>StepDown-n</strong> <ul> <li><strong>ëª©í‘œ:</strong> n-cm ë†’ì´ì˜ ê³„ë‹¨ ë‚´ë ¤ì˜¤ê¸°</li> <li><strong>í‰ê°€ ì§€í‘œ:</strong> ì„±ê³µë¥ ë§Œ ê¸°ë¡</li> </ul> </li> <li><strong>Incline</strong> <ul> <li><strong>ëª©í‘œ:</strong> 6ë„ ê²½ì‚¬ë¡œ ì˜¤ë¥´ê¸°</li> </ul> </li> <li><strong>Oily Surface</strong> <ul> <li><strong>ëª©í‘œ:</strong> ë¯¸ë„ëŸ¬ìš´ ì˜¤ì¼ì´ ë¿Œë ¤ì§„ í‘œë©´ ê±´ë„ˆê¸°</li> </ul> </li> </ol> <hr/> <h4 id="32-results">3.2 Results</h4> <ul> <li><strong>RMAì˜ ì„±ëŠ¥:</strong> <ul> <li><strong>ëª¨ë“  í™˜ê²½ì—ì„œ ë†’ì€ ì„±ê³µë¥  ë‹¬ì„±</strong></li> <li><strong>A1 ì»¨íŠ¸ë¡¤ëŸ¬ ëŒ€ë¹„ ì›”ë“±í•œ ì„±ëŠ¥ ë°œíœ˜</strong></li> </ul> </li> <li><strong>ì ì‘ ëª¨ë“ˆì˜ ì¤‘ìš”ì„±:</strong> <ul> <li>ì ì‘ ëª¨ë“ˆ ë¹„í™œì„±í™” ì‹œ ì„±ëŠ¥ í¬ê²Œ ì €í•˜</li> <li>ë§ì€ ê³¼ì œì—ì„œ ë¬¸ì œ í•´ê²° ë¶ˆê°€</li> </ul> </li> <li><strong>A1 ì»¨íŠ¸ë¡¤ëŸ¬ì˜ í•œê³„:</strong> <ul> <li><strong>Uneven Foam:</strong> ë¶ˆì•ˆì •í•œ ì§€ì§€ëŒ€ì—ì„œ ë¶ˆì•ˆì •í•¨ ë°œìƒ</li> <li><strong>StepUp/StepDown:</strong> ë†’ì€ ë‹¨ì°¨ì—ì„œ ì‹¤íŒ¨ ë¹ˆë²ˆ</li> <li><strong>Payload:</strong> 5kg ì´ìƒì˜ í•˜ì¤‘ì—ì„œëŠ” ì²˜ì§ ë°œìƒ ë° ë‚™í•˜</li> </ul> </li> <li><strong>RMAì˜ ê°•ì :</strong> <ul> <li>ìµœëŒ€ 12kg (ë¡œë´‡ ì²´ì¤‘ì˜ 100%) í•˜ì¤‘ ìš´ë°˜ ì„±ê³µ</li> <li>ë†’ì´ë¥¼ ìœ ì§€í•˜ë©° ì•ˆì •ì  ë³´í–‰ ê°€ëŠ¥</li> </ul> </li> <li><strong>RMA w/o ì ì‘ ëª¨ë“ˆ:</strong> <ul> <li>ëŒ€ë¶€ë¶„ ë‚™í•˜í•˜ì§€ ì•ŠìŒ</li> <li>ì „ì§„ ì›€ì§ì„ì€ ê±°ì˜ ì—†ìŒ</li> </ul> </li> <li><strong>Oily Surface</strong> <ul> <li><strong>RMA:</strong> ì„±ê³µì ìœ¼ë¡œ ë¯¸ë„ëŸ¬ìš´ ì§€í˜• í†µê³¼</li> <li><strong>RMA w/o Adaptation:</strong> ë‚˜ë¬´ ë°”ë‹¥ì—ì„œëŠ” ë³„ë„ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ ì„±ê³µì  ë³´í–‰ ê°€ëŠ¥</li> </ul> </li> </ul> <hr/> <h3 id="4-outdoor-experiments">4. Outdoor Experiments</h3> <ul> <li><strong>ëª©í‘œ:</strong> RMAì˜ ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ ì•¼ì™¸ í™˜ê²½ì—ì„œ ê²€ì¦</li> <li><strong>í…ŒìŠ¤íŠ¸ í™˜ê²½:</strong> <ul> <li>ëª¨ë˜ (Sand)</li> <li>ì§„í™ (Mud)</li> <li>í™ê¸¸ (Dirt)</li> <li>ë†’ì€ ì‹ë¬¼ ì§€ëŒ€ (Tall Vegetation)</li> <li>ë¤ë¶ˆ (Bush)</li> <li>ê³„ë‹¨ (Stairs)</li> <li>ê±´ì„¤ íê¸°ë¬¼ (Construction Debris)</li> </ul> </li> </ul> <hr/> <h4 id="41-results">4.1 Results</h4> <ol> <li><strong>ëª¨ë˜, ì§„í™, í™ê¸¸ (Sand, Mud, Dirt)</strong> <ul> <li><strong>ì„±ê³µë¥ :</strong> 100%</li> <li><strong>ë„ì „ ê³¼ì œ:</strong> <ul> <li>ë°œì´ ë¹ ì§€ê±°ë‚˜ ë‹¬ë¼ë¶™ëŠ” ë¬¸ì œ ë°œìƒ</li> <li>ë™ì  ë°œíŒ ì¡°ì • í•„ìš”</li> </ul> </li> </ul> </li> <li><strong>ë†’ì€ ì‹ë¬¼ ì§€ëŒ€ ë° ë¤ë¶ˆ (Tall Vegetation, Bush)</strong> <ul> <li><strong>ì„±ê³µë¥ :</strong> 100%</li> <li><strong>ë„ì „ ê³¼ì œ:</strong> <ul> <li>ë°œì´ ì¥ì• ë¬¼ì— ì–½í˜€ ë¶ˆì•ˆì • ë°œìƒ</li> <li>ì£¼ê¸°ì  ë°œíŒ ë¶ˆì•ˆì •ì„± í•´ê²° í•„ìš”</li> <li>ì¥ì• ë¬¼ì— ë§ì„œ ê°•ë ¥í•œ ì¶”ì§„ë ¥ í•„ìš”</li> </ul> </li> </ul> </li> <li><strong>í•˜ì´í‚¹ ê³„ë‹¨ (Stairs on Hiking Trail)</strong> <ul> <li><strong>ì„±ê³µë¥ :</strong> 70%</li> <li><strong>ë„ì „ ê³¼ì œ:</strong> <ul> <li>í›ˆë ¨ ì¤‘ ê³„ë‹¨ì„ ê²½í—˜í•˜ì§€ ëª»í•¨</li> <li>ë¶ˆê·œì¹™í•œ ë°œíŒê³¼ ë†’ë‚®ì´ ì¡°ì • í•„ìš”</li> </ul> </li> </ul> </li> <li><strong>ê±´ì„¤ íê¸°ë¬¼ (Construction Debris)</strong> <ul> <li><strong>í•˜ìœ„ ì‹¤í—˜:</strong> <ul> <li>ì§„í™ ë”ë¯¸ (Mud Pile): ì„±ê³µë¥  100%</li> <li>ì‹œë©˜íŠ¸ ë”ë¯¸ (Cement Pile): ì„±ê³µë¥  80%</li> <li>ìê°ˆ ë”ë¯¸ (Pebble Pile): ì„±ê³µë¥  80%</li> </ul> </li> <li><strong>ë„ì „ ê³¼ì œ:</strong> <ul> <li>ê¸‰ê²½ì‚¬ ë° ì¸¡ë©´ ê²½ì‚¬ë©´</li> <li>ë¶ˆê·œì¹™í•œ ë°œíŒê³¼ ë¬´ê²Œ ê· í˜• í•„ìš”</li> </ul> </li> </ul> </li> </ol> <hr/> <h3 id="5-simulation-results">5. Simulation Results</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table2-480.webp 480w,/assets/img/rma/table2-800.webp 800w,/assets/img/rma/table2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/table2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>ë¹„êµ ëŒ€ìƒ:</strong> ì—¬ëŸ¬ ê¸°ì¤€ ë°©ë²• (Table II)</li> <li><strong>í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¤ì •:</strong> <ul> <li><strong>í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë§¤ê°œë³€ìˆ˜:</strong> Table Iì— ë”°ë¼ ìƒ˜í”Œë§</li> <li><strong>ì¬ìƒ˜í”Œë§ í™•ë¥ :</strong> <ul> <li>í›ˆë ¨: ìŠ¤í…ë‹¹ 0.004</li> <li>í…ŒìŠ¤íŠ¸: ìŠ¤í…ë‹¹ 0.01</li> </ul> </li> </ul> </li> <li><strong>í‰ê°€ ë°©ë²•:</strong> <ul> <li><strong>ì •ì±… ì´ˆê¸°í™”:</strong> ë¬´ì‘ìœ„ë¡œ 3íšŒ ì´ˆê¸°í™”ëœ ì •ì±… ì‚¬ìš©</li> <li><strong>ì—í”¼ì†Œë“œ ìˆ˜:</strong> ì´ˆê¸°í™”ë‹¹ 1000íšŒ ì—í”¼ì†Œë“œ ì‹¤í–‰</li> </ul> </li> <li><strong>ì„±ëŠ¥ í‰ê°€:</strong> í‰ê· ê°’ ë„ì¶œ</li> </ul> <hr/> <h4 id="51-results">5.1 Results</h4> <ol> <li><strong>AWR (Advantage Weighted Regression)</strong> <ul> <li><strong>ì ì‘ ì†ë„ ì €í•˜:</strong> ë³€í™”í•˜ëŠ” í™˜ê²½ì— ëŠë¦° ì ì‘</li> <li><strong>ì„±ëŠ¥ ì €í•˜:</strong> ì§€ì†ì ì¸ í™˜ê²½ ë³€í™”ì— ì·¨ì•½</li> </ul> </li> <li><strong>Robust (ë„ë©”ì¸ ë¬´ì‘ìœ„í™” ê¸°ë°˜ ê°•ê±´ì„±)</strong> <ul> <li><strong>í™˜ê²½ íŠ¹ì„± ë¬´ì‹œ:</strong> Extrinsics vector(\(z_t\)) ì‚¬ìš©í•˜ì§€ ì•ŠìŒ</li> <li><strong>ë³´ìˆ˜ì  ì •ì±…:</strong> ì„±ëŠ¥ ì €í•˜ ë°œìƒ</li> </ul> </li> <li><strong>System Identification (SysID)</strong> <ul> <li><strong>í™˜ê²½ ë§¤ê°œë³€ìˆ˜(\(e_t\)) ì¶”ì •ì˜ ì–´ë ¤ì›€:</strong> ëª…ì‹œì  ë§¤ê°œë³€ìˆ˜ ì¶”ì • ì–´ë ¤ì›€</li> <li><strong>ë¶ˆí•„ìš”ì„±:</strong> ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì„ ìœ„í•´ í•„ìˆ˜ì ì´ì§€ ì•ŠìŒ</li> </ul> </li> <li><strong>RMA w/o Adaptation (ì ì‘ ëª¨ë“ˆ ë¯¸ì‚¬ìš© RMA)</strong> <ul> <li><strong>ì„±ëŠ¥ ê¸‰ê°:</strong> ì ì‘ ëª¨ë“ˆ ì—†ì´ëŠ” ì„±ëŠ¥ ì €í•˜ ì‹¬ê°</li> </ul> </li> </ol> <hr/> <h3 id="6-adaptation-analysis">6. Adaptation Analysis</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig4-480.webp 480w,/assets/img/rma/fig4-800.webp 800w,/assets/img/rma/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>ëª©í‘œ:</strong> ë¯¸ë„ëŸ¬ìš´ í‘œë©´ì—ì„œ RMAì˜ ë³´í–‰ íŒ¨í„´, í† í¬ í”„ë¡œíŒŒì¼, Extrinsics vector(\(\hat{z_t}\)) ë¶„ì„</li> <li><strong>ì‹¤í—˜ ì¡°ê±´:</strong> <ul> <li><strong>í‘œë©´:</strong> í”Œë¼ìŠ¤í‹± ë°”ë‹¥ì— ì˜¤ì¼ ë„í¬</li> <li><strong>ë¡œë´‡ ë°œ:</strong> í”Œë¼ìŠ¤í‹±ìœ¼ë¡œ ë®ìŒ</li> </ul> </li> </ul> <hr/> <h4 id="61-results">6.1 Results</h4> <ul> <li><strong>ì„±ê³µë¥ :</strong> 90%</li> <li><strong>ë¶„ì„ í•­ëª©:</strong> <ul> <li>ë¬´ë¦ í† í¬ í”„ë¡œíŒŒì¼ (Torque Profile of Knee)</li> <li>ë³´í–‰ íŒ¨í„´ (Gait Pattern)</li> <li>Extrinsics vector(\(\hat{z_t}\)) ë¶„ì„: 1ë²ˆì§¸ ë° 5ë²ˆì§¸ ì„±ë¶„ì˜ ì¤‘ìœ„ìˆ˜ í•„í„°ë§ëœ ê°’</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Robotics"/><category term="Robotics,"/><category term="Reinforcement-Learning"/><summary type="html"><![CDATA[RMA: Rapid Motor Adaptation for Legged Robots]]></summary></entry><entry><title type="html">Quantum Virtual Link Generation via Reinforcement Learning</title><link href="https://optreal.github.io/blog/2024/quantum_vl/" rel="alternate" type="text/html" title="Quantum Virtual Link Generation via Reinforcement Learning"/><published>2024-12-22T00:00:00+00:00</published><updated>2024-12-22T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/quantum_vl</id><content type="html" xml:base="https://optreal.github.io/blog/2024/quantum_vl/"><![CDATA[<h2 id="abstract">Abstract</h2> <ul> <li>Quantum networks leverage quantum entanglement as a fundamental building block.</li> <li>When two qubits are entangled, their states exhibit non-classical correlations, enabling novel applications such as quantum key distribution and distributed quantum computing, which are not possible with classical communication.</li> <li>However, <u>quantum entanglement is a probabilistic process heavily dependent on the characteristics of the involved devices</u>, such as optical fibers, lasers, and quantum memories.</li> <li>Managing this process to maintain entanglement with high quality for as long as possible is a <strong>stochastic control problem</strong>.</li> <li>This process can be modeled as a MDP and solved using the RL framework.</li> <li>In this work, we employ RL to develop an <strong>entanglement management policy</strong> that surpasses the current State-of-the-Art policies, particularly in scenarios where precise models of the quantum devices are unavailable.</li> <li>Reference: <a class="citation" href="#a10207249">(Aparicio-Pardo et al., 2023)</a></li> </ul> <hr/> <h2 id="introduction">Introduction</h2> <h3 id="1-ì–‘ì-ì¸í„°ë„·ì˜-ë“±ì¥">1. ì–‘ì ì¸í„°ë„·ì˜ ë“±ì¥</h3> <ul> <li>ìµœê·¼ <strong>ì–‘ì ë¬¼ë¦¬í•™ ì›ë¦¬(quantum physics principles)</strong>ê°€ ì»´í“¨í„° ë„¤íŠ¸ì›Œí¬ì— ì ìš©ë˜ë©° ì—°êµ¬ì™€ ì‚°ì—… ë¶„ì•¼ì—ì„œ ì£¼ëª©ë°›ê³  ìˆìŒ</li> <li><strong>IETF(Internet Engineering Task Force)</strong>ê°€ ì œì•ˆí•œ <strong>ì–‘ì ì¸í„°ë„·(Quantum Internet)</strong>ì˜ í‘œì¤€í™” ì‹œë„ê°€ ì´ë¥¼ ì¦ëª… <a class="citation" href="#rfc9340">(Kozlowski et al., 2023)</a>, <a class="citation" href="#rfc9583">(Wang et al., 2024)</a></li> <li><strong>ì–‘ì ì–½í˜(quantum entanglement)</strong>ì€ ì–‘ì í†µì‹ (Quantum Communication)ì„ ìœ„í•œ ê¸°ë³¸ ìì› <ul> <li>ì´ë¥¼ í†µí•´ <strong>ì–‘ì ì•”í˜¸ í‚¤ ë¶„ë°°(quantum key distribution)</strong>ì™€ <strong>ë¶„ì‚° ì–‘ì ì»´í“¨íŒ…(distributed quantum computing)</strong>ê³¼ ê°™ì€ ì‘ìš© ì‹¤í˜„ ê°€ëŠ¥</li> </ul> </li> </ul> <h3 id="2-ì–‘ì-ì–½í˜ì˜-íŠ¹ì„±ê³¼-ë¬¸ì œ">2. ì–‘ì ì–½í˜ì˜ íŠ¹ì„±ê³¼ ë¬¸ì œ</h3> <ul> <li>ì–‘ì ì–½í˜ì€ <strong>í™•ë¥ ì  ê³¼ì •(probabilistic process)</strong>ìœ¼ë¡œ, ê´€ë ¨ í†µì‹  ì¥ì¹˜(ê´‘ì„¬ìœ (optical fiber), ë ˆì´ì €(laser), ì–‘ì ë©”ëª¨ë¦¬(quantum memory) ë“±)ì˜ íŠ¹ì„±ì— í¬ê²Œ ì˜ì¡´</li> <li>ì–½í˜ ê´€ë¦¬ëŠ” <strong>í™•ë¥  ì œì–´ ë¬¸ì œ(stochastic control problem)</strong>ë¡œ, ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(Markov Decision Process, MDP)**ìœ¼ë¡œ ê³µì‹í™” ê°€ëŠ¥</li> <li>ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‘ ì›ê²© í†µì‹  ë…¸ë“œ(remote communication nodes) ê°„ì˜ ì–½í˜ì„ ì„¤ì •í•  ë•Œ DRLì˜ ì ìš© ê°€ëŠ¥ì„± ì¡°ì‚¬</li> </ul> <h3 id="3-ì–‘ì-ë¹„íŠ¸qubitì™€-ì–½í˜entanglement">3. <strong>ì–‘ì ë¹„íŠ¸(Qubit)ì™€ ì–½í˜(Entanglement)</strong></h3> <ul> <li><strong>ì–‘ì ë¹„íŠ¸(Qubit)</strong>ëŠ” ê³ ì „ ë¹„íŠ¸(classical bit)ì˜ ì–‘ìì  ëŒ€ì‘ë¬¼ <ul> <li>ê³ ì „ ë¹„íŠ¸ëŠ” â€œ0â€ ë˜ëŠ” â€œ1â€ì˜ ìƒíƒœë§Œ ê°€ì§€ì§€ë§Œ, ì–‘ì ë¹„íŠ¸ëŠ” ë‘ ìƒíƒœì˜ <strong>ì¤‘ì²©(superposition)</strong> ìƒíƒœë¥¼ ê°€ì§</li> <li>ì¸¡ì • í›„ í™•ë¥ ì— ë”°ë¼ â€œ0â€ ë˜ëŠ” â€œ1â€ ìƒíƒœë¡œ ê²°ì •ë¨</li> </ul> </li> <li><strong>ì–½í˜(Entanglement)</strong>: <ul> <li>ë‘ ì–‘ì ë¹„íŠ¸ê°€ ì–½íˆë©´ ê° ìƒíƒœë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ìŒ</li> <li>í•œ ìª½ ë¹„íŠ¸ ìƒíƒœê°€ ë³€í•˜ë©´, ë¬¼ë¦¬ì  ê±°ë¦¬ì— ê´€ê³„ì—†ì´ ë‹¤ë¥¸ ë¹„íŠ¸ì˜ ìƒíƒœë„ í•¨ê»˜ ë³€í™”</li> <li>ì–½í˜ì€ ì–‘ì ì•”í˜¸ì™€ ë¶„ì‚° ì–‘ì ì»´í“¨íŒ… ê°™ì€ ë¹„ê³ ì „ì (non-classical) ì‘ìš©ì˜ í•µì‹¬ ìš”ì†Œ</li> </ul> </li> </ul> <h3 id="4-ì–‘ì-ë„¤íŠ¸ì›Œí¬quantum-network">4. <strong>ì–‘ì ë„¤íŠ¸ì›Œí¬(Quantum Network)</strong></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-12-22-quantum_vl/fig1-480.webp 480w,/assets/img/posts/2024-12-22-quantum_vl/fig1-800.webp 800w,/assets/img/posts/2024-12-22-quantum_vl/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-12-22-quantum_vl/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>ì–‘ì ë„¤íŠ¸ì›Œí¬ëŠ” ì–½í˜ ìƒíƒœë¥¼ ë¶„ë°°í•˜ê³  <strong>ì–‘ì ë¹„íŠ¸(Qubit)</strong>ë¥¼ êµí™˜í•  ìˆ˜ ìˆëŠ” ë…¸ë“œ(node)ë“¤ë¡œ êµ¬ì„±. <ul> <li>ë…¸ë“œë“¤ì€ <strong>ê´‘ì„¬ìœ (optical fiber)</strong> ë˜ëŠ” <strong>ìœ„ì„± ë ˆì´ì € ë§í¬(satellite laser link)</strong>ë¡œ ì—°ê²°.</li> </ul> </li> <li>ì–½í˜ ìƒíƒœ ì„¤ì •: <ul> <li>ë‘ ì¸ì ‘ ë…¸ë“œì— ìœ„ì¹˜í•œ ì–‘ì ë¹„íŠ¸ ê°„ ì–½í˜ì€ <strong>ê¸°ë³¸ ë§í¬(elementary link)</strong>ë¥¼ êµ¬ì„±.</li> <li>ë‘ ë…¸ë“œ ê°„ ì–½í˜ ì„±ê³µ í™•ë¥ (\(P_{e_{i,j}}\))ì€ ê±°ë¦¬ ì¦ê°€ì— ë”°ë¼ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ <ul> <li>Short-distance entanglements (like A-B, in Fig. 1) are more likely to succeed than long-distance entanglements (like A-C, in Fig. 1)</li> </ul> </li> </ul> </li> <li><strong>ê°€ìƒ ë§í¬(Virtual Link)</strong> ìƒì„±: <ul> <li><strong><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜(Entanglement Swapping)</code></strong>ì„ í†µí•´ ë‘ ê°œì˜ ê¸°ë³¸ ë§í¬ë¥¼ ê²°í•© <ul> <li>ì˜ˆ) ê¸°ë³¸ ë§í¬ A-Bì™€ B-Cë¥¼ ì†Œë¹„í•˜ì—¬ A-Cë¼ëŠ” ì¥ê±°ë¦¬ ê°€ìƒ ë§í¬ ìƒì„±</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì„ ìˆ˜í–‰í•˜ëŠ” ì¤‘ê°„ ë…¸ë“œëŠ” <strong>ì–‘ì ë¦¬í”¼í„°(Quantum Repeater)</strong></li> <li>ì–‘ì ë¦¬í”¼í„°ëŠ” ê¸°ë³¸ ë§í¬ë“¤ì„ <strong>ì–‘ì ë©”ëª¨ë¦¬(Quantum Memory)</strong>ì— ì €ì¥ í›„ ì‚¬ìš© <ul> <li>ì˜ˆ) BëŠ” A-Bì™€ B-Cì˜ ê¸°ë³¸ ë§í¬ë¥¼ ì–‘ì ë©”ëª¨ë¦¬ì— ì €ì¥ í›„ ì‚¬ìš©</li> </ul> </li> </ul> </li> </ul> <h3 id="5-ì–‘ì-ë©”ëª¨ë¦¬-ìˆ˜ëª…quantum-memory-lifetimes">5. <strong>ì–‘ì ë©”ëª¨ë¦¬ ìˆ˜ëª…(Quantum Memory Lifetimes)</strong></h3> <ul> <li>ì–‘ì ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ì–½í˜ ìƒíƒœê°€ ì›ë˜ ìƒíƒœë¥¼ ìœ ì§€í•  í™•ë¥ (<strong>ë©”ëª¨ë¦¬ íš¨ìœ¨(memory efficiency), \(\eta\)</strong>)ì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ê°ì†Œ <ul> <li>ì´ ê³¼ì •ì€ <strong>ë°ì½”íˆì–´ëŸ°ìŠ¤(Decoherence)</strong>ë¡œ ì•Œë ¤ì§.</li> <li><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ (<strong>\(P_s\)</strong>)ì€ ê°€ì¥ ì˜¤ë˜ëœ ì–‘ì ë©”ëª¨ë¦¬ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ \(\eta\)ì— ì˜ì¡´</li> </ul> </li> </ul> <h3 id="6-ë³¸-ì—°êµ¬ì˜-ê¸°ì—¬">6. ë³¸ ì—°êµ¬ì˜ ê¸°ì—¬</h3> <ul> <li>ì–‘ì ê°€ìƒ ë§í¬ ìƒì„± ê³¼ì •ì„ <strong>ê³ ì „ì  MDP(Classical MDP)</strong>ë¡œ ëª¨ë¸ë§í•˜ê³ , DRL ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ìƒì„± ì •ì±…(policy)ì„ ë„ì¶œ</li> <li>ë³¸ ì—°êµ¬ëŠ” ê¸°ë³¸ ë§í¬ì˜ <strong>ë‚˜ì´(age)</strong>ë¥¼ ì¶”ì í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆ: <ul> <li>ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ë§í¬ ìƒì„± ì„±ê³µ ì‹œì ì˜ íƒ€ì„ìŠ¤íƒ¬í”„, ì¦‰ ë§í¬ì˜ ë‚˜ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ</li> </ul> </li> </ul> <table class="mbtablestyle table table-striped"> <tbody> <tr> <td><strong>ì‹¬ë³¼</strong></td> <td><strong>ì„¤ëª…</strong></td> <td><strong>ë¹„ê³ </strong></td> </tr> <tr> <td>\(P_{e_{i,j}}\)</td> <td>ë‘ ë…¸ë“œ ê°„ ì–½í˜ ì„±ê³µ í™•ë¥ </td> <td>Â </td> </tr> <tr> <td>\(\eta\)</td> <td>ì–‘ì ë©”ëª¨ë¦¬ íš¨ìœ¨</td> <td>ì–‘ì ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ì–½í˜ ìƒíƒœê°€ ì›ë˜ ìƒíƒœë¥¼ ìœ ì§€í•  í™•ë¥  (ì €ì¥ ì‹œê°„ì— ë”°ë¼ ê°ì†Œ, Mims ëª¨ë¸ ë”°ë¦„ <a class="citation" href="#Ortu2022a">(Ortu et al., 2022)</a>)</td> </tr> <tr> <td>\(P_s\)</td> <td><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ </td> <td>ê°€ì¥ ì˜¤ë˜ëœ ì–‘ì ë©”ëª¨ë¦¬ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨, ì¦‰, \(\eta\)ì— ì˜ì¡´</td> </tr> <tr> <td>\(t_c\)</td> <td>ì»·ì˜¤í”„ ì‹œê°„</td> <td>Â </td> </tr> </tbody> </table> <hr/> <h2 id="related-works">Related Works</h2> <h3 id="1-quantum-decision-processqdp">1. Quantum Decision Process(QDP)</h3> <ul> <li>Quantum Decision Process(QDP)ëŠ” MDPì˜ ì–‘ìì  ì¼ë°˜í™”ë¡œ, Khatriì˜ ë°•ì‚¬ ë…¼ë¬¸<a class="citation" href="#10.5555/AAI29111215">(Khatri, 2021)</a>ì—ì„œ ì œì•ˆë¨.</li> <li>ì–‘ì ì»´í“¨í„°ì˜ ì‚¬ìš©ì„ ì „ì œë¡œ í•˜ëŠ” QDPì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ -ì–‘ì ìƒíƒœ(Quantum State) - QDPì—ì„œëŠ” ìƒíƒœë¥¼ <strong>ì–‘ì ìƒíƒœ(Quantum State)</strong>ë¡œ í‘œí˜„ - ì–‘ì ìƒíƒœëŠ” ìƒíƒœ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚˜ë©°, ì¤‘ì²©(Superposition)ê³¼ ì–½í˜(Entanglement)ì„ í¬í•¨í•  ìˆ˜ ìˆìŒ <ul> <li>ì–‘ì í–‰ë™(Quantum Action) <ul> <li>QDPì˜ í–‰ë™ì€ ê³ ì „ì  ì•¡ì…˜ì´ ì•„ë‹Œ <strong>ì–‘ì ì—°ì‚°ì(Quantum Operator)</strong>ë¡œ í‘œí˜„ë¨</li> <li>ì´ëŠ” ìœ ë‹ˆí„°ë¦¬ ì—°ì‚°ì´ë‚˜ ì¸¡ì •ê³¼ ê°™ì€ ì–‘ìì—­í•™ì  ì—°ì‚°ìœ¼ë¡œ êµ¬í˜„ë˜ë©°, ìƒíƒœì— ì‘ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ìƒì„±í•¨</li> </ul> </li> <li>ì–‘ì ìƒíƒœ ì „ì´ <ul> <li>ì „ì´ í™•ë¥ ì´ ê³ ì „ì  í™•ë¥  ë¶„í¬ ëŒ€ì‹  ì–‘ì ì—°ì‚°ì— ì˜í•´ ê²°ì •ë¨</li> <li>ì–‘ì ì—°ì‚°ì€ ìƒíƒœë¥¼ ë³€í™˜í•˜ë©´ì„œ ê³ ì „ì  ì‹œìŠ¤í…œê³¼ ë‹¬ë¦¬ ë¹„ì„ í˜•ì ì´ê³  ì¤‘ì²©ëœ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ</li> </ul> </li> <li>ë³´ìƒ í•¨ìˆ˜(Reward Function) <ul> <li>QDPì˜ ë³´ìƒ í•¨ìˆ˜ëŠ” ì–‘ì ìƒíƒœë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ì˜ë˜ë©°, íŠ¹ì • ì–‘ì ìƒíƒœ ë˜ëŠ” ê²°ê³¼ë¥¼ ì–»ëŠ” ê²ƒì— ëŒ€í•œ ê°€ì¹˜(value)ë¥¼ í‘œí˜„í•¨</li> <li>ë³´ìƒ í•¨ìˆ˜ëŠ” ìƒíƒœì™€ í–‰ë™ì˜ ì¡°í•©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ</li> </ul> </li> </ul> </li> </ul> <h3 id="2-ì´-ë…¼ë¬¸ì˜-ì ‘ê·¼-ë°©ì‹">2. ì´ ë…¼ë¬¸ì˜ ì ‘ê·¼ ë°©ì‹</h3> <ul> <li>ê¸°ì¡´ QDP ëª¨ë¸ê³¼ëŠ” ë‹¬ë¦¬, ì´ ë…¼ë¬¸ì€ ì¸¡ì •ëœ ë¬¼ë¦¬ì  ì†ì„±ìœ¼ë¡œ ê¸°ìˆ ëœ ìƒíƒœì™€ ê±°ì‹œì  ìˆ˜ì¤€ì˜ ì•¡ì…˜ìœ¼ë¡œ êµ¬ì„±ëœ <strong>í´ë˜ì‹ MDP</strong>ë¡œ ëª¨ë¸ë§í•¨.</li> <li>Khatriì˜ ì•„ì´ë””ì–´ ì¤‘ í˜„ì¬ ê¸°ìˆ  ìˆ˜ì¤€ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ê²ƒë“¤ì„ ë„ì…, ì–‘ì ì»´í“¨í„° ê°œë°œì„ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ì´ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ ì œì‹œí•¨.</li> </ul> <h3 id="3-ë…¼ë¬¸-ëª¨ë¸ë§-ëŒ€ìƒ">3. ë…¼ë¬¸ ëª¨ë¸ë§ ëŒ€ìƒ</h3> <ul> <li>ë…¼ë¬¸ì˜ ëŒ€ìƒì€ Khatri ë…¼ë¬¸ì˜ ë¶€ë¡ Dì—ì„œ ì œì‹œëœ <strong><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>(entanglement swapping)ì„ ì´ìš©í•œ ê°€ìƒ ë§í¬ ìƒì„±</strong></li> <li>ê°€ìƒ ë§í¬ ìƒì„±ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ë§¥ë½ì—ì„œ ì—°êµ¬ë¨: <ul> <li><strong>ì–‘ì ì¤‘ê³„ê¸°(Quantum Repeaters) ì²´ì¸</strong>ì—ì„œì˜ ì¥ê±°ë¦¬ ì–½í˜ ìƒì„±</li> <li><strong>ì–‘ì ì–½í˜ ë¼ìš°íŒ…(Quantum Entanglement Routing)</strong>ì—ì„œì˜ Mesh ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì–½í˜ ìƒì„±</li> </ul> </li> </ul> <h3 id="4-ê¸°ì¡´-ì—°êµ¬ì™€ì˜-ì°¨ì´ì ">4. ê¸°ì¡´ ì—°êµ¬ì™€ì˜ ì°¨ì´ì </h3> <ul> <li>ê¸°ì¡´ ì—°êµ¬ëŠ” ë§í¬ ìƒì„±ì˜ <strong>íˆìŠ¤í† ë¦¬(íƒ€ì„ìŠ¤íƒ¬í”„)</strong>ë¥¼ ë¬´ì‹œ</li> <li>ì¦‰, ê°€ìƒ ë§í¬ ìƒì„± ì‹œ ë¬´í•œ ë©”ëª¨ë¦¬ ì»·ì˜¤í”„ ì‹œê°„ ì •ì±…(infinity memory cutoff-time policy)ì„ ë”°ë¦„ <ul> <li>ì´ˆê¸° ë§í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì–½íŒ ì´í›„ ë‹¤ìŒ ë§í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì–½í ë•Œê¹Œì§€ ì´ˆê¸° ë§í¬ëŠ” ì›ë˜ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ìœ ì§€í•  í™•ë¥ ì„ 100% ë¡œ ì„¤ì •</li> <li>ì˜¤ë˜ëœ ë§í¬ì˜ ë” í° <code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code>ê°€ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ (\(P_s\))ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code> <ul> <li>íë¹„íŠ¸ ìƒíƒœê°€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì†ì‹¤ë˜ëŠ” ê³¼ì •</li> <li>ì›ì¸: ì–‘ì ë©”ëª¨ë¦¬ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ ì™„ë²½íˆ ê³ ë¦½ë  ìˆ˜ ì—†ê¸° ë•Œë¬¸</li> </ul> </li> </ul> <h3 id="5-ì´-ë…¼ë¬¸ì˜-ê°œì„ ì ">5. ì´ ë…¼ë¬¸ì˜ ê°œì„ ì </h3> <ul> <li>ê¸°ì¡´ ì ‘ê·¼ë²•ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ì—¬, íˆìŠ¤í† ë¦¬ì™€ <code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code> ì˜í–¥ì„ í¬í•¨í•œ MDP ëª¨ë¸ë§ì„ ì œì•ˆ.</li> </ul> <hr/> <h2 id="reinforcement-learning-for-virtual-link-generation">Reinforcement Learning for Virtual Link Generation</h2> <h3 id="1-ë¬¸ì œ-ì •ì˜-ê°€ìƒ-ë§í¬-ìƒì„±-ë¬¸ì œ">1. ë¬¸ì œ ì •ì˜: <code class="language-plaintext highlighter-rouge">ê°€ìƒ ë§í¬ ìƒì„± ë¬¸ì œ</code></h3> <ul> <li><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì„ í†µí•´ ë‘ ê°œì˜ ê¸°ë³¸ ë§í¬ë¡œë¶€í„° ê°€ìƒ ë§í¬ë¥¼ ìƒì„±í•  ë•Œ <strong><u>ì‹œê°„ë‹¹ ì„±ê³µì ì¸ `ì–½í˜ êµí™˜` íšŸìˆ˜(ê°€ìƒ ë§í¬ ìƒì„±ë¥ )ë¥¼ ìµœëŒ€í™”</u></strong>í•˜ëŠ” ë¬¸ì œ</li> <li><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ (<strong>\(P_s\)</strong>): <ul> <li>ë‘ ê°œì˜ ê¸°ë³¸ ë§í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì–´ì•¼ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì‹œë„ ê°€ëŠ¥</li> <li>ì²˜ìŒ ìƒì„±ëœ ê¸°ë³¸ ë§í¬ê°€ ì˜¤ë˜ë ìˆ˜ë¡ ì„±ê³µ í™•ë¥  ê°ì†Œ</li> </ul> </li> <li><strong>ì»·ì˜¤í”„ ì‹œê°„(</strong>\(t_c\)<strong>)</strong> <ul> <li><u>ì–½í˜ ìƒíƒœë¥¼ ìœ ì§€í•˜ë‹¤ê°€ íê¸°í•˜ëŠ” ì‹œì ì„ ê²°ì •í•˜ëŠ” ì‹œê°„ ì„ê³„ê°’</u></li> <li>ì–½í˜ í’ˆì§ˆ ê´€ë¦¬ <ul> <li>ì–½í˜ ìƒíƒœëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ <code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code> í˜„ìƒìœ¼ë¡œ ì¸í•´ ì ì°¨ í’ˆì§ˆì´ ì €í•˜ë¨</li> <li>í’ˆì§ˆì´ ë‚®ì•„ì§€ë©´ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì˜ ì„±ê³µ í™•ë¥ ì´ ê°ì†Œí•˜ë¯€ë¡œ, íŠ¹ì • ì‹œê°„ ì´í›„ í’ˆì§ˆ ì €í•˜ëœ ì–½í˜ ìƒíƒœë¥¼ íê¸°í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•¨</li> </ul> </li> <li>ë¦¬ì†ŒìŠ¤ ìµœì í™” <ul> <li>ì–½í˜ ìƒíƒœë¥¼ ì˜¤ë˜ ìœ ì§€í•˜ë©´ ì„±ê³µ í™•ë¥ ì€ ê°ì†Œí•˜ì§€ë§Œ, ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŒ</li> <li>ë°˜ëŒ€ë¡œ, ì–½í˜ ìƒíƒœë¥¼ íê¸°í•˜ê³  ìƒˆë¡œ ìƒì„±í•˜ë©´ ë†’ì€ ì„±ê³µ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ, ìƒì„± ê³¼ì •ì—ì„œ ë¹„ìš©ê³¼ ì‹œê°„ì´ ì†Œëª¨ë˜ê³  <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì‹œë„ë¥¼ ì§€ì—°ì‹œí‚´</li> </ul> </li> <li>ì–½í˜ íê¸°ì™€ ì¬ìƒì„±ì„ í†µí•´ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ (<strong>\(P_s\)</strong>)ì„ ë†’ì¼ ìˆ˜ ìˆìŒ</li> </ul> </li> <li>ì»·ì˜¤í”„ ì‹œê°„ ì„¤ì •ì˜ ì¤‘ìš”ì„±</li> </ul> <table class="mbtablestyle table table-striped"> <tbody> <tr> <td><strong>ì§§ì€ ì»·ì˜¤í”„ ì‹œê°„</strong></td> <td><strong>ê¸´ ì»·ì˜¤í”„ ì‹œê°„</strong></td> </tr> <tr> <td>ì–½í˜ ìƒíƒœë¥¼ ë¹ ë¥´ê²Œ íê¸°í•˜ê³  ìƒˆë¡œ ìƒì„±í•¨</td> <td>ì–½í˜ ìƒíƒœë¥¼ ì˜¤ë˜ ìœ ì§€í•˜ë©° ìƒˆë¡œìš´ ìƒì„± ë¹ˆë„ë¥¼ ì¤„ì„</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ ì€ ë†’ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë§í¬ ìƒì„±ì˜ ë°˜ë³µìœ¼ë¡œ ì¸í•´ ì „ì²´ ê³¼ì •ì´ ì§€ì—°ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§</td> <td>ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•˜ë©° ë¹ ë¥´ê²Œ êµí™˜ ì‹œë„ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆì§€ë§Œ, <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì˜ ì„±ê³µ í™•ë¥ ì€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ</td> </tr> <tr> <td>ë¦¬ì†ŒìŠ¤ ì†Œëª¨ ì¦ê°€</td> <td><code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code> ì˜í–¥ì´ í¬ë‹¤ë©´ êµí™˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§</td> </tr> </tbody> </table> <ul> <li>ì»·ì˜¤í”„ ì‹œê°„ ê²°ì •ì˜ ì–´ë ¤ì›€ <ul> <li>ì–½í˜ ìƒì„± ì„±ê³µ í™•ë¥ (\(P_{e_{i,j}}\))ì™€ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code> ì„±ê³µ í™•ë¥ (\(P_s\))ì€ ë‹¤ì–‘í•œ í™˜ê²½ì  ìš”ì¸(ì˜ˆ: ê´‘ì„¬ìœ  ê¸¸ì´, ì–‘ì ë©”ëª¨ë¦¬ íŠ¹ì„±)ì— ë”°ë¼ ë³€í™”</li> <li>ìµœì ì˜ ì»·ì˜¤í”„ ì‹œê°„ì„ ì„¤ì •í•˜ë ¤ë©´ ë‹¤ìŒì„ ê³ ë ¤í•´ì•¼ í•¨ <ul> <li>ì–½í˜ ìƒíƒœì˜ í’ˆì§ˆ ë³€í™” ì†ë„(<code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code> ì˜í–¥)</li> <li>ìƒˆë¡œìš´ ì–½í˜ ìƒì„±ì˜ ì„±ê³µ í™•ë¥  ë° ì†Œìš” ì‹œê°„</li> <li>ì „ì²´ ê°€ìƒ ë§í¬ ìƒì„±ë¥ ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì‹œê°„-ì„±ëŠ¥ ê· í˜•</li> </ul> </li> </ul> </li> </ul> <h3 id="2-ì–‘ì-ì–½í˜-ê´€ë¦¬-ë¬¸ì œì—-ëŒ€í•œ-mdp-ì •ì˜">2. ì–‘ì ì–½í˜ ê´€ë¦¬ ë¬¸ì œì— ëŒ€í•œ MDP ì •ì˜</h3> <ul> <li><strong>íƒ€ì„ ìŠ¤í…(time step) \(t\)</strong> : <ul> <li>ì œì–´ ì—ì´ì „íŠ¸ëŠ” í˜„ì¬ ìƒíƒœ \(s_t\)ë¥¼ ê´€ì°°í•œ í›„ íŠ¹ì • í–‰ë™ \(a_t\)ë¥¼ ì ìš©</li> <li>ì´ í–‰ë™ì˜ ì‹¤í–‰ì€ ìƒíƒœ ì „ì´ í™•ë¥  \(p(s_t, a_t, s_{t+1})\)ì— ì˜í•˜ì—¬ ìƒˆë¡œìš´ ìƒíƒœ \(s_{t+1}\)ë¡œì˜ ì „í™˜ì„ ìœ ë°œ</li> <li>ì—ì´ì „íŠ¸ëŠ” ìƒíƒœ-í–‰ë™ ìŒ \((s_t, a_t)\)ì˜ í‰ê°€ì— ë”°ë¼ ë³´ìƒ \(r_{t+1}\)ì„ ë°›ìŒ</li> <li>ì—ì´ì „íŠ¸ëŠ” ìƒˆë¡œìš´ ìƒíƒœ \(s_{t+1}\)ë¥¼ ê´€ì°°í•˜ê³  ì´ ê³¼ì •ì„ ë°˜ë³µ</li> </ul> </li> <li><strong>MDPì˜ ê²½ë¡œ(trajectory):</strong> <ul> <li>ì´ˆê¸° ìƒíƒœ \(s_0\)ì—ì„œ ì‹œì‘í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ê²½ë¡œë¥¼ ìƒì„±: \(s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, \dots\)</li> <li>ì´ ê²½ë¡œëŠ” ì—ì´ì „íŠ¸ ì •ì±…(policy) \(\pi(s, a)\)ì— ë”°ë¼ ìƒì„±</li> </ul> </li> <li><strong>ì‹œìŠ¤í…œ ìƒíƒœ:</strong> <ul> <li>ë‘ ê¸°ë³¸ ë§í¬(elementary links)ì™€ ê°€ìƒ ë§í¬(virtual link)ì˜ ìƒíƒœ(í–‰ë™)ë¥¼ ì—°ê²°(concatenate)í•˜ì—¬ êµ¬ì„±</li> <li>ê° ë§í¬ì˜ ìƒíƒœëŠ” ë²¡í„° \(s = [x, m]\)ë¡œ í‘œí˜„: <ul> <li>\(x\): ì–½í˜(entanglement) ìƒíƒœ <ul> <li>\(x = 1\): ì–½í˜ì´ í™œì„± ìƒíƒœ</li> <li>\(x = 0\): ì–½í˜ì´ ë¹„í™œì„± ìƒíƒœ</li> </ul> </li> <li>\(m\): ì–½í˜ì˜ ë‚˜ì´(age) <ul> <li>ì–½í˜ì´ ë¹„í™œì„± ìƒíƒœ(ì¦‰, \(x = 0\))ì¸ ê²½ìš° \(m = -1\)</li> </ul> </li> </ul> </li> </ul> </li> <li><strong>ê° ë§í¬ë³„ ê°€ëŠ¥í•œ í–‰ë™:</strong> <ul> <li>ê° ê¸°ë³¸ ë˜ëŠ” ê°€ìƒ ë§í¬ì— ëŒ€í•´ ë‹¤ìŒ ë‘ ê°€ì§€ í–‰ë™ ì¤‘ í•˜ë‚˜ ì„ íƒ: <ol> <li><strong>ì¬ì„¤ì •(reset):</strong> <ul> <li>ë§í¬ ìƒì„± ì¬ì‹œë„</li> <li>í™•ë¥  \(P_{e_{i,j}}\) ë° \(P_s\)ì— ë”°ë¼ í™•ë¥ ì  ì „í™˜ ìœ ë°œ</li> <li>í•´ë‹¹ ë§í¬ ìƒíƒœ \(s\)ì— ëŒ€í•´ <ul> <li>\(x\)ê°€ 1 ë˜ëŠ” 0ìœ¼ë¡œ ë³€ê²½</li> <li>\(x=1\)ì´ë¼ë©´ \(m\)ì€ 0ìœ¼ë¡œ ë³€ê²½</li> </ul> </li> </ul> </li> <li><strong>ëŒ€ê¸°(wait):</strong> <ul> <li>ë§í¬ ìƒì„±ì´ ì¬ì‹œë„ë˜ì§€ ì•Šìœ¼ë©°, í˜„ì¬ ë§í¬ ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€</li> <li>í•´ë‹¹ ë§í¬ ìƒíƒœ \(s\)ì— ëŒ€í•´ <ul> <li>\(x\)ëŠ” ë³€í™˜ì—†ê³ </li> <li>\(x=1\)ì´ë¼ë©´ \(m\) ì¦ê°€ ìœ ë°œ</li> </ul> </li> </ul> </li> </ol> </li> <li>ê°€ìƒ ë§í¬ì— ëŒ€í•œ <strong>ì¬ì„¤ì •(reset)</strong> <ul> <li>ë‘ ê¸°ë³¸ ë§í¬ì— ëŒ€í•˜ì—¬ <code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì„ í†µí•´ í•˜ë‚˜ì˜ ê°€ìƒ ë§í¬ ìƒì„±</li> </ul> </li> <li>í–‰ë™ ê³µê°„ í¬ê¸° \(2^3 = 8\)</li> </ul> </li> <li><strong>ë³´ìƒ(reward):</strong> <ul> <li><code class="language-plaintext highlighter-rouge">ì–½í˜ êµí™˜</code>ì´ ì„±ê³µí•˜ë©´ ë³´ìƒ ê°’ì€ 1</li> <li>ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë³´ìƒ ê°’ì€ 0</li> </ul> </li> </ul> <h3 id="3-ì–‘ì-í™˜ê²½-ëª¨ë¸ì—-ëŒ€í•œ-ê°€ì •">3. ì–‘ì í™˜ê²½ ëª¨ë¸ì— ëŒ€í•œ ê°€ì •</h3> <ul> <li>ì–½í˜ ìƒì„± ë° ì €ì¥ ë°©ì‹ <ul> <li>ì–½í˜ì€ <code class="language-plaintext highlighter-rouge">ì‹ í˜¸(herald)</code> ë°©ì‹ìœ¼ë¡œ ìƒì„±ë¨ <ul> <li>ì–½í˜ ìƒíƒœê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆì„ ë•Œ, í•´ë‹¹ ì„±ê³µ ì—¬ë¶€ë¥¼ ì™¸ë¶€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì‹ í˜¸(herald)ë¥¼ ì œê³µí•˜ëŠ” ì–½í˜ ìƒì„± ê¸°ìˆ ì„ ì§€ì¹­í•¨</li> <li>ì–‘ì ì–½í˜ ìƒì„± ê³¼ì •ì—ì„œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ</li> </ul> </li> <li>ì–½í˜ ìƒì„± ì‹œê°ê³¼ ì»·ì˜¤í”„ ì‹œê°„ \(t_c\)ì™€ì˜ ì—°ê´€ <ul> <li><code class="language-plaintext highlighter-rouge">ì‹ í˜¸(herald)</code> ë°©ì‹ì— ì˜í•´ ì»·ì˜¤í”„ ì‹œê°„ \(t_c\)ë¥¼ ì •ë°€í•˜ê²Œ ì¸¡ì •í•˜ì—¬ ì–½í˜ í’ˆì§ˆ ê´€ë¦¬ì— í™œìš© <ul> <li>ì–½í˜ ìƒì„± ì‹œì ë¶€í„° ì‹œê°„ì´ ê²½ê³¼í•¨ì— ë”°ë¼ ì–½í˜ ìƒíƒœëŠ” <code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code>ë¡œ ì¸í•´ í’ˆì§ˆì´ ì €í•˜ë˜ë¯€ë¡œ, ì»·ì˜¤í”„ ì‹œê°„ \(t_c\)ì´ ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ ê´€ë¦¬ì— ì¤‘ìš”í•¨</li> </ul> </li> </ul> </li> <li>ì–½í˜ ìƒíƒœ ì €ì¥ <ul> <li>ìƒì„±ëœ ì–½í˜ì€ <strong>ì–‘ì ë©”ëª¨ë¦¬</strong>ì— ì €ì¥ë˜ë©°, ê°€ìƒ ë§í¬ ìƒì„±ì„ ìœ„í•´ ì–½í˜ êµí™˜ì— ì‚¬ìš©ë¨</li> </ul> </li> <li>ì–½í˜ ìƒì„± ë°©ì‹ìœ¼ë¡œ <strong>DLCZ ê¸°ë°˜ í”„ë¡œí† ì½œ</strong> ì‚¬ìš© <ul> <li>ê´‘ì ë°©ì¶œê³¼ ê²€ì¶œì„ í†µí•´ ì–½í˜ ìƒì„± ì„±ê³µ ì—¬ë¶€ë¥¼ í™•ì¸</li> <li>ì„±ê³µ ì—¬ë¶€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–½í˜ ìƒíƒœë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê³  ì»·ì˜¤í”„ ì‹œê°„ \(t_c\) ê´€ë¦¬</li> </ul> </li> </ul> </li> <li>ì‹œê°„ ìŠ¬ë¡¯ ëª¨ë¸ <ul> <li>ì „ì²´ ì‹œê°„ì„ ìŠ¬ë¡¯ ë‹¨ìœ„ë¡œ ë‚˜ëˆ” <ul> <li>ìŠ¬ë¡¯ ê¸¸ì´: \(\frac{L_0}{v}\)</li> <li>\(L_0\): ê¸°ë³¸ ë§í¬(ê´‘ì„¬ìœ )ì˜ ê¸¸ì´</li> <li>\(v\): ê´‘ì„¬ìœ ì—ì„œ ë¹›ì˜ ì „íŒŒ ì†ë„</li> </ul> </li> <li>ìŠ¬ë¡¯ ê¸¸ì´ëŠ” ìƒˆë¡œìš´ ìƒíƒœë¥¼ ê´€ì°°í•˜ê³  ë§í¬ ìƒì„±ì„ ì¬ì‹œë„í•˜ëŠ”ë° ìš”êµ¬ë˜ëŠ” ì‹œê°„ë³´ë‹¤ëŠ” ê¸¸ì–´ì•¼ í•¨</li> </ul> </li> <li>ê¸°ë³¸ ë§í¬ ì–½í˜ ì„±ê³µ í™•ë¥  \(P_{e_{i,j}}\) <ul> <li>ê´‘ì„¬ìœ  ê±°ë¦¬ \(L_0\)ì— ë”°ë¼ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ <ul> <li><a class="citation" href="#sangouard2009">(Sangouard et al., 2009)</a>, (missing reference) ë˜ëŠ” <a class="citation" href="#Uphoff_2016">(Uphoff et al., 2016)</a>ì— ì œì‹œëœ ê²°ê³¼ì— ê¸°ë°˜í•¨</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Entanglement Generation Probability Model</p> </blockquote> <blockquote> <p>Once a heralded local entanglement is generated at each node, the two photons must be sent to the BSM and must be measured The entanglement generation probability for an elementary link \(e_{i,j}\) is equal to:</p> </blockquote> \[P_{e_{i,j}} = \frac{1}{2} \nu^o \left( p e^{-\frac{d_{i,j}}{2L_0}} \right)^2 = \frac{1}{2} \nu^o p^2 e^{-\frac{d_{i,j}}{L_0}}\] <blockquote> <p>where \(\nu^o\) denotes the optical BSM efficiency (assumed constant at each node, \(\nu^o=0.39\)), \(d_{i,j}\) denotes the length of elementary link \(e_{i,j}\), \(p\) indicates the success probability of detecting a pair of photons during the entanglement generation processÂ (\(pâ‰ˆ0.05\sim0.1\)), \(L_0\)ï¿¼denotes the attenuation length of the optical fiber (\(L_0 = 22 \, \mathrm{km}\)), and the term \(\frac{1}{2}\) accounts for the optical BSM capability of unambiguously identifying only two out of four bell states</p> </blockquote> <ul> <li>ì–‘ì ë©”ëª¨ë¦¬ íš¨ìœ¨ \(\eta\) <ul> <li>ì €ì¥ ì‹œê°„ì— ë”°ë¼ ê°ì†Œ <ul> <li><a class="citation" href="#Ortu2022a">(Ortu et al., 2022)</a>ì—ì„œ ì„¤ëª…ëœ Mims ëª¨ë¸ì— ë”°ë¦„</li> </ul> </li> <li>ì´ëŠ” ì–½í˜ êµí™˜ ì„±ê³µ í™•ë¥  \(P_s\)ì— ì£¼ìš” ì˜í–¥ì„ ë¯¸ì¹¨</li> </ul> </li> </ul> <blockquote> <p>Quantum Memory Efficiency Model</p> </blockquote> <blockquote> <p>The efficiency of a quantum memory, denoted as \(\eta(t)\) (the probability that the qubit remains in its original state at time \(t\)), can be expressed as:</p> </blockquote> \[\eta(t) = \eta(0) \cdot e^{-\frac{t}{T_o}}\] <blockquote> <p>where \(\eta(0)\) represents the probability that the qubit remains in its original state at time \(t=0\). Typically, \(\eta(0)=1\) for ideal systems but may be less than 1 in practical cases due to initialization imperfections. \(t\) is the time for which the qubit is stored in the quantum memory. \(T_o\) indicates the characteristic memory lifetime or decoherence time, representing the time scale over which the memory retains its original state.</p> </blockquote> <ul> <li>\(T_o\) (ì–‘ì ë©”ëª¨ë¦¬ ìˆ˜ëª…)ì˜ ì¼ë°˜ì ì¸ ê°’ <ul> <li>ë¬¼ë¦¬ì  ì‹œìŠ¤í…œë³„ <ul> <li>ì›ì ì§‘í•©(Atomic Ensembles): ìˆ˜ë°± ë°€ë¦¬ì´ˆ ~ ëª‡ ì´ˆ</li> <li>ì´ì˜¨ íŠ¸ë©(Ion Traps): ì§„ê³µ ìƒíƒœì—ì„œ 1ì´ˆ ~ 100ì´ˆ</li> <li>ê³ ì²´ ìƒíƒœ ì–‘ì ë©”ëª¨ë¦¬(Solid-State Quantum Memory): <ul> <li>í¬í† ë¥˜ ì´ì˜¨: 1~10ë°€ë¦¬ì´ˆ</li> <li>NV ì„¼í„°: ìµœëŒ€ ìˆ˜ë°± ë°€ë¦¬ì´ˆ</li> </ul> </li> <li>ì´ˆì „ë„ íë¹„íŠ¸(Superconducting Qubits): 10~500ë§ˆì´í¬ë¡œì´ˆ</li> </ul> </li> <li>\(T_o\)ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ <ul> <li>í™˜ê²½ ì¡ìŒ: \(T_o\) ê°ì†Œ</li> <li>ì˜¨ë„: ê·¹ì €ì˜¨ ì¡°ê±´ì—ì„œ \(T_o\) ì¦ê°€</li> <li>ì˜¤ë¥˜ ë³´ì •: ë‹¤ì´ë‚˜ë¯¹ ë””ì»¤í”Œë§(Dynamic Decoupling) ë“±ì˜ ê¸°ìˆ ë¡œ \(T_o\) ì—°ì¥ ê°€ëŠ¥</li> </ul> </li> <li>ì–‘ì ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ ëª©í‘œ <ul> <li>ì‹¤ìš©ì ì¸ ì–‘ì ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ìµœì†Œ \(T_o\)ì´ 1~10ì´ˆ ì´ìƒ í•„ìš”</li> </ul> </li> </ul> </li> <li>êµí™˜ ì„±ê³µ í™•ë¥  \(P_s\) <ul> <li>ì–½í˜ êµí™˜ ê³¼ì •ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ì˜ íš¨ìœ¨ \(\eta(t)\) ì— ë”°ë¼ ê²°ì •</li> </ul> </li> </ul> \[P_s = \eta(t_{oldest}) = \eta(0) \cdot e^{-\frac{t_{oldest}}{T_o}}\] <h3 id="4-ê°•í™”í•™ìŠµ-ê¸°ë°˜-ë¬¸ì œ-í•´ê²°-ì ‘ê·¼-ë°©ë²•">4. ê°•í™”í•™ìŠµ ê¸°ë°˜ ë¬¸ì œ í•´ê²° ì ‘ê·¼ ë°©ë²•</h3> <ul> <li>í™•ë¥  ëª¨ë¸ì˜ ë¶€ì¬ <ul> <li>ê¸°ë³¸ ë§í¬ ìƒì„± í™•ë¥  \(P_{e_{i,j}}\)ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ \(\eta\) (ë”°ë¼ì„œ, ì–½í˜ êµí™˜ ì„±ê³µ í™•ë¥  \(P_s\))ì— ëŒ€í•œ ì •í™•í•œ ëª¨ë¸ì„ ì•Œ ìˆ˜ ì—†ìŒ</li> <li>ë”°ë¼ì„œ, ë‹¹ì—°íˆ ìƒíƒœ ì „ì´ í™•ë¥  \(p(s_t, a_t, s_{t+1})\)ë„ ì•Œ ìˆ˜ ì—†ìŒ</li> </ul> </li> <li>ê¹Šì€ ê°•í™”í•™ìŠµ(DRL)ì˜ ì ìš© <ul> <li>ê°€ìƒ ë§í¬ ìƒì„±ë¥ ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì •ì±…ì„ ì°¾ê¸° ìœ„í•´ DRL ì ìš©</li> <li>ì´ ê³¼ì •ì—ì„œ \(\pi\)ë¥¼ ë”°ë¥´ëŠ” \(Q\)-value í•¨ìˆ˜ì¸ \(Q^\pi(s, a)\)ë¥¼ ì •ì˜í•˜ì—¬ í™œìš©</li> </ul> </li> <li>\(Q\)-value í•¨ìˆ˜ ì •ì˜ <ul> <li>\(Q^\pi(s, a)\): ì£¼ì–´ì§„ ìƒíƒœ-í–‰ë™ ìŒ \((s, a)\)ì— ëŒ€í•´ ì •ì±… \(\pi\)ë¥¼ ë”°ëì„ ë•Œ í• ì¸ëœ ëˆ„ì  ë³´ìƒ ê¸°ëŒ€ê°’ (expected discounted return)</li> <li>í• ì¸ëœ ëˆ„ì  ë³´ìƒì€ ê¶¤ì (trajectory)ì—ì„œ ë¯¸ë˜ ë³´ìƒì˜ í•©ìœ¼ë¡œ ì •ì˜ë¨: \(\sum_{k=0}^\infty \gamma^k r_{t+k+1}, \quad \gamma \in [0, 1]\)</li> <li>ì—ì´ì „íŠ¸ëŠ” \(Q\)-value í•¨ìˆ˜ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ìµœì  ì •ì±… \(\pi^*\)ë¥¼ íƒìƒ‰</li> </ul> </li> <li>ì •ì±…ì˜ êµ¬ì„± <ul> <li>ë³¸ ë¬¸ì œì—ì„œ ì •ì±…ì€ ê°€ì¥ ì ì ˆí•œ ì»·ì˜¤í”„ ì‹œê°„ \(t_c\)ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒ</li> </ul> </li> <li>DQN ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ í•™ìŠµ <ul> <li>ë³¸ ì—°êµ¬ì—ì„œëŠ” <strong>Deep Q-Network(DQN)</strong> ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">ê°€ìƒ ë§í¬ ìƒì„± ë¬¸ì œ</code>ì— ì í•©í•˜ë„ë¡ í•™ìŠµ ë£¨í‹´ì„ êµ¬ì„±</li> <li>í•™ìŠµ ë£¨í‹´ì€ ê¸°ì¡´ì˜ DQN ë°©ì‹ì„ ë”°ë¥´ë©´ì„œë„ ë³¸ ì—°êµ¬ ë¬¸ì œì— ë§ê²Œ ì¡°ì •ë¨</li> </ul> </li> </ul> <h2 id="experimental-results">Experimental Results</h2> <h3 id="1-ì‹¤í—˜-ì„¸íŒ…">1. ì‹¤í—˜ ì„¸íŒ…</h3> <ul> <li>ê¸°ë³¸ ì„¤ì • <ul> <li>ê´‘ì„¬ìœ  ê¸¸ì´ \(L_0 = 100 \, \mathrm{km}\), ë¹›ì˜ ì†ë„(light speed) \(200,000 \, \mathrm{km/s}\) <ul> <li>íƒ€ì„ ìŠ¬ë¡¯ ê¸¸ì´ (duration of a time step): \(0.5 \, \mathrm{ms}\)</li> </ul> </li> <li>ê´‘ì„¬ìœ  ì†ì‹¤ì€ \(0.2 \, \mathrm{dB/km}\) <ul> <li>ê´‘ì„¬ìœ ë¥¼ í†µí•´ ë¹›ì´ \(1 \, \mathrm{km}\)ë¥¼ ê°ˆ ë•Œ, ë¹›ì˜ ì„¸ê¸°ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“œëŠ” ì •ë„ë¥¼ $0.2$ë¡œ ê°€ì •</li> <li>ì´ ì¤„ì–´ë“œëŠ” ì •ë„ëŠ” ë¹› íŒŒì¥ì´ \(1,550 \, \mathrm{nm}\) ë•Œ ê°€ëŠ¥</li> </ul> </li> <li>Mims ë°©ì •ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ëª¨ë¸ë§ ìˆ˜í–‰</li> <li>zero-time efficiency ê°€ì • <ul> <li>ì–½í˜ ìƒì„±, ì „ì†¡, ë˜ëŠ” êµí™˜ ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ ì‹œê°„ ì§€ì—°(ì˜ˆ: ì‹ í˜¸ ì „íŒŒ ì‹œê°„, ì—°ì‚° ì‹œê°„ ë“±)ì´ ì—†ë‹¤ê³  ê°€ì •</li> <li>ì–‘ì ë©”ëª¨ë¦¬ì—ì„œ Swapì„ ìˆ˜í–‰í•  ë•Œ, ì‹œê°„ì´ ì†Œëª¨ë˜ì§€ ì•Šê³  ì¦‰ê°ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤ê³  ê°„ì£¼</li> </ul> </li> </ul> </li> <li>ì„¸ ê³„ì˜ ê³„ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ \(Q\)-value í•¨ìˆ˜ êµ¬í˜„ <ul> <li>ë‘ ê°œì˜ Dense Layerì—ëŠ” ê°ê° 32ê°œì˜ ë‰´ëŸ° ì¡´ì¬</li> <li>ê° ì¸µì€ \(\tanh\) í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©</li> <li>ì¶œë ¥ì¸µì—ëŠ” í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìœ¼ë©°, ì•¡ì…˜ ìˆ˜ì™€ ë™ì¼í•œ ë‰´ëŸ° ìˆ˜ë¥¼ ê°€ì§ <ul> <li>ì•¡ì…˜ ìˆ˜ëŠ” 8 (ë‘ ê°œì˜ ê¸°ë³¸ ë§í¬ì™€ í•˜ë‚˜ì˜ ê°€ìƒ ë§í¬ë¥¼ ê³ ë ¤)</li> </ul> </li> </ul> </li> <li>í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ <ul> <li>OpenAI Baselines ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” DQN ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©</li> <li>DRL ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹ ê²½ë§ í•™ìŠµ</li> </ul> </li> <li>ë³´ìƒ í‰ê°€ <ul> <li>ì—í”¼ì†Œë“œ ë³´ìƒì€ í•™ìŠµ ì—í”¼ì†Œë“œ ë™ì•ˆ ë°œìƒí•œ ëª¨ë“  ë‹¨ê³„ì˜ ë³´ìƒ \(r\)ì˜ í•©ìœ¼ë¡œ ê³„ì‚°</li> <li>ì—í”¼ì†Œë“œëŠ” 10,000 ìŠ¤í…ìœ¼ë¡œ êµ¬ì„±</li> <li>í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒì´ í•™ìŠµ ì‹œê°„ì— ë”°ë¼ ì¦ê°€í•˜ë©°, 600 ì—í”¼ì†Œë“œ ì´í›„ ì•ˆì •í™”ë¨</li> </ul> </li> </ul> <h3 id="2-ì‹¤í—˜-ê²°ê³¼">2. ì‹¤í—˜ ê²°ê³¼</h3> <ul> <li>Benchmark 1: <strong>Inf-cutoff-time policy</strong> <ul> <li>ì»·ì˜¤í”„ ì‹œê°„ ë¬´í•œëŒ€ ì„¤ì •</li> <li>ì¦‰, ì²« ë²ˆì§¸ ê¸°ë³¸ ë§í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´, ë‘ ë²ˆì§¸ ê¸°ë³¸ ë§í¬ê°€ ì„±ê³µí•  ë•Œê¹Œì§€ ì²« ë²ˆì§¸ ë§í¬ë¥¼ ê³„ì† ìœ ì§€ <ul> <li><code class="language-plaintext highlighter-rouge">ì—´í™”(decoherence)</code>ë¡œ ì¸í•´ ì²« ë²ˆì§¸ ë§í¬ì˜ ìƒíƒœê°€ ì €í•˜ë˜ë”ë¼ë„ íê¸°í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©</li> </ul> </li> <li>ì´ ë°©ë²•ì€ ìµœì  ì •ì±… ì„±ëŠ¥ì˜ í•˜í•œì„ ìœ¼ë¡œ í™œìš©ë¨</li> <li>ì¥ì  <ul> <li>ì²« ë²ˆì§¸ ë§í¬ë¥¼ íê¸°í•˜ì§€ ì•Šê³  ìœ ì§€í•˜ë¯€ë¡œ, ì¬ìƒì„±ì— ë”°ë¥¸ ì¶”ê°€ ë¹„ìš©ê³¼ ì‹œê°„ì„ ì ˆì•½</li> </ul> </li> <li>ë‹¨ì  <ul> <li>ì²« ë²ˆì§¸ ë§í¬ê°€ ì˜¤ë˜ ìœ ì§€ë˜ë©´ì„œ í’ˆì§ˆì´ ì €í•˜ë  ê°€ëŠ¥ì„±ì´ í¼ <ul> <li>ì €í•˜ëœ í’ˆì§ˆë¡œ ì¸í•´ ì–½í˜ êµí™˜ì˜ ì„±ê³µ í™•ë¥ ì´ í¬ê²Œ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ</li> </ul> </li> </ul> </li> </ul> </li> <li>Benchmark 2: <strong>Opt-cutoff-time policy</strong> <ul> <li>ë¬¸ì œì˜ ëª¨ë“  ê°€ëŠ¥í•œ ì»·ì˜¤í”„ ì‹œê°„ì„ ì‹œë„í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ê°’ì„ ì„ íƒ</li> <li>ì¦‰, ê°€ì¥ ì˜¤ë˜ëœ ê¸°ë³¸ ë§í¬ì˜ ì»·ì˜¤í”„ ì‹œê°„ì„ ì—¬ëŸ¬ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë‘ ì‹œë®¬ë ˆì´ì…˜ <ul> <li>ê° ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ë†’ì€ ì„±ëŠ¥(ì˜ˆ: ìµœëŒ€ ì–½í˜ êµí™˜ ì„±ê³µë¥ )ì„ ë‚´ëŠ” ì»·ì˜¤í”„ ì‹œê°„ì„ ì„ íƒ</li> </ul> </li> <li>ì´ ë°©ë²•ì€ ìµœì  ì •ì±… ì„±ëŠ¥ì˜ ìƒí•œì„ ìœ¼ë¡œ í™œìš©ë¨</li> </ul> </li> <li>ì‹¤í—˜ ê²°ê³¼ ì„¤ëª…</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-12-22-quantum_vl/fig2-480.webp 480w,/assets/img/posts/2024-12-22-quantum_vl/fig2-800.webp 800w,/assets/img/posts/2024-12-22-quantum_vl/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-12-22-quantum_vl/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>ì •ì±… í…ŒìŠ¤íŠ¸ ë°©ë²• <ul> <li>MDP í”„ë¡œì„¸ìŠ¤ë¥¼ 1,000ë²ˆ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì •ì±…ì„ í…ŒìŠ¤íŠ¸</li> <li>í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œëŠ” 10,000 ìŠ¤í…ìœ¼ë¡œ êµ¬ì„±ë¨.</li> </ul> </li> <li>ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ <ul> <li>DRL ì—ì´ì „íŠ¸ëŠ” Inf-cutoff-time ì •ì±…ë³´ë‹¤ ëª…í™•íˆ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li> <li>DRL ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì€ opt-cutoff-time ì •ì±…ì— ë§¤ìš° ê·¼ì ‘í•¨.</li> </ul> </li> <li>ì»·ì˜¤í”„ ì‹œê°„ ê²°ê³¼ <ul> <li>DRL ì •ì±…ì—ì„œ ë°œê²¬ëœ ìµœì  ì»·ì˜¤í”„ ì‹œê°„: 146.0 ìŠ¤í…</li> <li>opt-cutoff-time ì •ì±…ì—ì„œ ë°œê²¬ëœ ìµœì  ì»·ì˜¤í”„ ì‹œê°„: 108 ìŠ¤í…</li> </ul> </li> <li>ì˜ë¯¸ <ul> <li>DRL ì •ì±…ì€ ìµœì  ì •ì±…(opt-cutoff-time)ì— ê·¼ì ‘í•˜ë©´ì„œë„ ì´ ì •ì±…ì—ì„œ ì‚¬ìš©í•˜ê³  ìˆëŠ” Brute-force ë°©ë²•ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•¨</li> <li>Inf-cutoff-time ì •ì±…ë³´ë‹¤ ë” ë‚˜ì€ ì»·ì˜¤í”„ ì‹œê°„ì„ ì°¾ìŒìœ¼ë¡œì¨ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Quantum"/><category term="Networks"/><category term="Quantum,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Quantum Virtual Link Generation via Reinforcement Learning]]></summary></entry><entry><title type="html">UDC</title><link href="https://optreal.github.io/blog/2024/tabs-ttt/" rel="alternate" type="text/html" title="UDC"/><published>2024-11-26T00:32:13+00:00</published><updated>2024-11-26T00:32:13+00:00</updated><id>https://optreal.github.io/blog/2024/tabs-ttt</id><content type="html" xml:base="https://optreal.github.io/blog/2024/tabs-ttt/"><![CDATA[<h3 id="abstract">Abstract</h3> <p><strong>Single-Stage Neural Combinatorial Optimization Solvers</strong></p> <ul> <li>Exhibit significant performance degradation when applied to large-scale combinatorial optimization (CO) problems.</li> </ul> <p><strong>Two-Stage Neural Methods</strong></p> <ul> <li>Inspired by Divide-and-Conquer strategies.</li> <li>Efficient in addressing large-scale CO problems but rely heavily on problem-specific heuristics in either the dividing or conquering phase, limiting general applicability.</li> <li>Typically, employ separate training schemes, overlooking interdependencies between the two phases, often leading to convergence to suboptimal solutions.</li> </ul> <p>Unified Neural Divide-and-Conquer Framework (UDC)</p> <ul> <li>Introduces the Divide-Conquer-Reunion (DCR) training method to address issues arising from suboptimal dividing policies.</li> <li>Utilizes a lightweight Graph Neural Network (GNN) to decompose large-scale CO instances.</li> <li>Employs a constructive solver to conquer the divided sub-problems effectively. Demonstrates extensive applicability to diverse CO problems.</li> <li>Achieves superior performance across 10 representative large-scale CO problems.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_1-480.webp 480w,/assets/img/table_1-800.webp 800w,/assets/img/table_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h3 id="introduction">Introduction</h3> <p><strong>Combinatorial Optimization (CO) Applications</strong></p> <ul> <li>Route planning</li> <li>Circuit design</li> <li>Biology</li> </ul> <p><strong>Reinforcement Learning (RL)-based Constructive Neural Combinatorial Optimization (NCO) Methods</strong></p> <ul> <li>Generate near-optimal solutions for small-scale instances (e.g., TSP instances with up to 200 nodes) without requiring expert knowledge.</li> <li>Construct solutions in an end-to-end manner, node-by-node.</li> <li>Limited capability when applied to large-scale instances.</li> </ul> <p><strong>Categories of NCO Methods</strong></p> <ol> <li><strong>Modified Single-Stage Solvers</strong> <ul> <li>Methods like BQ-NCO and LEHD develop sub-path construction using heavy decoders.</li> <li>Require supervised learning (SL), limiting applicability when high-quality labeled solutions are unavailable.</li> </ul> </li> <li><strong>Auxiliary Information for RL-Based Solvers</strong> <ul> <li>Methods like ELG, ICAM, and DAR use auxiliary information to guide solvers.</li> <li>Problem-specific auxiliary designs limit general applicability.</li> <li>Complexity issues arise, particularly with self-attention mechanisms (e.g., \(O(N^2)\) complexity).</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Inspired by traditional heuristic divide-and-conquer methods.</li> <li>Use a two-stage approach: dividing the instance and conquering sub-problems.</li> <li>Methods like TAM, H-TSP, and GLOP show improved efficiency in large-scale TSP and CVRP problems.</li> </ul> </li> </ol> <p><strong>Challenges in Large-Scale NCO</strong></p> <ul> <li>Heavy models requiring SL are limited by the availability of labeled solutions.</li> <li>Self-attention complexity \(O(N^2)\) hinders scalability.</li> <li>Problem-specific auxiliary information limits general applicability.</li> </ul> <h3 id="shortcomings-of-neural-divide-and-conquer-approaches">Shortcomings of Neural Divide-and-Conquer Approaches</h3> <p><strong>Limitations in Applicability and Solution Quality</strong></p> <ul> <li>Rely on problem-specific heuristics in either the dividing (e.g., GLOP, SO) or conquering (e.g., L2D, RBG) stages, which limits generalizability.</li> </ul> <p><strong>Issues with Separate Training Process</strong></p> <ul> <li>Dividing and conquering policies are trained separately, which fails to consider their interdependencies, often resulting in convergence to local optima.</li> </ul> <p><strong>Importance of Mitigating Sub-Optimal Dividing Impact</strong></p> <ul> <li>Addressing suboptimal dividing is crucial for achieving high-quality solutions.</li> </ul> <h3 id="proposed-approach">Proposed Approach</h3> <p><strong>Divide-Conquer-Reunion (DCR)</strong></p> <ul> <li>A novel RL-based training method designed to consider interdependencies between dividing and conquering stages.</li> </ul> <p><strong>Unified Neural Divide-and-Conquer Framework (UDC)</strong></p> <ul> <li>Incorporates DCR in a unified training scheme.</li> <li>Uses a lightweight GNN to efficiently decompose large-scale instances into manageable sub-problems.</li> <li>Constructive solvers then effectively solve these sub-problems.</li> </ul> <p><strong>Contributions</strong></p> <ul> <li>Propose DCR to mitigate the impact of suboptimal dividing policies.</li> <li>Achieve a unified training scheme in UDC, leading to improved performance.</li> <li>Demonstrate UDCâ€™s applicability across various CO problems.</li> </ul> <hr/> <h3 id="preliminaries-neural-divide-and-conquer">Preliminaries: Neural Divide-and-Conquer</h3> <p><strong>CO Problem Definition</strong></p> <ul> <li>Involves $N$ decision variables.</li> <li>Objective: Minimize function \(f(x, \mathcal{G})\), where $G$ is the CO instance, and \(\Omega\) is the set of feasible solutions.</li> </ul> \[\text{minimize}_ f(x, \mathcal{G})\] <p><strong>Divide-and-Conquer in CO</strong></p> <ul> <li><strong>Traditional Methods</strong> <ul> <li>Use heuristic algorithms like large-neighborhood-search to divide and conquer.</li> <li>Dividing stage selects sub-problems, and conquering stage repairs sub-problems.</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Dividing policy \(\pi_d(\mathcal{G})\) decomposes instance $G$ into sub-problems.</li> <li>Conquering policy \(\pi_c\) solves each sub-problem, and the total solution is obtained by concatenating sub-solutions.</li> </ul> </li> </ul> <h3 id="constructive-neural-solver">Constructive Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Efficient for small-scale CO problems.</li> <li>Uses an attention-based encoder-decoder network to construct solutions.</li> </ul> <p><strong>Training Process</strong></p> <ul> <li>Modeled as a Markov Decision Process (MDP).</li> <li>Trained using Deep Reinforcement Learning (DRL) without expert experience.</li> </ul> <p><strong>Solution Generation</strong></p> <ul> <li>Constructs solutions step-by-step using a trained policy \(\pi\).</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = \prod_{t=1}^{\tau} p_{\theta}(x_t \mid x_{1:t-1}, \mathcal{G}, \Omega)\] <h3 id="heatmap-based-neural-solver">Heatmap-Based Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Uses lightweight GNNs for problem-solving, especially for large-scale CO problems like VRPs.</li> </ul> <p><strong>Limitations</strong></p> <ul> <li><strong>Non-Autoregressive Generation</strong>: Lacks partial solution order information, which can lead to poor solution quality.</li> <li><strong>Search Algorithm Dependence</strong>: Relies on search algorithms for high-quality solutions.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = p_{\theta}(\mathcal{H} \mid \mathcal{G}, \Omega) p(x_1) \prod_{t=2}^{\tau} \frac{\exp(\mathcal{H}_{x_{t-1}, x_t})}{\sum_{i=t}^{N} \exp(\mathcal{H}_{x_{t-1}, x_i})},\] <hr/> <h3 id="methodology-unified-divide-and-conquer-udc">Methodology: Unified Divide-and-Conquer (UDC)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_1-480.webp 480w,/assets/img/figure_1-800.webp 800w,/assets/img/figure_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="general-framework">General Framework</h4> <ul> <li><strong>Two Stages</strong>: Dividing and Conquering.</li> <li><strong>Dividing Stage</strong>: Generates initial solutions using an Anisotropic GNN (AGNN).</li> <li><strong>Conquering Stage</strong>: Decomposes the original instance into sub-problems and solves them using constructive neural solvers.</li> </ul> <p><strong>Solver Integration</strong></p> <ul> <li>Different solvers are used based on the type of CO problem.</li> <li><strong>AGNN</strong> for Maximum Independent Set (MIS).</li> <li><strong>POMO</strong> for Vehicle Routing Problem (VRP).</li> <li><strong>ICAM</strong> for 0-1 Knapsack Problem (KP).</li> </ul> <p><strong>Dividing Stage</strong></p> \[\pi_d(x_0|\mathcal{G}_D, \Omega, \phi) = \begin{cases} p(\mathcal{H}|\mathcal{G}_D, \Omega, \phi) p(x_{0,1}) \prod_{t=2}^\tau \frac{\exp(\mathcal{H}_{x_0, t-1, x_0, t})}{\sum_{i=t}^N \exp(\mathcal{H}_{x_0, t-1, x_0, i})}, &amp; \text{if } x_0 \in \Omega \\ 0, &amp; \text{otherwise} \end{cases}\] <ul> <li>original CO instance \(\mathcal{G}\)</li> <li>sparse graph \(\mathcal{G}_D = \{ \mathbb{V}, \mathbb{E} \}\)</li> <li>parameter $$\phi$ of Anisotropic Graph Neural Networks (AGNN)</li> <li>heatmap \(\mathcal{H}\) (e.g For \(N\)-node VRPs, the heatmap \(\mathcal{H} \in \mathbb{R}^{NÃ—N}\) )</li> <li>initial solution \(x_0 = (x_{0,1},...,x_{0,\tau})\), \(\tau\) is length</li> </ul> <p><strong>Conquering Stage: Sub-problem Preparation</strong></p> <ul> <li>sub-problems \(\{ \mathcal{G}_1,..., \mathcal{G}_{\lfloor \frac{N}{n} \rfloor} \}\)</li> <li>\(\{ \Omega_1,..., \Omega_{\lfloor \frac{N}{n} \rfloor} \}\) constraints of sub-problems (e.g no self-loop in sub-TSPs)</li> </ul> <p><strong>Conquering Stage: Constructive Neural Conquering</strong></p> <p>\(\pi_c(s_k|\mathcal{G}_k, \Omega_k, \theta) = \begin{cases} \prod_{t=1}^n p(s_{k,t} | s_{k,1:t-1}, \mathcal{G}_k, \Omega_k, \theta), &amp; \text{if } s_k \in \Omega_k \\ 0, &amp; \text{otherwise} \end{cases}\)</p> <ul> <li>utilize constructive solvers with parameter $$\theta$ for most involved sub-CO problems.</li> <li>sub-solution \(s_{k} = (s_{k,1},...,s_{k,n})\), $k \in { 1,â€¦, \lfloor \frac{N}{n} \rfloor}$$</li> <li>conquering policy \(\pi_c\)</li> <li>Replacement of original solution fragments in the final conquering stage: sub-solutions with improvements on the objective function replace the original solution fragment in \(x_0\)</li> <li>Formation of merged solution: the merged solution becomes \(x_1\)</li> <li>Repeated execution of conquering stage: conquering stage can be executed repeatedly on the new merged solution</li> <li>Gradual improvement in solution quality: the solution after \(r\) conquering stages is noted as \(x_r\)</li> </ul> <h3 id="training-method-divide-conquer-reunion-dcr">Training Method: Divide-Conquer-Reunion (DCR)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_2-480.webp 480w,/assets/img/figure_2-800.webp 800w,/assets/img/figure_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dividing and conquering stages modeled as MDPs.</li> <li>Separate training for conquering and dividing policies.</li> <li>Need for problem-specific datasets.</li> <li>Lack of collaboration in optimizing policies.</li> <li>Impact of sub-optimal sub-problem decomposition.</li> <li>Divide-Conquer-Reunion (DCR) process introduction.</li> <li>Additional Reunion step for better integration of sub-problems.</li> <li>Improved stability and convergence in training.</li> <li>Use of REINFORCE algorithm for unified training.</li> <li>Baseline calculation for both dividing and conquering policies.</li> </ul> <p>\(\nabla \mathcal{L}d(\mathcal{G}) = \frac{1}{\alpha} \sum_{i=1}^\alpha\) \(\left( f(x_2^i, \mathcal{G}) - \frac{1}{\alpha} \sum_{j=1}^\alpha f(x_2^j, \mathcal{G}) \right) \nabla \log \pi_d(x_2^i|\mathcal{G}_D, \Omega, \phi)\) \(\nabla \mathcal{L}{c1}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{1,i}, \mathcal{G}_{c}^0) - \frac{1}{\beta} \sum_{j=1}^\beta f{\prime}(s_{c}^{1,j}, \mathcal{G}_{c}^0) \right) \nabla \log \pi_c(s_{c}^{1,j}|\mathcal{G}_{c}^0, \Omega_{c}, \theta) \right)\) \(\nabla \mathcal{L}{c2}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{2,i}, \mathcal{G}_{c}^1) - \frac{1}{\beta} \sum_{j=1}^\beta f{\prime}(s_{c}^{2,j}, \mathcal{G}_{c}^1) \right) \nabla \log \pi_c(s_{c}^{2,j}|\mathcal{G}_{c}^1, \Omega_{c}, \theta) \right)\)</p> <ul> <li>\(\{ x_2^1, ..., x_{2}^{\alpha} \}\) represents the \(\alpha\) sampled solutions.</li> <li>there are \(\alpha \lfloor \frac{N}{n} \rfloor$ sub-problems\)\mathcal{G}^{0}<em>{c},c \in { 1, â€¦, \lfloor \frac{N}{n} \rfloor, â€¦, \alpha \lfloor \frac{N}{n} \rfloor}\(generated based on\){ x_0^1, â€¦, x</em>{0}^{\alpha} }$$ in the first conquering stage</li> <li>\(\alpha \lfloor \frac{N}{n} \rfloor\) can be regarded as the batch size of sub-problems</li> <li>The \(\beta\) sampled sub-solutions for sub-problem \(\mathcal{G}_{c}^{0}, \mathcal{G}_{c}^{1},c \in \{1,..., \alpha \lfloor \frac{N}{n} \rfloor\}\) are noted as \(\{s_{c}^{1,i},...,s_{c}^{1,\beta}\},\{s_{c}^{2,i},...,s_{c}^{2,i}\}\).</li> </ul> <p><strong>Challenges and Proposed Solution</strong></p> <ul> <li>Existing methods fail to train dividing and conquering policies simultaneously, leading to unsolvable antagonisms.</li> <li><strong>Unified Training Requirement</strong>: DCR enables collaborative optimization of dividing and conquering policies by treating connections between sub-problems as new problems to reconquer.</li> </ul> <p><strong>Training Process with REINFORCE</strong></p> <ul> <li>Uses the REINFORCE algorithm to train both dividing and conquering policies, ensuring better reward estimation and improved convergence.</li> </ul> <h3 id="application-general-co-problems">Application: General CO Problems</h3> <p><strong>Conditions for Applicability</strong></p> <ol> <li><strong>Decomposable Objective Functions</strong>: The objective function must contain decomposable aggregate functions (i.e., no functions like Rank or Top-k).</li> <li><strong>Feasibility of Initial and Sub-Solutions</strong>: Ensured using feasibility masks.</li> <li><strong>Non-Uniqueness of Sub-Problem Solutions</strong>: Solutions for sub-problems should not be unique to ensure flexibility in merging sub-solutions.</li> </ol> <p><strong>Limitations</strong></p> <ul> <li>Complex CO problems may face issues where solutions cannot be guaranteed as legal through the process, limiting applicability.</li> <li>Problems such as TSPTW may have constraints that make ensuring legal initial and sub-solutions difficult.</li> </ul> <hr/> <h3 id="experiment">Experiment</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_2-480.webp 480w,/assets/img/table_2-800.webp 800w,/assets/img/table_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_3-480.webp 480w,/assets/img/table_3-800.webp 800w,/assets/img/table_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_4-480.webp 480w,/assets/img/table_4-800.webp 800w,/assets/img/table_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_3-480.webp 480w,/assets/img/figure_3-800.webp 800w,/assets/img/figure_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Overview</strong></p> <ul> <li>To verify the applicability and efficiency of UDC, experiments were conducted across 10 different CO problems, including TSP, CVRP, KP, MIS, and more.</li> <li>UDC was compared to both classical and neural solvers.</li> </ul> <p><strong>Performance Evaluation</strong></p> <ul> <li>UDC demonstrated superior performance in terms of solution quality and computational efficiency across large-scale CO instances, ranging from 500-node to 2,000-node problems.</li> </ul> <p><strong>Comparison to Baselines</strong></p> <ul> <li>Classical solvers like LKH and other neural methods (e.g., ELG, GLOP) were used as baselines.</li> <li>UDC consistently outperformed other methods, particularly in large-scale settings where scalability is critical.</li> </ul> <hr/> <h3 id="conclusion">Conclusion</h3> <p><strong>Summary</strong></p> <ul> <li>UDC, with its novel DCR training mechanism, successfully addresses the limitations of existing neural divide-and-conquer methods for large-scale CO problems.</li> <li>The unified training scheme ensures that both dividing and conquering stages work in synergy, thereby achieving better overall optimization.</li> </ul> <p><strong>Future Work</strong></p> <ul> <li>Further improvements can be made by designing better loss functions for training.</li> <li>Extending UDCâ€™s applicability to other complex CO problems not covered in the current study is another promising direction for future research.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</title><link href="https://optreal.github.io/blog/2024/udc/" rel="alternate" type="text/html" title="A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems"/><published>2024-11-26T00:00:00+00:00</published><updated>2024-11-26T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/udc</id><content type="html" xml:base="https://optreal.github.io/blog/2024/udc/"><![CDATA[<h3 id="abstract">Abstract</h3> <p><strong>Single-Stage Neural Combinatorial Optimization Solvers</strong></p> <ul> <li>Exhibit significant performance degradation when applied to large-scale combinatorial optimization (CO) problems.</li> </ul> <p><strong>Two-Stage Neural Methods</strong></p> <ul> <li>Inspired by Divide-and-Conquer strategies.</li> <li>Efficient in addressing large-scale CO problems but rely heavily on problem-specific heuristics in either the dividing or conquering phase, limiting general applicability.</li> <li>Typically, employ separate training schemes, overlooking interdependencies between the two phases, often leading to convergence to suboptimal solutions.</li> </ul> <p>Unified Neural Divide-and-Conquer Framework (UDC)</p> <ul> <li>Introduces the Divide-Conquer-Reunion (DCR) training method to address issues arising from suboptimal dividing policies.</li> <li>Utilizes a lightweight Graph Neural Network (GNN) to decompose large-scale CO instances.</li> <li>Employs a constructive solver to conquer the divided sub-problems effectively. Demonstrates extensive applicability to diverse CO problems.</li> <li>Achieves superior performance across 10 representative large-scale CO problems.</li> </ul> <p><img src="assets/udc_img/table_1.png" alt="poster"/></p> <hr/> <h3 id="introduction">Introduction</h3> <p><strong>Combinatorial Optimization (CO) Applications</strong></p> <ul> <li>Route planning</li> <li>Circuit design</li> <li>Biology</li> </ul> <p><strong>Reinforcement Learning (RL)-based Constructive Neural Combinatorial Optimization (NCO) Methods</strong></p> <ul> <li>Generate near-optimal solutions for small-scale instances (e.g., TSP instances with up to 200 nodes) without requiring expert knowledge.</li> <li>Construct solutions in an end-to-end manner, node-by-node.</li> <li>Limited capability when applied to large-scale instances.</li> </ul> <p><strong>Categories of NCO Methods</strong></p> <ol> <li><strong>Modified Single-Stage Solvers</strong> <ul> <li>Methods like BQ-NCO and LEHD develop sub-path construction using heavy decoders.</li> <li>Require supervised learning (SL), limiting applicability when high-quality labeled solutions are unavailable.</li> </ul> </li> <li><strong>Auxiliary Information for RL-Based Solvers</strong> <ul> <li>Methods like ELG, ICAM, and DAR use auxiliary information to guide solvers.</li> <li>Problem-specific auxiliary designs limit general applicability.</li> <li>Complexity issues arise, particularly with self-attention mechanisms (e.g., $O(N^2)$ complexity).</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Inspired by traditional heuristic divide-and-conquer methods.</li> <li>Use a two-stage approach: dividing the instance and conquering sub-problems.</li> <li>Methods like TAM, H-TSP, and GLOP show improved efficiency in large-scale TSP and CVRP problems.</li> </ul> </li> </ol> <p><strong>Challenges in Large-Scale NCO</strong></p> <ul> <li>Heavy models requiring SL are limited by the availability of labeled solutions.</li> <li>Self-attention complexity $O(N^2)$ hinders scalability.</li> <li>Problem-specific auxiliary information limits general applicability.</li> </ul> <h3 id="shortcomings-of-neural-divide-and-conquer-approaches">Shortcomings of Neural Divide-and-Conquer Approaches</h3> <p><strong>Limitations in Applicability and Solution Quality</strong></p> <ul> <li>Rely on problem-specific heuristics in either the dividing (e.g., GLOP, SO) or conquering (e.g., L2D, RBG) stages, which limits generalizability.</li> </ul> <p><strong>Issues with Separate Training Process</strong></p> <ul> <li>Dividing and conquering policies are trained separately, which fails to consider their interdependencies, often resulting in convergence to local optima.</li> </ul> <p><strong>Importance of Mitigating Sub-Optimal Dividing Impact</strong></p> <ul> <li>Addressing suboptimal dividing is crucial for achieving high-quality solutions.</li> </ul> <h3 id="proposed-approach">Proposed Approach</h3> <p><strong>Divide-Conquer-Reunion (DCR)</strong></p> <ul> <li>A novel RL-based training method designed to consider interdependencies between dividing and conquering stages.</li> </ul> <p><strong>Unified Neural Divide-and-Conquer Framework (UDC)</strong></p> <ul> <li>Incorporates DCR in a unified training scheme.</li> <li>Uses a lightweight GNN to efficiently decompose large-scale instances into manageable sub-problems.</li> <li>Constructive solvers then effectively solve these sub-problems.</li> </ul> <p><strong>Contributions</strong></p> <ul> <li>Propose DCR to mitigate the impact of suboptimal dividing policies.</li> <li>Achieve a unified training scheme in UDC, leading to improved performance.</li> <li>Demonstrate UDCâ€™s applicability across various CO problems.</li> </ul> <hr/> <h3 id="preliminaries-neural-divide-and-conquer">Preliminaries: Neural Divide-and-Conquer</h3> <p><strong>CO Problem Definition</strong></p> <ul> <li>Involves $N$ decision variables.</li> <li>Objective: Minimize function $f(x, G)$, where $G$ is the CO instance, and $\Omega$ is the set of feasible solutions. \(\text{minimize}_ f(x, \mathcal{G})\)</li> </ul> <p><strong>Divide-and-Conquer in CO</strong></p> <ul> <li><strong>Traditional Methods</strong> <ul> <li>Use heuristic algorithms like large-neighborhood-search to divide and conquer.</li> <li>Dividing stage selects sub-problems, and conquering stage repairs sub-problems.</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Dividing policy $\pi_d(G)$ decomposes instance $G$ into sub-problems.</li> <li>Conquering policy $\pi_c$ solves each sub-problem, and the total solution is obtained by concatenating sub-solutions.</li> </ul> </li> </ul> <h3 id="constructive-neural-solver">Constructive Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Efficient for small-scale CO problems.</li> <li>Uses an attention-based encoder-decoder network to construct solutions.</li> </ul> <p><strong>Training Process</strong></p> <ul> <li>Modeled as a Markov Decision Process (MDP).</li> <li>Trained using Deep Reinforcement Learning (DRL) without expert experience.</li> </ul> <p><strong>Solution Generation</strong></p> <ul> <li>Constructs solutions step-by-step using a trained policy $\pi$.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = \prod_{t=1}^{\tau} p_{\theta}(x_t \mid x_{1:t-1}, \mathcal{G}, \Omega)\] <h3 id="heatmap-based-neural-solver">Heatmap-Based Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Uses lightweight GNNs for problem-solving, especially for large-scale CO problems like VRPs.</li> </ul> <p><strong>Limitations</strong></p> <ul> <li><strong>Non-Autoregressive Generation</strong>: Lacks partial solution order information, which can lead to poor solution quality.</li> <li><strong>Search Algorithm Dependence</strong>: Relies on search algorithms for high-quality solutions.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = p_{\theta}(\mathcal{H} \mid \mathcal{G}, \Omega) p(x_1) \prod_{t=2}^{\tau} \frac{\exp(\mathcal{H}_{x_{t-1}, x_t})}{\sum_{i=t}^{N} \exp(\mathcal{H}_{x_{t-1}, x_i})},\] <hr/> <h3 id="methodology-unified-divide-and-conquer-udc">Methodology: Unified Divide-and-Conquer (UDC)</h3> <p><img src="udc_img/figure_1.png" alt="poster"/></p> <h4 id="general-framework">General Framework</h4> <ul> <li><strong>Two Stages</strong>: Dividing and Conquering.</li> <li><strong>Dividing Stage</strong>: Generates initial solutions using an Anisotropic GNN (AGNN).</li> <li><strong>Conquering Stage</strong>: Decomposes the original instance into sub-problems and solves them using constructive neural solvers.</li> </ul> <p><strong>Solver Integration</strong></p> <ul> <li>Different solvers are used based on the type of CO problem.</li> <li><strong>AGNN</strong> for Maximum Independent Set (MIS).</li> <li><strong>POMO</strong> for Vehicle Routing Problem (VRP).</li> <li><strong>ICAM</strong> for 0-1 Knapsack Problem (KP).</li> </ul> <p><strong>Dividing Stage</strong></p> \[\pi_d(x_0|\mathcal{G}_D, \Omega, \phi) = \begin{cases} p(\mathcal{H}|\mathcal{G}_D, \Omega, \phi) p(x_{0,1}) \prod_{t=2}^\tau \frac{\exp(\mathcal{H}_{x_0, t-1, x_0, t})}{\sum_{i=t}^N \exp(\mathcal{H}_{x_0, t-1, x_0, i})}, &amp; \text{if } x_0 \in \Omega \\ 0, &amp; \text{otherwise} \end{cases}\] <ul> <li>original CO instance $\mathcal{G}$</li> <li>sparse graph $\mathcal{G}_D = { \mathbb{V}, \mathbb{E} }$</li> <li>parameter $\phi$ of Anisotropic Graph Neural Networks (AGNN)</li> <li>heatmap $\mathcal{H}$ (e.g For $N$-node VRPs, the heatmap $\mathcal{H} \in \mathbb{R}^{NÃ—N}$ )</li> <li>initial solution $x_0 = (x_{0,1},â€¦,x_{0,\tau})$, $\tau$ is length</li> </ul> <p><strong>Conquering Stage: Sub-problem Preparation</strong></p> <ul> <li>sub-problems ${ \mathcal{G}<em>1,â€¦, \mathcal{G}</em>{\lfloor \frac{N}{n} \rfloor} }$</li> <li>${ \Omega_1,â€¦, \Omega_{\lfloor \frac{N}{n} \rfloor} }$ constraints of sub-problems (e.g no self-loop in sub-TSPs)</li> </ul> <p><strong>Conquering Stage: Constructive Neural Conquering</strong></p> <p>\(\pi_c(s_k|\mathcal{G}_k, \Omega_k, \theta) = \begin{cases} \prod_{t=1}^n p(s_{k,t} | s_{k,1:t-1}, \mathcal{G}_k, \Omega_k, \theta), &amp; \text{if } s_k \in \Omega_k \\ 0, &amp; \text{otherwise} \end{cases}\)</p> <ul> <li>utilize constructive solvers with parameter $\theta$ for most involved sub-CO problems.</li> <li>sub-solution $s_{k} = (s_{k,1},â€¦,s_{k,n})$, $k \in { 1,â€¦, \lfloor \frac{N}{n} \rfloor}$</li> <li>conquering policy $\pi_c$</li> <li>Replacement of original solution fragments in the final conquering stage: sub-solutions with improvements on the objective function replace the original solution fragment in $x_0$</li> <li>Formation of merged solution: the merged solution becomes $x_1$</li> <li>Repeated execution of conquering stage: conquering stage can be executed repeatedly on the new merged solution</li> <li>Gradual improvement in solution quality: the solution after $r$ conquering stages is noted as $x_r$</li> </ul> <h3 id="training-method-divide-conquer-reunion-dcr">Training Method: Divide-Conquer-Reunion (DCR)</h3> <p><img src="udc_img/figure_2.png" alt="poster"/></p> <ul> <li>Dividing and conquering stages modeled as MDPs.</li> <li>Separate training for conquering and dividing policies.</li> <li>Need for problem-specific datasets.</li> <li>Lack of collaboration in optimizing policies.</li> <li>Impact of sub-optimal sub-problem decomposition.</li> <li>Divide-Conquer-Reunion (DCR) process introduction.</li> <li>Additional Reunion step for better integration of sub-problems.</li> <li>Improved stability and convergence in training.</li> <li>Use of REINFORCE algorithm for unified training.</li> <li>Baseline calculation for both dividing and conquering policies.</li> </ul> <table> <tbody> <tr> <td>$$\nabla \mathcal{L}d(\mathcal{G}) = \frac{1}{\alpha} \sum_{i=1}^\alpha \left( f(x_2^i, \mathcal{G}) - \frac{1}{\alpha} \sum_{j=1}^\alpha f(x_2^j, \mathcal{G}) \right) \nabla \log \pi_d(x_2^i</td> <td>\mathcal{G}_D, \Omega, \phi) $$</td> </tr> <tr> <td>$$\nabla \mathcal{L}{c1}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{1,i}, \mathcal{G}<em>{c}^0) - \frac{1}{\beta} \sum</em>{j=1}^\beta f{\prime}(s_{c}^{1,j}, \mathcal{G}<em>{c}^0) \right) \nabla \log \pi_c(s</em>{c}^{1,j}</td> <td>\mathcal{G}<em>{c}^0, \Omega</em>{c}, \theta) \right)$$</td> </tr> <tr> <td>$$\nabla \mathcal{L}{c2}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{2,i}, \mathcal{G}<em>{c}^1) - \frac{1}{\beta} \sum</em>{j=1}^\beta f{\prime}(s_{c}^{2,j}, \mathcal{G}<em>{c}^1) \right) \nabla \log \pi_c(s</em>{c}^{2,j}</td> <td>\mathcal{G}<em>{c}^1, \Omega</em>{c}, \theta) \right)$$</td> </tr> </tbody> </table> <ul> <li>${ x_2^1, â€¦, x_{2}^{\alpha} }$ represents the $\alpha$ sampled solutions.</li> <li>there are $\alpha \lfloor \frac{N}{n} \rfloor$ sub-problems $\mathcal{G}^{0}<em>{c},c \in { 1, â€¦, \lfloor \frac{N}{n} \rfloor, â€¦, \alpha \lfloor \frac{N}{n} \rfloor}$ generated based on ${ x_0^1, â€¦, x</em>{0}^{\alpha} }$ in the first conquering stage</li> <li>$\alpha \lfloor \frac{N}{n} \rfloor$ can be regarded as the batch size of sub-problems</li> <li>The $\beta$ sampled sub-solutions for sub-problem $\mathcal{G}<em>{c}^{0}, \mathcal{G}</em>{c}^{1},c \in {1,â€¦, \alpha \lfloor \frac{N}{n} \rfloor}$ are noted as ${s_{c}^{1,i},â€¦,s_{c}^{1,\beta}},{s_{c}^{2,i},â€¦,s_{c}^{2,i}}$.</li> </ul> <p><strong>Challenges and Proposed Solution</strong></p> <ul> <li>Existing methods fail to train dividing and conquering policies simultaneously, leading to unsolvable antagonisms.</li> <li><strong>Unified Training Requirement</strong>: DCR enables collaborative optimization of dividing and conquering policies by treating connections between sub-problems as new problems to reconquer.</li> </ul> <p><strong>Training Process with REINFORCE</strong></p> <ul> <li>Uses the REINFORCE algorithm to train both dividing and conquering policies, ensuring better reward estimation and improved convergence.</li> </ul> <h3 id="application-general-co-problems">Application: General CO Problems</h3> <p><strong>Conditions for Applicability</strong></p> <ol> <li><strong>Decomposable Objective Functions</strong>: The objective function must contain decomposable aggregate functions (i.e., no functions like Rank or Top-k).</li> <li><strong>Feasibility of Initial and Sub-Solutions</strong>: Ensured using feasibility masks.</li> <li><strong>Non-Uniqueness of Sub-Problem Solutions</strong>: Solutions for sub-problems should not be unique to ensure flexibility in merging sub-solutions.</li> </ol> <p><strong>Limitations</strong></p> <ul> <li>Complex CO problems may face issues where solutions cannot be guaranteed as legal through the process, limiting applicability.</li> <li>Problems such as TSPTW may have constraints that make ensuring legal initial and sub-solutions difficult.</li> </ul> <hr/> <h3 id="experiment">Experiment</h3> <p><img src="udc_img/table_2.png" alt="poster"/> <img src="udc_img/table_3.png" alt="poster"/> <img src="udc_img/table_4.png" alt="poster"/> <img src="udc_img/figure_3.png" alt="poster"/></p> <p><strong>Overview</strong></p> <ul> <li>To verify the applicability and efficiency of UDC, experiments were conducted across 10 different CO problems, including TSP, CVRP, KP, MIS, and more.</li> <li>UDC was compared to both classical and neural solvers.</li> </ul> <p><strong>Performance Evaluation</strong></p> <ul> <li>UDC demonstrated superior performance in terms of solution quality and computational efficiency across large-scale CO instances, ranging from 500-node to 2,000-node problems.</li> </ul> <p><strong>Comparison to Baselines</strong></p> <ul> <li>Classical solvers like LKH and other neural methods (e.g., ELG, GLOP) were used as baselines.</li> <li>UDC consistently outperformed other methods, particularly in large-scale settings where scalability is critical.</li> </ul> <hr/> <h3 id="conclusion">Conclusion</h3> <p><strong>Summary</strong></p> <ul> <li>UDC, with its novel DCR training mechanism, successfully addresses the limitations of existing neural divide-and-conquer methods for large-scale CO problems.</li> <li>The unified training scheme ensures that both dividing and conquering stages work in synergy, thereby achieving better overall optimization.</li> </ul> <p><strong>Future Work</strong></p> <ul> <li>Further improvements can be made by designing better loss functions for training.</li> <li>Extending UDCâ€™s applicability to other complex CO problems not covered in the current study is another promising direction for future research.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="Divide-and-Conquer,"/><category term="Optimization"/><summary type="html"><![CDATA[A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems]]></summary></entry><entry><title type="html">a post with tabs2</title><link href="https://optreal.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs2"/><published>2024-05-02T00:32:13+00:00</published><updated>2024-05-02T00:32:13+00:00</updated><id>https://optreal.github.io/blog/2024/tabs</id><content type="html" xml:base="https://optreal.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="59d1223a-b510-4eba-92dc-63fd84acbece" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="59d1223a-b510-4eba-92dc-63fd84acbece" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="2f955564-64ab-4d63-8187-1fbcc7a1259f" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="2f955564-64ab-4d63-8187-1fbcc7a1259f" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="825f155e-5354-48f7-aab2-b2f3a0308824" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="825f155e-5354-48f7-aab2-b2f3a0308824" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry></feed>