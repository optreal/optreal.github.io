<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://optreal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://optreal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-29T13:13:30+00:00</updated><id>https://optreal.github.io/feed.xml</id><title type="html">Optreal</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Copycat 문제</title><link href="https://optreal.github.io/blog/2025/copycat/" rel="alternate" type="text/html" title="Copycat 문제"/><published>2025-01-29T00:00:00+00:00</published><updated>2025-01-29T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/copycat</id><content type="html" xml:base="https://optreal.github.io/blog/2025/copycat/"><![CDATA[<h3 id="copycat-문제란"><strong>Copycat 문제란?</strong></h3> <p>Copycat 문제는 <strong>“속임수 학습”</strong> 문제라고 볼 수 있다.</p> <ul> <li>어떤 학생이 선생님의 설명을 듣고 따라 해야 하는 숙제가 있다고 가정하자. 이 학생이 선생님의 말을 진정으로 이해하고 따라 하는 것이 아니라, <strong>단순히 직전에 한 말을 그대로 따라 하기만 한다면 어떨까?</strong></li> <li>이러한 방식은 즉각적인 반응을 보일 수는 있지만, <strong>왜 그렇게 말해야 하는지, 어떤 상황에서 그 말을 해야 하는지는 전혀 학습되지 않는다.</strong></li> <li>Copycat 문제는 <strong>모방 학습(Imitation Learning, IL)</strong>에서 발생하는 대표적인 <strong>인과적 혼동(Causal Confusion)</strong>문제 중 하나이다.</li> <li>에이전트는 환경의 동역학(dynamics)을 이해하려 하지 않고, 단순히 전문가의 과거 행동을 복사하는 방식으로 학습할 가능성이 높아진다.</li> <li>이러한 문제는 복잡한 환경에서 일반화 성능을 저하시킬 뿐만 아니라, 안전에 영향을 미치는 중요한 작업에서도 오류를 유발할 수 있다.</li> </ul> <hr/> <h3 id="예제-1-수학-문제-풀기"><strong>예제 1: 수학 문제 풀기</strong></h3> <p>선생님이 칠판에 수학 문제를 풀면서 <strong>“3 + 5 = 8”</strong>이라고 설명했다고 가정하자.<br/> 만약 학생이 <strong>진정으로 수학의 원리를 이해했다면</strong>, 같은 방법을 활용하여 <strong>4 + 6</strong>과 같은 새로운 문제도 해결할 수 있어야 한다.<br/> 그러나 학생이 <strong>단순히 칠판에 적힌 답만을 외워서 “3 + 5는 무조건 8”이라고만 기억한다면</strong>, 새로운 문제가 제시되었을 때 해결할 수 없게 된다.</p> <p>이것이 바로 <strong>Copycat 문제</strong>이다.</p> <blockquote> <p><strong>즉, 원리를 이해하지 않고 단순히 따라 하기만 하면, 새로운 상황에서 이를 적용할 수 없다는 점이 문제로 작용한다.</strong></p> </blockquote> <hr/> <h3 id="예제-2-자율주행-자동차"><strong>예제 2: 자율주행 자동차</strong></h3> <p>자율주행 자동차가 <strong>운전자의 행동을 모방 학습한다고 가정해보자.</strong></p> <ul> <li>운전자가 <strong>항상 신호등이 초록색일 때만 출발했다</strong>고 하자.</li> <li>그러나 자율주행 자동차가 <strong>신호등의 색을 인식하는 것이 아니라, 단순히 “운전자가 언제 출발했는지”만 학습했다면?</strong></li> <li>이러한 방식으로 학습된 자동차는 <strong>새로운 도로 상황이나 다른 신호등 앞에서는 올바르게 출발해야 할지 여부를 판단하지 못할 가능성이 높다.</strong></li> <li>또한 다른 예제로서, 자율 주행 차량에서 전문가가 이전에 좌회전을 했고, 현재도 좌회전할 가능성이 높은 경우, 에이전트는 주변 환경 정보를 고려하지 않고 그저 이전 행동을 따라하는 방식으로 정책을 학습할 수 있다.</li> </ul> <p>이것도 <strong>Copycat 문제</strong>이다.</p> <blockquote> <p><strong>즉, 상황을 보고 스스로 판단하는 것이 아니라, 단순히 과거 행동을 기계적으로 따라 하기 때문에 발생하는 문제이다.</strong></p> </blockquote> <hr/> <h3 id="copycat-문제를-해결하는-방법의-기본-아이디어"><strong>Copycat 문제를 해결하는 방법의 기본 아이디어</strong></h3> <p>Copycat 문제를 해결하기 위해서는 <strong>단순히 따라 하는 것이 아니라, 행동의 원인을 이해하는 학습 방식이 필요하다.</strong></p> <h4 id="1-잘못된-학습-습관을-제거하기"><strong>1. 잘못된 학습 습관을 제거하기</strong></h4> <ul> <li>단순히 따라 하는 것이 아니라, <strong>왜 해당 행동이 필요한지</strong>에 대한 분석이 이루어져야 한다.</li> </ul> <h4 id="2-필요한-정보만-학습하기"><strong>2. 필요한 정보만 학습하기</strong></h4> <ul> <li>자율주행 자동차의 예시에서 단순히 <strong>“운전자가 출발했다”</strong>는 정보를 학습하는 것이 아니라,</li> <li><strong>“신호등이 초록색일 때 출발해야 한다”</strong>는 규칙을 학습하도록 유도해야 한다.</li> </ul> <h4 id="3-실제-환경에서-연습하기"><strong>3. 실제 환경에서 연습하기</strong></h4> <ul> <li>다양한 상황에서 직접 경험하도록 하여, 새로운 문제에서도 스스로 판단할 수 있도록 학습을 유도해야 한다.</li> </ul> <hr/> <h3 id="copycat-문제를-해결하는-전문적인-방법"><strong>Copycat 문제를 해결하는 전문적인 방법</strong></h3> <p>Copycat 문제를 해결하기 위해, 연구자들은 <strong>전문가 행동을 단순 복사하지 않고, 환경을 이해하는 방식으로 학습할 수 있도록 유도하는 기법</strong>을 개발했다.</p> <h4 id="1-적대적-학습-기반-피처-학습-adversarial-feature-learning"><strong>1. 적대적 학습 기반 피처 학습 (Adversarial Feature Learning)</strong></h4> <ul> <li>Wen et al. [1]은 Copycat 문제를 해결하기 위해 <strong>적대적 학습(adversarial learning)</strong>을 활용하였다.</li> <li><strong>핵심 아이디어</strong>: <ul> <li>특정 특징(feature representation)을 학습할 때, <strong>전문가의 과거 행동(previous actions)에 대한 정보를 제거</strong>하고,</li> <li><strong>올바른 다음 행동(next action)을 예측하는 데 필요한 정보만 유지</strong>하도록 함.</li> </ul> </li> <li>이를 통해, <strong>에이전트가 환경을 직접 분석하고 판단하는 능력을 가지도록 유도</strong>하였다.</li> </ul> <h4 id="2-메모리-기반-인과-구조-학습-memory-based-causal-structure-learning"><strong>2. 메모리 기반 인과 구조 학습 (Memory-based Causal Structure Learning)</strong></h4> <ul> <li>Chuang et al. [2]은 Copycat 문제 해결 방법을 <strong>고차원 이미지 관찰(high-dimensional image observations)으로 확장</strong>하였다.</li> <li><strong>메모리 추출 모듈(memory extraction module)</strong>을 사용하여: <ul> <li><strong>과거 관찰(observation history)에서 필요한 정보만 추출</strong>하고,</li> <li><strong>이전 행동(previous actions)과 관련된 불필요한 정보(nuisance correlates)를 제거</strong>하였다.</li> </ul> </li> <li>이를 통해, 에이전트가 <strong>단순한 복사가 아닌, 실제로 환경을 이해하는 방식으로 정책을 학습</strong>하도록 유도하였다.</li> </ul> <h4 id="3-환경-동역학-모델-활용-dynamics-model-based-training"><strong>3. 환경 동역학 모델 활용 (Dynamics Model-Based Training)</strong></h4> <ul> <li>Copycat 문제를 방지하려면 <strong>에이전트가 환경의 동역학을 학습하는 과정이 포함</strong>되어야 한다.</li> <li>이를 위해: <ul> <li><strong>강화 학습(Reinforcement Learning, RL) 기반 접근법</strong>을 적용하여 <strong>환경과 상호작용하면서 학습</strong>하도록 함.</li> <li><strong>보상 함수 설계(Reward Function Design)</strong>를 통해, 전문가 행동을 단순히 따라 하는 것이 아니라, <strong>환경에 적응하는 행동을 장려</strong>하는 방식으로 유도할 수 있음.</li> </ul> </li> </ul> <hr/> <h3 id="결론"><strong>결론</strong></h3> <p>Copycat 문제는 <strong>진정한 원리를 학습하지 않고, 과거 행동을 단순히 복사하는 과정에서 발생하는 문제</strong>이다.<br/> 이를 해결하기 위해서는 <strong>과거 행동을 그대로 따라 하는 것이 아니라, 행동의 원인과 결과(인과 관계)를 학습하는 방식이 필요하다.</strong></p>]]></content><author><name></name></author><category term="RL"/><category term="Imitation"/><category term="Learning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Copycat 문제]]></summary></entry><entry><title type="html">Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments</title><link href="https://optreal.github.io/blog/2025/traj_planning/" rel="alternate" type="text/html" title="Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments"/><published>2025-01-14T00:00:00+00:00</published><updated>2025-01-14T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2025/traj_planning</id><content type="html" xml:base="https://optreal.github.io/blog/2025/traj_planning/"><![CDATA[<div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/authors-480.webp 480w,/assets/img/traj_planning/authors-800.webp 800w,/assets/img/traj_planning/authors-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/authors.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="abstract">Abstract</h2> <ul> <li>Motivation <ul> <li>Increase necessary to swiftly evade obstacles and adapt trajectories under hard real-time constraints. <ul> <li>Generate viable paths that prevent collisions while maintaining high speeds with minimal tracking errors.</li> </ul> </li> </ul> </li> <li>Method <ul> <li>The proposed method combines a supervised learning approach, as teacher policy, with deep reinforcement learning (DRL), as student policy. <ol> <li>Train the teacher policy using a path planning algorithm that prioritizes safety while minimizing jerk and flight time.</li> <li>Use this policy to guide the learning of the student policy in various unknown environments.</li> </ol> </li> </ul> </li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="introduction">Introduction</h3> <ul> <li>Previous works’ limitation <ul> <li>Require <ul> <li>Known environments.</li> <li>Extensive information for reliable outcomes, which is seldom the case in real-world missions.</li> </ul> </li> </ul> </li> <li>Proposed Method <ul> <li>A trajectory planning method for generating agile flight trajectories in unknown environments solely based on data from onboard sensors.</li> <li>The framework integrates two neural networks: <ul> <li>The teacher policy <ul> <li>Supervised learning.</li> <li>Incorporates a geometry-based trajectory planning strategy enriched with a heuristic to optimize flight time and enhance safety.</li> </ul> </li> <li>The student policy <ul> <li>DQN-PER.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="methodology">Methodology</h2> <ul> <li>The primary aim <ul> <li>The primary aim of our proposed approach is to facilitate the safe and agile navigation of UAVs in <strong>unknown environments</strong>, leveraging <strong>3D LiDAR</strong> for environmental perception.</li> <li>Our strategy involves establishing the UAV trajectory with a minimal flight time based on real-time sensory data while prioritizing safety.</li> </ul> </li> <li>The proposed privileged reinforcement learning framework <ul> <li>The teacher policy <ul> <li>Deep Feedforward Neural Network (DFNN).</li> <li>Trains using an expert algorithm proficient.</li> <li>Provides optimal action insights across diverse environments.</li> <li>Evaluates the student policy <strong>(DRL reward)</strong>.</li> </ul> </li> <li>The student policy <ul> <li>DQN-PER</li> <li>Operates based on data identifiable by the UAV’s 3D Lidar around its current pose.</li> </ul> </li> <li>Both networks output the new ideal waypoints to be followed (\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> <li>The distilled knowledge from the teacher policy is integrated into the student policy, functioning without privileged information.</li> </ul> </li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="problem-definition">Problem Definition</h4> <ul> <li>Our focus is on formulating a practical and reliable trajectory planning strategy, aiming to optimize three key objectives: <ul> <li>Minimizing jerk.</li> <li>Reducing flight time.</li> <li>Enhancing safety by maximizing the distance to the obstacles in the environment.</li> </ul> </li> <li>The goal is to minimize equations (1), (2), and (3) while maximizing (4) to enhance safety and ensure collision free trajectories for high-speed flights. <ol> <li>Goal Distance: \(D_{\text{goal}} = \sum_{i=0}^n \sqrt{(\mathbf{p}_{\text{goal}} - \mathbf{p}_i)^2}\)</li> <li>Next Step Distance: \(D_{\text{next}} = \sum_{i=0}^{n-1} \sqrt{(\mathbf{p}_{i+1} - \mathbf{p}_i)^2}\)</li> <li>Jerk Cost: \(D_{\text{jerk}} = \int \left( \frac{d^3 \mathbf{p}}{dt^3} \right)^2 dt\)</li> <li>Obstacle Distance: \(D_{\text{obs}} = \max \sum_{j=0}^k \sqrt{(\mathbf{p}_i - \mathbf{O}_j)^2}\)</li> </ol> </li> </ul> <h3 id="expert">Expert</h3> <ul> <li>Bidirectional A* algorithm <ul> <li>We integrate objective equations as heuristics for trajectory generation: \(f(i) = (D_{\text{goal}} + D_{\text{obs}} + D_{\text{next}}) \times D_{\text{jerk}}\)</li> </ul> </li> <li>Input: <ul> <li>Entire environment.</li> <li>Global position.</li> <li>Goal Node.</li> </ul> </li> <li>Output: <ul> <li>The ideal next waypoint for each waypoint.</li> </ul> </li> </ul> <h3 id="teacher-policy">Teacher Policy</h3> <ul> <li>The policy is trained across multiple randomly generated scenarios. <ul> <li>Rainforests.</li> <li>Mazes.</li> <li>Disaster Areas.</li> </ul> </li> <li>To ensure precision, a distinct model is learned for each scenario, resulting in 30 models trained across different scenarios, with 10 models for each environment.</li> <li>Input: <ul> <li>Goal node.</li> <li>Global position.</li> <li>Orientation.</li> <li>The environment around within a 5 × 5 × 2 meter radius range.</li> </ul> </li> <li>Output: <ul> <li>(\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> <li>The subsequent ideal action determined by our expert.</li> </ul> </li> </ul> <h3 id="student-policy">Student Policy</h3> <ul> <li>The student policy efficiently produces real-time, collision-free trajectories for agile flights, relying solely on onboard sensor measurements.</li> <li>Input: <ul> <li>obstacle positions within a 5 × 5 × 2 meter radius obtained from the 3D Lidar.</li> <li>UAV position</li> <li>Orientation.</li> <li>Goal node.</li> </ul> </li> <li>Ouput: <ul> <li>(\(F\), \(B\), \(R\), \(L\), \(U\), \(D\)).</li> </ul> </li> <li>During flight, the UAV records obstacle positions, triggering policy execution upon identifying new obstacles.</li> <li>The policy generates a new trajectory using obstacle positions providing a single waypoint per iteration.</li> <li>Multiple policy runs are conducted to create the final trajectory.</li> <li>Upon generating a trajectory, it utilizes Bézier curves to transform the anticipated trajectory into a comprehensive state representation.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/framework-480.webp 480w,/assets/img/traj_planning/framework-800.webp 800w,/assets/img/traj_planning/framework-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/framework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="results">Results</h2> <ul> <li>The proposed method is evaluated based on: <ul> <li>Flight time (seconds)</li> <li>Processing time (seconds)</li> <li>Representing the time the UAV is at least at 80% of the maximum speed</li> <li>RMSE from the guidance trajectory (meters)</li> <li>Success rate (%)</li> </ul> </li> <li>Baseline: <ul> <li>the standard bidirectional A* in an unknown environment</li> <li>our expert operating in an unknown environment</li> <li>The original DQN-PER</li> <li>Our teacher policy</li> </ul> </li> <li>Testing in simulation demonstrates noteworthy advancements, including an 80% reduction in tracking error, a 31% decrease in flight time, a 19% increase in high-speed duration, and a success rate improvement from 50% to 100%, as compared to baseline methods.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traj_planning/results-480.webp 480w,/assets/img/traj_planning/results-800.webp 800w,/assets/img/traj_planning/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/traj_planning/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="Drone"/><category term="Trajectory-Planning,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments]]></summary></entry><entry><title type="html">RMA: Rapid Motor Adaptation for Legged Robots</title><link href="https://optreal.github.io/blog/2024/rma/" rel="alternate" type="text/html" title="RMA: Rapid Motor Adaptation for Legged Robots"/><published>2024-12-30T16:00:00+00:00</published><updated>2024-12-30T16:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/rma</id><content type="html" xml:base="https://optreal.github.io/blog/2024/rma/"><![CDATA[<div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/title-480.webp 480w,/assets/img/rma/title-800.webp 800w,/assets/img/rma/title-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/title.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="i-introduction">I Introduction</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig1-480.webp 480w,/assets/img/rma/fig1-800.webp 800w,/assets/img/rma/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig2-480.webp 480w,/assets/img/rma/fig2-800.webp 800w,/assets/img/rma/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="1-legged-robotics의-발전">1. Legged Robotics의 발전</h3> <ul> <li>물리적 역학 모델링 및 제어 이론 도구 활용</li> <li>인간 설계자의 전문성 요구</li> <li>강화 학습 및 모방 학습 기법 적용 <ul> <li>설계 부담 완화</li> <li>성능 향상 가능성</li> </ul> </li> </ul> <hr/> <h3 id="2-강화-학습-기반-컨트롤러의-표준-패러다임">2. 강화 학습 기반 컨트롤러의 표준 패러다임</h3> <ul> <li>물리 시뮬레이션 환경에서 RL 기반 컨트롤러 훈련</li> <li>시뮬레이션-현실 간 전이 (sim-to-real) 기술 적용 <ul> <li>물리 로봇과 시뮬레이터 모델의 차이</li> <li>현실 세계 지형의 다양성</li> <li>시뮬레이터의 물리적 한계 <ul> <li>접촉력, 변형 가능한 표면 등 복잡한 물리 현상</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="3-quadruped-locomotion-challenge-solving-approach">3. Quadruped Locomotion Challenge Solving Approach</h3> <ul> <li><strong>A1 로봇 (Unitree)의 실험 플랫폼 사용</strong></li> <li><strong>실제 인간 보행의 특징</strong> <ul> <li>다양한 지형 적응 (흙, 언덕, 하중 등)</li> <li>피로 및 부상 대응 능력</li> </ul> </li> <li><strong>RMA (Rapid Motor Adaptation)</strong> <ul> <li>실시간 온라인 적응 필요 (초 단위 적응)</li> <li>물리 세계에서 다중 실험 및 최적화 불가능</li> <li>실제 환경에서의 데이터 수집 어려움 <ul> <li>3-5분 데이터 수집조차 비현실적</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="4-전략">4. 전략</h3> <ul> <li>기본 보행 정책 및 RMA를 시뮬레이션에서 훈련</li> <li>직접 현실 세계에 배포</li> </ul> <hr/> <h3 id="5-rma-rapid-motor-adaptation">5. RMA (Rapid Motor Adaptation)</h3> <ul> <li><strong>두 가지 하위 시스템</strong> <ul> <li>기본 정책 (Base Policy, \(\pi\))</li> <li>적응 모듈 (Adaptation Module, \(\phi\))</li> </ul> </li> <li><strong>실시간 온라인 적응 지원</strong> <ul> <li>다양한 환경 구성에 적응</li> </ul> </li> </ul> <hr/> <h4 id="51-base-policy-pi">5.1 Base Policy, \(\pi\)</h4> <ul> <li><strong>훈련 방식</strong> <ul> <li>강화 학습(RL)을 통한 시뮬레이션 훈련</li> <li>환경 구성 벡터(\(e_t\))의 privileged information 활용 (마찰, 하중 등)</li> </ul> </li> <li><strong>환경 구성 벡터(\(e_t\)) 활용 과정</strong> <ul> <li>인코더 네트워크(\(\mu\))를 통해 잠재 특징 공간(\(z_t\))으로 인코딩</li> <li>잠재 벡터(\(z_t\)) → 기본 정책(\(π\)) 입력</li> <li>로봇의 현재 상태(\(x_t\)) 및 이전 행동(\(a_{t-1}\))과 함께 사용</li> <li>목표 관절 위치(\(a_t\)) 예측</li> </ul> </li> <li><strong>End-to-End Training</strong> <ul> <li>정책(\(π\))과 인코더(\(\mu\))의 End-to-End 강화 학습</li> </ul> </li> </ul> <hr/> <h4 id="52-adaptation-module-phi">5.2 Adaptation Module, \(\phi\)</h4> <ul> <li><strong>목표</strong> <ul> <li>실시간으로 잠재 벡터(\(z_t\)) 추정</li> <li>Privileged information(\(e_t\)) 없이 상태 및 행동 history를 통해 추정</li> </ul> </li> <li><strong>원리</strong> <ul> <li>로봇 관절의 명령된 움직임과 실제 움직임의 차이를 기반으로 환경 특성 추정</li> <li>칼만 필터와 유사한 접근법</li> </ul> </li> <li><strong>훈련 과정</strong> <ul> <li>시뮬레이션에서 감독 학습으로 훈련</li> <li>상태 이력과 잠재 벡터(\(z_t\)) 계산 가능</li> </ul> </li> <li><strong>실행 시 동작</strong> <ul> <li>실시간 추정으로 정책(\(π\))에 환경 특성 벡터(\(z_t\)) 제공</li> <li>비동기 병렬 실행 <ul> <li>Base policy(\(π\)): 100Hz 실행</li> <li>Adaptation module(\(φ\)): 10Hz 실행</li> <li>중앙 클럭 없이 독립 실행</li> </ul> </li> <li>잠재 벡터(\(z_t\))를 기본 정책에 전달하여 행동(\(a_t\)) 예측 지원</li> </ul> </li> </ul> <hr/> <h3 id="6-기존-접근법과의-비교">6. 기존 접근법과의 비교</h3> <ul> <li><strong>기존 접근법</strong> <ul> <li>새로운 환경에서 소규모 데이터셋 수집 후 정책 적응</li> <li>물리적 매개변수 (마찰 등) 또는 잠재 인코딩 활용</li> </ul> </li> <li><strong>기존 접근법의 문제점</strong> <ul> <li>초기 데이터셋 수집 중 낙상 및 로봇 손상 위험</li> </ul> </li> <li><strong>RMA 접근법의 장점</strong> <ul> <li>잠재 벡터(\(z_t\))의 빠른 추정</li> <li>즉각적 정책 적응을 통해 낙상 방지</li> </ul> </li> </ul> <hr/> <h3 id="7-rma의-주요-특징-및-핵심-원리">7. RMA의 주요 특징 및 핵심 원리</h3> <hr/> <h4 id="71-base-policy">7.1 Base Policy</h4> <ul> <li><strong>기존 접근법과의 유사점</strong> <ul> <li>환경 매개변수를 추가 인자로 활용한 강화 학습(RL) 훈련</li> </ul> </li> <li><strong>새로운 측면</strong> <ul> <li>다양한 지형 생성기 사용</li> <li>생체에너지학에서 영감을 받은 자연스러운 보상 함수 사용</li> <li>Reference demonstrations 없이 보행 정책 학습 가능</li> </ul> </li> </ul> <hr/> <h4 id="72-adaptation-module의-작동-원리">7.2 Adaptation Module의 작동 원리</h4> <ul> <li><strong>System Identification의 원리 활용</strong> <ul> <li>최적화 문제로서의 시스템 식별 접근</li> <li>신경망을 사용한 입력-출력 관계 근사</li> </ul> </li> <li><strong>완벽한 시스템 식별의 불필요성</strong> <ul> <li>Extrinsics Vector(\(z_t\)): 환경 매개변수의 저차원 비선형 투영</li> <li>정확한 ‘정답’ 벡터가 아닌 ‘올바른 행동’을 유도하는 벡터로 최적화</li> </ul> </li> <li><strong>다양한 훈련 상황 제공</strong> <ul> <li>프랙탈 지형 생성기 사용</li> <li>질량, 마찰 등 매개변수 무작위화</li> <li>다양한 물리적 맥락에서의 보행 반응 훈련</li> </ul> </li> </ul> <hr/> <h3 id="8-실제-환경에서의-성능-평가">8. 실제 환경에서의 성능 평가</h3> <ul> <li><strong>다양한 지형에서의 검증</strong> <ul> <li>미끄러운 표면</li> <li>불규칙한 지형</li> <li>변형 가능한 표면 (스펀지, 매트리스 등)</li> <li>자연 지형 (잔디, 긴 식물, 콘크리트, 자갈, 바위, 모래 등)</li> </ul> </li> </ul> <hr/> <h3 id="9-rma의-주요-기여">9. RMA의 주요 기여</h3> <ul> <li>실시간 적응 모듈을 통한 환경 변화 대응</li> <li>데이터셋 의존 없이 즉각적인 정책 조정</li> <li>강건한 보행 정책의 다양한 환경 적용 가능성</li> </ul> <hr/> <h2 id="ii-related-work">II Related Work</h2> <hr/> <h3 id="1-전통적인-제어-기반-접근법-control-based-methods">1. 전통적인 제어 기반 접근법 (Control-Based Methods)</h3> <ul> <li><strong>주요 로봇 사례</strong> <ul> <li>MIT Cheetah 3 <ul> <li>정규화된 <strong>모델 예측 제어 (MPC)</strong> 사용</li> <li>단순화된 동역학 활용</li> <li>고속 이동 및 장애물 점프 가능</li> </ul> </li> <li>ANYmal 로봇 <ul> <li>매개변수화된 제어기 최적화</li> <li>역진자 모델 기반 계획 수행</li> </ul> </li> </ul> </li> <li><strong>한계점</strong> <ul> <li>정확한 실제 동역학 모델 요구</li> <li>로봇에 대한 사전 지식 필요</li> <li>보행 및 행동 수동 튜닝 요구</li> </ul> </li> <li><strong>개선 방안</strong> <ul> <li>제어기 최적화 및 MPC 결합 <ul> <li>문제 일부 완화</li> <li>여전히 과제별 특징 엔지니어링 필요</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="2-학습-기반-보행-접근법-learning-for-legged-locomotion">2. 학습 기반 보행 접근법 (Learning for Legged Locomotion)</h3> <ul> <li><strong>초기 시도</strong> <ul> <li>DARPA Learning Locomotion Program</li> </ul> </li> <li><strong>최근 동향</strong> <ul> <li>심층 강화 학습(RL)의 도입 <ul> <li>인간 전문성 의존도 감소</li> <li>시뮬레이션에서 우수한 결과 도출</li> </ul> </li> </ul> </li> <li><strong>한계점</strong> <ul> <li>정책의 현실 세계 전이 어려움</li> </ul> </li> <li><strong>해결책</strong> <ul> <li>현실 세계에서 직접 훈련 <ul> <li>단순한 환경에 한정됨</li> <li>복잡한 환경에서는 비효율적 및 안전 문제 발생</li> </ul> </li> </ul> </li> </ul> <hr/> <h3 id="3-sim-to-real-강화-학습-sim-to-real-reinforcement-learning">3. Sim-to-Real 강화 학습 (Sim-to-Real Reinforcement Learning)</h3> <ul> <li><strong>Domain Randomization</strong> <ul> <li>다양한 환경 매개변수 및 센서 노이즈 사용</li> <li>강건한 정책 학습 가능</li> <li>최적성(Optimality) 희생 → 과도하게 보수적인 정책 생성</li> </ul> </li> <li><strong>Simulation Accuracy Improvement</strong> <ul> <li>모터 모델 개선 <ul> <li>Tan et al.: 선형 함수로 모터 데이터 피팅</li> <li>Hwangbo et al.: 신경망으로 액추에이터 모델 매개변수화</li> </ul> </li> </ul> </li> <li><strong>Limitations</strong> <ul> <li>초기 데이터 수집 필요</li> <li>새로운 환경마다 재조정 필요</li> </ul> </li> </ul> <hr/> <h3 id="4-시스템-식별-및-적응-system-identification-and-adaptation">4. 시스템 식별 및 적응 (System Identification and Adaptation)</h3> <ul> <li><strong>Online System Identification</strong> <ul> <li>시뮬레이션에서 훈련된 모듈을 통해 매개변수 추정</li> <li>진화 알고리즘을 사용한 직접 최적화</li> </ul> </li> <li><strong>Latent Embedding 활용</strong> <ul> <li>저차원 잠재 임베딩을 통해 시스템 매개변수 표현</li> <li>실제 환경 롤아웃 기반 최적화 <ul> <li>정책 그래디언트 방법 사용</li> <li>베이지안 최적화 적용</li> <li>무작위 탐색 활용</li> </ul> </li> </ul> </li> <li><strong>Meta-Learning</strong> <ul> <li>빠른 온라인 적응을 위한 정책 네트워크 초기화 학습</li> </ul> </li> <li><strong>Limitations</strong> <ul> <li>여러 실제 환경 롤아웃 필요</li> <li>현실 세계에서의 최적화 비용 및 시간 문제</li> </ul> </li> </ul> <hr/> <h3 id="5-요약-및-비교">5. 요약 및 비교</h3> <ul> <li><strong>전통적 제어 접근법:</strong> 정확한 모델과 수작업 튜닝 요구</li> <li><strong>학습 기반 접근법:</strong> 시뮬레이션 성능 우수, 현실 전이 어려움</li> <li><strong>Sim-to-Real 접근법:</strong> 도메인 무작위화와 시뮬레이션 정확도 향상</li> <li><strong>시스템 식별 및 적응:</strong> 저차원 잠재 임베딩과 메타 학습 활용</li> </ul> <p><strong>결론:</strong> RMA는 기존 접근법의 한계를 극복하며 다양한 환경에서 강건한 보행 정책을 실시간으로 적응 가능하게 함.</p> <hr/> <h2 id="iii-rapid-motor-adaptation">III Rapid Motor Adaptation</h2> <hr/> <h3 id="1-base-policy">1. Base Policy</h3> <hr/> <h4 id="11-base-policy">1.1 Base Policy</h4> \[a_t = \pi(x_t, a_{t-1}, z_t)\] <ul> <li><strong>Input</strong> <ul> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> <li>extrinsics vector: \(z_t \in \mathbb{R}^8\) <ul> <li>form: \(z_t=\mu(e_t)\)</li> <li>environment vector \(e_t \in \mathbb{R}^{17}\)</li> <li>environment factor endoder: \(\mu\)</li> </ul> </li> </ul> </li> <li><strong>Ouput</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> <li>\(a_t\)는 12개의 로봇 관절이 원하는 위치로 설정된 값으로, PD 제어기를 사용하여 토크로 변환</li> </ul> </li> </ul> <hr/> <h4 id="12-정책-학습">1.2 정책 학습</h4> <ul> <li><strong>네트워크 구조:</strong> <ul> <li>정책 네트워크(\(\pi\)) 및 인코더(\(\mu\)): 다층 퍼셉트론(MLP)으로 구현</li> </ul> </li> <li><strong>End-to-End 훈련:</strong> Model-free RL</li> <li><strong>목표:</strong> 정책의 기대 반환(\(J(\pi)\)) 최대화</li> </ul> \[J(\pi) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[\sum_{t=0}^{T-1} \gamma^t r_t \right]\] <ul> <li><strong>경로(\(\tau\)):</strong> 정책(\(\pi\))을 따르는 에이전트의 상태, 행동, 보상 시퀀스</li> </ul> <hr/> <h4 id="13-자연적-제약을-통한-안정적인-보행-stable-gait-through-natural-constraints">1.3 자연적 제약을 통한 안정적인 보행 (Stable Gait through Natural Constraints)</h4> <ul> <li><strong>자연적 제약 활용:</strong> <ul> <li><strong>생체에너지학 기반 보상 함수:</strong> 작업 최소화, 지면 충격 감소 목표</li> <li><strong>고르지 않은 지형에서 훈련:</strong> 추가 보상 없이 보행 강건성 확보</li> </ul> </li> <li><strong>학습 환경:</strong> <ul> <li>인공적인 시뮬레이션 노이즈 대신 자연적 제약 적용</li> </ul> </li> <li><strong>정책 전이:</strong> <ul> <li>단순 환경(콘크리트, 나무 바닥)으로 자연스럽게 전이</li> <li>추가적인 미세 조정(finetuning) 불필요</li> </ul> </li> </ul> <hr/> <h4 id="14-기존-sim-to-real-접근법과의-차이점">1.4 기존 Sim-to-Real 접근법과의 차이점</h4> <ul> <li><strong>기존 접근법:</strong> <ul> <li>시뮬레이션과 현실 간 보정(calibration) 수행 [51, 23]</li> <li>현실 세계에서 정책 미세 조정(finetuning) 필요 [41]</li> </ul> </li> <li><strong>RMA 접근법:</strong> <ul> <li>적응 모듈(Adaptation Module)을 통해 단순 환경에서 복잡한 지형으로 확장 가능</li> </ul> </li> </ul> <hr/> <h4 id="15-강화-학습-보상-rl-rewards">1.5 강화 학습 보상 (RL Rewards)</h4> <p>보상 함수는 에이전트가 최대 0.35 m/s 속도로 전진하도록 장려 및 불규칙하고 비효율적인 움직임에 패널티 부여.</p> <ul> <li><strong>선형 속도: \(\mathbf{v}\)</strong></li> <li><strong>자세: \(\mathbf{\theta}\)</strong></li> <li><strong>각속도: \(\omega\)</strong></li> <li><strong>관절 각도: \(\mathbf{q}\)</strong></li> <li><strong>관절 속도: \(\dot{\mathbf{q}}\)</strong></li> <li><strong>관절 토크: \(\tau\)</strong></li> <li><strong>발의 지면 반력: \(\mathbf{f}\)</strong></li> <li><strong>발의 속도: \(\mathbf{v}_\mathbf{f}\)</strong></li> <li><strong>발 접촉 이진 벡터: \(\mathbf{g}\)</strong></li> </ul> <p>시간 \(t\)에서의 보상은 다음 요소들의 합으로 정의됩니다:</p> <ol> <li><strong>전진:</strong> \(\operatorname{min}(v^t_x, 0.35)\)</li> <li><strong>측면 이동 및 회전:</strong> \(- \| v_{y}^{t} \|^2 - \| \omega_{\text{yaw}}^{t} \|^2\)</li> <li><strong>작업:</strong> \(- \| \boldsymbol{\tau}^{T} \cdot (\mathbf{q}^{t} - \mathbf{q}^{t-1}) \|\)</li> <li><strong>지면 충격:</strong> \(- \| \mathbf{f}^{t} - \mathbf{f}^{t-1} \|^2\)</li> <li><strong>부드러움:</strong> \(- \| \mathbf{\tau}^{t} - \mathbf{\tau}^{t-1} \|^2\)</li> <li><strong>행동 크기:</strong> \(- \| \mathbf{a}^{t} \|^2\)</li> <li><strong>관절 속도:</strong> \(- \| \dot{\mathbf{q}}^{t} \|^2\)</li> <li><strong>자세 안정성:</strong> \(- \| \boldsymbol{\theta}^{t}_{\text{roll, pitch}} \|^2\)</li> <li><strong>Z축 가속도:</strong> \(- \| v^t_z \|^2\)</li> <li><strong>발 미끄러짐:</strong> \(- \| \operatorname{diag}(\mathbf{g}^{t}) \cdot \mathbf{v}_{\mathbf{f}}^{t} \|^2\)</li> </ol> <p>각 보상 항목의 스케일링 계수: 20, 21, 0.002, 0.02, 0.001, 0.07, 0.002, 1.5, 2.0, 0.8.</p> <hr/> <h4 id="16-training-curriculum">1.6 Training Curriculum</h4> <p>위 보상 함수로 학습 시, 움직직에 대한 패널티로 인해 제자리에서 머무르는 현상 발생 가능. 이를 방지하기 위해 다음 전략 사용:</p> <ol> <li><strong>패널티 계수:</strong> 초기에는 매우 작은 패널티 계수로 시작, 훈련이 진행됨에 따라 점진적으로 계수 증가.</li> <li><strong>환경 변화:</strong> 질량, 마찰, 모터 힘 등의 난이도를 선형적으로 증가.</li> <li><strong>지형 난이도:</strong> 지형에 대한 별도의 커리큘럼 없음, 고정된 난이도에서 무작위 지형 프로필 샘플링.</li> </ol> <hr/> <h3 id="2-adaptation-module">2. Adaptation Module</h3> <hr/> <h4 id="21-adaptation-module">2.1 Adaptation Module</h4> \[\hat{z_t} = \phi(x_{t-k:t-1}, a_{t-k:t-1})\] <ul> <li><strong>역할</strong> <ul> <li>실제 환경에서 \(z_t\)를 온라인 추정</li> <li>privileged environment configuration, <strong>\(e_t\)</strong> 없이 작동</li> </ul> </li> <li><strong>Input</strong> <ul> <li>history of robot’s states: \(x_{t-k:t-1}\)</li> <li>history of robot’s actions(\(a_{t-k:t-1}\))</li> </ul> </li> <li><strong>Output</strong> <ul> <li>predicted extrinsics vector: \(\hat{z_t}\)</li> </ul> </li> <li><strong>Hyperparameter</strong> <ul> <li>k=50 사용 (약 0.5초에 해당)</li> </ul> </li> </ul> <hr/> <h4 id="22-모델-학습">2.2 모델 학습</h4> \[\operatorname{MSE}(\hat{z}_{t}, z_{t}) = \| \hat{z}_{t} - z_{t} \|^2\] <ul> <li><strong>네트워크 구조</strong>: <ul> <li><strong>1D CNN</strong> 사용: 시간적 상관관계 포착</li> </ul> </li> </ul> <hr/> <h4 id="23-데이터-수집-방식">2.3 데이터 수집 방식</h4> <ul> <li><strong>문제점</strong> <ul> <li>Base policy(\(\pi\))을 기반으로 한 데이터셋은 최적 경로만 포함</li> <li>실제 배포 시 발생할 편차를 충분히 다루지 못함</li> </ul> </li> <li><strong>해결책: On-Policy 데이터 사용</strong> <ul> <li>무작위로 초기화된 <strong>Adaptation module(\(\phi\))</strong> 사용하여 데이터 수집</li> <li>상태-행동 이력과 목표 잠재 벡터(\(z_t\)) 쌍으로 학습 진행</li> </ul> </li> <li><strong>훈련 절차</strong> <ul> <li>Base policy(\(\pi\))을 \(\hat{z_t}\)로 롤아웃</li> <li>state action history과 ground truth \(z_t\) 생성</li> <li>이 과정을 반복하여 수렴 유도</li> </ul> </li> <li><strong>강건성 확보 메커니즘</strong> <ul> <li>랜덤 초기화된 <strong>Adaptation module</strong>(\(\phi\)) 사용</li> <li><strong>imperfect prediction</strong>(\(\hat{z_t}\)) 수용</li> <li>훈련 중 충분한 탐색 경로 확보</li> </ul> </li> </ul> <hr/> <h3 id="3-asynchronous-deployment">3. Asynchronous Deployment</h3> <hr/> <h4 id="31-비동기식-배포">3.1 비동기식 배포</h4> <ul> <li><strong>훈련 및 배포</strong> <ul> <li>완전한 시뮬레이션 기반 훈련</li> <li>현실 세계로 직접 배포 (수정 및 미세 조정 불필요)</li> </ul> </li> <li><strong>비동기식 실행 구조</strong> <ul> <li>두 하위 시스템의 비동기적 실행</li> <li>서로 다른 주기로 작동 → 온보드 컴퓨팅 부담 최소화</li> </ul> </li> </ul> <hr/> <h4 id="32-adaptation-module">3.2 Adaptation Module</h4> <ul> <li><strong>작동 주기:</strong> 10Hz</li> <li><strong>입력:</strong> 최근 50 타임 스텝의 상태 및 행동 이력</li> <li><strong>출력:</strong> Extrinsics Vector, \(\hat{z_t}\)</li> <li><strong>특징:</strong> 비교적 느리게 업데이트되나 성능에 영향 없음</li> </ul> <hr/> <h4 id="33-base-policy">3.3 Base Policy</h4> <ul> <li><strong>작동 주기:</strong> 100Hz</li> <li><strong>입력:</strong> <ul> <li>the most recent \(\hat{z_t}\) generated by the adaptation module</li> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> </ul> </li> <li><strong>Output</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> </ul> </li> <li><strong>특징:</strong> <ul> <li>빠른 주기로 작동</li> <li>적응 모듈의 비동기적 업데이트 수용 가능</li> </ul> </li> </ul> <hr/> <h4 id="34-단일-정책-설계의-한계점">3.4 단일 정책 설계의 한계점</h4> <ul> <li><strong>상태 및 행동 이력을 직접 입력받는 단일 정책 접근법의 문제점</strong> <ul> <li><strong>비자연스러운 보행 패턴 발생</strong></li> <li><strong>시뮬레이션 성능 저하</strong></li> <li><strong>온보드 컴퓨팅 한계:</strong> 10Hz에서만 작동 가능</li> <li><strong>비동기적 설계 불가능:</strong> 하위 시스템 간 동기화 및 보정 필요</li> </ul> </li> </ul> <hr/> <h3 id="35-비동기-설계의-장점">3.5 비동기 설계의 장점</h3> <ul> <li>상대적으로 느리게 변하는 Extrinsics Vector(\(\hat{z_t}\))와 빠르게 변하는 로봇 상태(\(x_t\))의 분리</li> <li>효율적인 온보드 컴퓨팅 자원 활용 가능</li> <li>동기화 및 추가 보정 불필요 → 원활한 Real-world 배포 가능</li> </ul> <hr/> <h2 id="iv-experimental-setup">IV Experimental Setup</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table1-480.webp 480w,/assets/img/rma/table1-800.webp 800w,/assets/img/rma/table1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/table1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h3 id="1-environment-details">1. Environment Details</h3> <hr/> <h4 id="11-hardware-details">1.1 Hardware Details</h4> <ul> <li><strong>로봇 플랫폼:</strong> Unitree의 A1 로봇</li> <li><strong>특징:</strong> <ul> <li>중형 크기, 저비용 사족 보행 로봇</li> <li><strong>자유도:</strong> 총 18자유도 (12자유도는 액추에이터 작동, 다리당 3개 모터)</li> <li><strong>무게:</strong> 약 12kg</li> </ul> </li> <li><strong>센서 입력:</strong> <ul> <li><strong>모터 인코더:</strong> 관절 위치 및 속도 측정</li> <li><strong>IMU 센서:</strong> 롤(Roll), 피치(Pitch) 측정</li> <li><strong>발 센서:</strong> 이진화된 발 접촉 상태</li> </ul> </li> <li><strong>제어 방식:</strong> <ul> <li><strong>관절 위치 제어 사용</strong></li> <li><strong>PD 컨트롤러로 토크 변환</strong> <ul> <li>Gain: \(K_p = 55, K_d = 0.8\)</li> </ul> </li> </ul> </li> </ul> <hr/> <h4 id="12-simulation-setup">1.2 Simulation Setup</h4> <ul> <li><strong>시뮬레이터:</strong> RaiSim 사용</li> <li><strong>로봇 모델:</strong> Unitree A1 URDF 파일 사용</li> <li><strong>지형 생성기:</strong> 프랙탈 지형 생성기 <ul> <li>fractal octaves: 2</li> <li>fractal lacunarity: 2.0</li> <li>fractal gain: 0.25</li> <li>Z-scale: 0.27</li> </ul> </li> <li><strong>에피소드 길이:</strong> 최대 1000 steps <ul> <li>조기 종료 조건: <ul> <li>높이 &lt; 0.28m</li> <li>롤 각도 &gt; 0.4 radians</li> <li>피치 각도 &gt; 0.2 radians</li> </ul> </li> </ul> </li> <li><strong>제어 주파수:</strong> 100 Hz</li> <li><strong>시뮬레이션 시간 간격:</strong> 0.025s = 1/40</li> </ul> <hr/> <h4 id="13-state-action-space">1.3 State-Action Space</h4> <ul> <li><strong>상태 벡터(</strong>\(x_t \in \mathbb{R}^{30}\)<strong>)</strong> <ul> <li><strong>관절 위치:</strong> 12개 값</li> <li><strong>관절 속도:</strong> 12개 값</li> <li><strong>몸체 롤 및 피치:</strong> 2개 값</li> <li><strong>발 접촉 상태:</strong> 4개 값</li> </ul> </li> <li><strong>행동 벡터(</strong>\(a \in \mathbb{R}^{12}\)<strong>)</strong> <ul> <li><strong>관절 목표 위치 예측:</strong> \(a = \hat{q} \in \mathbb{R}^{12}\)</li> <li><strong>토크 변환:</strong> PD 컨트롤러 사용 <ul> <li>수식: \(\tau = K_p (\hat{q} - q) + K_d (\hat{\dot{q}} - \dot{q})\)</li> </ul> </li> <li><strong>이득 값:</strong> \(K_p\) 및 \(K_d\) (수동 설정)</li> <li><strong>목표 관절 속도:</strong> \(\hat{\dot{q}} = 0\)</li> </ul> </li> </ul> <hr/> <h4 id="14-environmental-variations">1.4 Environmental Variations</h4> <ul> <li><strong>환경 벡터(\(e_t \in \mathbb{R}^{17}\))</strong> <ul> <li><strong>질량 및 로봇 내 위치:</strong> 3차원</li> <li><strong>모터 강도:</strong> 12차원</li> <li><strong>마찰 계수:</strong> 스칼라 값</li> <li><strong>지형 높이:</strong> 스칼라 값</li> </ul> </li> <li><strong>지형 높이 측정 방법:</strong> <ul> <li>각 발 아래 지형 높이 값을 소수점 첫째 자리로 이산화</li> <li>네 발 중 최대값 사용</li> </ul> </li> <li><strong>지형 프로파일 특성:</strong> <ul> <li>고정된 난이도</li> <li>로컬 지형 높이의 동적 변화</li> </ul> </li> </ul> <hr/> <h3 id="2-training-details">2. Training Details</h3> <hr/> <h4 id="21-base-policy-and-environment-factor-encoder-architecture">2.1 Base Policy and Environment Factor Encoder Architecture</h4> <ul> <li><strong>Base Policy</strong> <ul> <li><strong>구조:</strong> 3-layer MLP</li> <li><strong>Hidden layer sizes:</strong> 128</li> </ul> </li> <li><strong>Environment Factor Encoder</strong> <ul> <li><strong>구조:</strong> 3층 다층 퍼셉트론(MLP)</li> <li><strong>Hidden layer sizes:</strong> 256, 128</li> </ul> </li> </ul> <hr/> <h4 id="22-adaptation-module-architecture">2.2 Adaptation Module Architecture</h4> <ul> <li><strong>1단계: 상태 및 행동 임베딩</strong> <ul> <li><strong>구조:</strong> 2-layer MLP</li> </ul> </li> <li><strong>2단계: 시간적 상관관계 학습</strong> <ul> <li><strong>구조:</strong> 3-layer 1D CNN</li> <li><strong>입력-출력 특성:</strong> <ul> <li><strong>1층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 8, 스트라이드: 4</li> <li><strong>2층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1</li> <li><strong>3층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1</li> </ul> </li> </ul> </li> <li><strong>3단계: 최종 예측</strong> <ul> <li><strong>구조</strong>: Linear projection</li> </ul> </li> </ul> <hr/> <h2 id="v-results-and-analysis">V Results and Analysis</h2> <p><strong>비교 대상:</strong></p> <ul> <li><strong>시뮬레이션:</strong> 여러 기준 모델(Table II)</li> <li><strong>현실 환경:</strong> 제조사 제공 A1 컨트롤러(Figure 3)</li> <li><strong>다양한 야외 지형:</strong> 다양한 환경에서 RMA 테스트(Figure 1)</li> </ul> <hr/> <h3 id="1-baselines">1. Baselines</h3> <ol> <li><strong>A1 컨트롤러 (A1 Controller)</strong> <ul> <li>제조사 기본 컨트롤러</li> <li>힘 기반 제어 및 모델 예측 제어(MPC) 사용</li> </ul> </li> <li><strong>Robustness through Domain Randomization</strong> <ul> <li>Extrinsics vector(\(z_t\)) 없이 학습된 기본 정책</li> <li>훈련 범위 내 변화를 견디도록 설계</li> </ul> </li> <li><strong>Expert Adaptation Policy</strong> <ul> <li>시뮬레이션에서 true extrinsics vector(\(z_t\)) 사용</li> <li>RMA의 이론적 상한 성능 제공</li> </ul> </li> <li><strong>RMA w/o Adaptation</strong> <ul> <li>Adaptation module 없이 base policy 단독 평가</li> <li>Adaptation module의 중요성 분석</li> </ul> </li> <li><strong>System Identification</strong> <ul> <li>Extrinsics vector(\(\hat{z_t}\)) 대신 system parameters \(\hat{e_t}\) 직접 예측</li> </ul> </li> <li><strong>AWR (Advantage Weighted Regression for Domain Adaptation)</strong> <ul> <li>오프라인으로 Extrinsics vector(\(\hat{z_t}\)) 최적화</li> <li>실제 환경 롤아웃을 기반으로 테스트 환경에 적응</li> </ul> </li> </ol> <p><strong>학습 조건 통일:</strong></p> <ul> <li>동일한 네트워크 아키텍처 사용</li> <li>동일한 보상 함수 사용</li> <li>동일한 하이퍼파라미터 적용</li> </ul> <hr/> <h3 id="2-metrics">2. Metrics</h3> <ol> <li><strong>Time-to-Fall (TTF):</strong> <ul> <li>최대 에피소드 길이로 나눈 <strong>시간당 추락 비율</strong></li> <li>0~1 범위 정규화</li> </ul> </li> <li><strong>평균 전진 보상 (Average Forward Reward):</strong></li> <li><strong>성공률 (Success Rate):</strong></li> <li><strong>이동 거리 (Distance Covered):</strong></li> <li><strong>적응에 필요한 탐색 샘플 수 (Exploration Samples Needed for Adaptation):</strong></li> <li><strong>토크 사용량 (Torque Applied):</strong></li> <li><strong>부드러움 (Smoothness):</strong> 토크 미분 값</li> <li><strong>지면 충격 (Ground Impact):</strong> 발이 지면에 미치는 충격 수준</li> </ol> <hr/> <h3 id="3-indoor-experiments">3. Indoor Experiments</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig3-480.webp 480w,/assets/img/rma/fig3-800.webp 800w,/assets/img/rma/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>비교 대상:</strong> <ul> <li><strong>RMA (Rapid Motor Adaptation)</strong></li> <li><strong>A1 기본 컨트롤러</strong></li> <li><strong>적응 모듈 제거된 RMA (RMA w/o Adaptation)</strong></li> </ul> </li> <li><strong>비교 목적:</strong> 로봇 하드웨어 손상 최소화</li> <li><strong>실험 조건:</strong> <ul> <li>각 방법당 5회 실험</li> <li>심각한 실패 발생 시 2회만 진행 후 실패로 기록</li> </ul> </li> <li><strong>평가 지표:</strong> <ul> <li>성공률 (Success Rate)</li> <li>낙하 시간 (TTF: Time-to-Fall)</li> <li>이동 거리 (Distance Covered)</li> </ul> </li> </ul> <hr/> <h4 id="31-setups">3.1 Setups</h4> <ol> <li><strong>n-kg 페이로드 (n-kg Payload)</strong> <ul> <li><strong>목표:</strong> n-kg 하중을 싣고 300cm 걷기</li> </ul> </li> <li><strong>StepUp-n</strong> <ul> <li><strong>목표:</strong> n-cm 높이의 계단 오르기</li> <li><strong>평가 지표:</strong> 성공률만 기록</li> </ul> </li> <li><strong>Uneven Foam</strong> <ul> <li><strong>목표:</strong> 중앙이 솟아있는 스펀지 위 180cm 걷기</li> </ul> </li> <li><strong>Mattress</strong> <ul> <li><strong>목표:</strong> 메모리폼 매트리스 위 60cm 걷기</li> </ul> </li> <li><strong>StepDown-n</strong> <ul> <li><strong>목표:</strong> n-cm 높이의 계단 내려오기</li> <li><strong>평가 지표:</strong> 성공률만 기록</li> </ul> </li> <li><strong>Incline</strong> <ul> <li><strong>목표:</strong> 6도 경사로 오르기</li> </ul> </li> <li><strong>Oily Surface</strong> <ul> <li><strong>목표:</strong> 미끄러운 오일이 뿌려진 표면 건너기</li> </ul> </li> </ol> <hr/> <h4 id="32-results">3.2 Results</h4> <ul> <li><strong>RMA의 성능:</strong> <ul> <li><strong>모든 환경에서 높은 성공률 달성</strong></li> <li><strong>A1 컨트롤러 대비 월등한 성능 발휘</strong></li> </ul> </li> <li><strong>적응 모듈의 중요성:</strong> <ul> <li>적응 모듈 비활성화 시 성능 크게 저하</li> <li>많은 과제에서 문제 해결 불가</li> </ul> </li> <li><strong>A1 컨트롤러의 한계:</strong> <ul> <li><strong>Uneven Foam:</strong> 불안정한 지지대에서 불안정함 발생</li> <li><strong>StepUp/StepDown:</strong> 높은 단차에서 실패 빈번</li> <li><strong>Payload:</strong> 5kg 이상의 하중에서는 처짐 발생 및 낙하</li> </ul> </li> <li><strong>RMA의 강점:</strong> <ul> <li>최대 12kg (로봇 체중의 100%) 하중 운반 성공</li> <li>높이를 유지하며 안정적 보행 가능</li> </ul> </li> <li><strong>RMA w/o 적응 모듈:</strong> <ul> <li>대부분 낙하하지 않음</li> <li>전진 움직임은 거의 없음</li> </ul> </li> <li><strong>Oily Surface</strong> <ul> <li><strong>RMA:</strong> 성공적으로 미끄러운 지형 통과</li> <li><strong>RMA w/o Adaptation:</strong> 나무 바닥에서는 별도 미세 조정 없이도 성공적 보행 가능</li> </ul> </li> </ul> <hr/> <h3 id="4-outdoor-experiments">4. Outdoor Experiments</h3> <ul> <li><strong>목표:</strong> RMA의 성능을 다양한 야외 환경에서 검증</li> <li><strong>테스트 환경:</strong> <ul> <li>모래 (Sand)</li> <li>진흙 (Mud)</li> <li>흙길 (Dirt)</li> <li>높은 식물 지대 (Tall Vegetation)</li> <li>덤불 (Bush)</li> <li>계단 (Stairs)</li> <li>건설 폐기물 (Construction Debris)</li> </ul> </li> </ul> <hr/> <h4 id="41-results">4.1 Results</h4> <ol> <li><strong>모래, 진흙, 흙길 (Sand, Mud, Dirt)</strong> <ul> <li><strong>성공률:</strong> 100%</li> <li><strong>도전 과제:</strong> <ul> <li>발이 빠지거나 달라붙는 문제 발생</li> <li>동적 발판 조정 필요</li> </ul> </li> </ul> </li> <li><strong>높은 식물 지대 및 덤불 (Tall Vegetation, Bush)</strong> <ul> <li><strong>성공률:</strong> 100%</li> <li><strong>도전 과제:</strong> <ul> <li>발이 장애물에 얽혀 불안정 발생</li> <li>주기적 발판 불안정성 해결 필요</li> <li>장애물에 맞서 강력한 추진력 필요</li> </ul> </li> </ul> </li> <li><strong>하이킹 계단 (Stairs on Hiking Trail)</strong> <ul> <li><strong>성공률:</strong> 70%</li> <li><strong>도전 과제:</strong> <ul> <li>훈련 중 계단을 경험하지 못함</li> <li>불규칙한 발판과 높낮이 조정 필요</li> </ul> </li> </ul> </li> <li><strong>건설 폐기물 (Construction Debris)</strong> <ul> <li><strong>하위 실험:</strong> <ul> <li>진흙 더미 (Mud Pile): 성공률 100%</li> <li>시멘트 더미 (Cement Pile): 성공률 80%</li> <li>자갈 더미 (Pebble Pile): 성공률 80%</li> </ul> </li> <li><strong>도전 과제:</strong> <ul> <li>급경사 및 측면 경사면</li> <li>불규칙한 발판과 무게 균형 필요</li> </ul> </li> </ul> </li> </ol> <hr/> <h3 id="5-simulation-results">5. Simulation Results</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table2-480.webp 480w,/assets/img/rma/table2-800.webp 800w,/assets/img/rma/table2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/table2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>비교 대상:</strong> 여러 기준 방법 (Table II)</li> <li><strong>훈련 및 테스트 설정:</strong> <ul> <li><strong>훈련 및 테스트 매개변수:</strong> Table I에 따라 샘플링</li> <li><strong>재샘플링 확률:</strong> <ul> <li>훈련: 스텝당 0.004</li> <li>테스트: 스텝당 0.01</li> </ul> </li> </ul> </li> <li><strong>평가 방법:</strong> <ul> <li><strong>정책 초기화:</strong> 무작위로 3회 초기화된 정책 사용</li> <li><strong>에피소드 수:</strong> 초기화당 1000회 에피소드 실행</li> </ul> </li> <li><strong>성능 평가:</strong> 평균값 도출</li> </ul> <hr/> <h4 id="51-results">5.1 Results</h4> <ol> <li><strong>AWR (Advantage Weighted Regression)</strong> <ul> <li><strong>적응 속도 저하:</strong> 변화하는 환경에 느린 적응</li> <li><strong>성능 저하:</strong> 지속적인 환경 변화에 취약</li> </ul> </li> <li><strong>Robust (도메인 무작위화 기반 강건성)</strong> <ul> <li><strong>환경 특성 무시:</strong> Extrinsics vector(\(z_t\)) 사용하지 않음</li> <li><strong>보수적 정책:</strong> 성능 저하 발생</li> </ul> </li> <li><strong>System Identification (SysID)</strong> <ul> <li><strong>환경 매개변수(\(e_t\)) 추정의 어려움:</strong> 명시적 매개변수 추정 어려움</li> <li><strong>불필요성:</strong> 높은 성능 달성을 위해 필수적이지 않음</li> </ul> </li> <li><strong>RMA w/o Adaptation (적응 모듈 미사용 RMA)</strong> <ul> <li><strong>성능 급감:</strong> 적응 모듈 없이는 성능 저하 심각</li> </ul> </li> </ol> <hr/> <h3 id="6-adaptation-analysis">6. Adaptation Analysis</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig4-480.webp 480w,/assets/img/rma/fig4-800.webp 800w,/assets/img/rma/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rma/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>목표:</strong> 미끄러운 표면에서 RMA의 보행 패턴, 토크 프로파일, Extrinsics vector(\(\hat{z_t}\)) 분석</li> <li><strong>실험 조건:</strong> <ul> <li><strong>표면:</strong> 플라스틱 바닥에 오일 도포</li> <li><strong>로봇 발:</strong> 플라스틱으로 덮음</li> </ul> </li> </ul> <hr/> <h4 id="61-results">6.1 Results</h4> <ul> <li><strong>성공률:</strong> 90%</li> <li><strong>분석 항목:</strong> <ul> <li>무릎 토크 프로파일 (Torque Profile of Knee)</li> <li>보행 패턴 (Gait Pattern)</li> <li>Extrinsics vector(\(\hat{z_t}\)) 분석: 1번째 및 5번째 성분의 중위수 필터링된 값</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Robotics"/><category term="Robotics,"/><category term="Reinforcement-Learning"/><summary type="html"><![CDATA[RMA: Rapid Motor Adaptation for Legged Robots]]></summary></entry><entry><title type="html">Quantum Virtual Link Generation via Reinforcement Learning</title><link href="https://optreal.github.io/blog/2024/quantum_vl/" rel="alternate" type="text/html" title="Quantum Virtual Link Generation via Reinforcement Learning"/><published>2024-12-22T00:00:00+00:00</published><updated>2024-12-22T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/quantum_vl</id><content type="html" xml:base="https://optreal.github.io/blog/2024/quantum_vl/"><![CDATA[<h2 id="abstract">Abstract</h2> <ul> <li>Quantum networks leverage quantum entanglement as a fundamental building block.</li> <li>When two qubits are entangled, their states exhibit non-classical correlations, enabling novel applications such as quantum key distribution and distributed quantum computing, which are not possible with classical communication.</li> <li>However, <u>quantum entanglement is a probabilistic process heavily dependent on the characteristics of the involved devices</u>, such as optical fibers, lasers, and quantum memories.</li> <li>Managing this process to maintain entanglement with high quality for as long as possible is a <strong>stochastic control problem</strong>.</li> <li>This process can be modeled as a MDP and solved using the RL framework.</li> <li>In this work, we employ RL to develop an <strong>entanglement management policy</strong> that surpasses the current State-of-the-Art policies, particularly in scenarios where precise models of the quantum devices are unavailable.</li> <li>Reference: <a class="citation" href="#a10207249">(Aparicio-Pardo et al., 2023)</a></li> </ul> <hr/> <h2 id="introduction">Introduction</h2> <h3 id="1-양자-인터넷의-등장">1. 양자 인터넷의 등장</h3> <ul> <li>최근 <strong>양자 물리학 원리(quantum physics principles)</strong>가 컴퓨터 네트워크에 적용되며 연구와 산업 분야에서 주목받고 있음</li> <li><strong>IETF(Internet Engineering Task Force)</strong>가 제안한 <strong>양자 인터넷(Quantum Internet)</strong>의 표준화 시도가 이를 증명 <a class="citation" href="#rfc9340">(Kozlowski et al., 2023)</a>, <a class="citation" href="#rfc9583">(Wang et al., 2024)</a></li> <li><strong>양자 얽힘(quantum entanglement)</strong>은 양자 통신(Quantum Communication)을 위한 기본 자원 <ul> <li>이를 통해 <strong>양자 암호 키 분배(quantum key distribution)</strong>와 <strong>분산 양자 컴퓨팅(distributed quantum computing)</strong>과 같은 응용 실현 가능</li> </ul> </li> </ul> <h3 id="2-양자-얽힘의-특성과-문제">2. 양자 얽힘의 특성과 문제</h3> <ul> <li>양자 얽힘은 <strong>확률적 과정(probabilistic process)</strong>으로, 관련 통신 장치(광섬유(optical fiber), 레이저(laser), 양자 메모리(quantum memory) 등)의 특성에 크게 의존</li> <li>얽힘 관리는 <strong>확률 제어 문제(stochastic control problem)</strong>로, 마르코프 결정 과정(Markov Decision Process, MDP)**으로 공식화 가능</li> <li>본 연구에서는 두 원격 통신 노드(remote communication nodes) 간의 얽힘을 설정할 때 DRL의 적용 가능성 조사</li> </ul> <h3 id="3-양자-비트qubit와-얽힘entanglement">3. <strong>양자 비트(Qubit)와 얽힘(Entanglement)</strong></h3> <ul> <li><strong>양자 비트(Qubit)</strong>는 고전 비트(classical bit)의 양자적 대응물 <ul> <li>고전 비트는 “0” 또는 “1”의 상태만 가지지만, 양자 비트는 두 상태의 <strong>중첩(superposition)</strong> 상태를 가짐</li> <li>측정 후 확률에 따라 “0” 또는 “1” 상태로 결정됨</li> </ul> </li> <li><strong>얽힘(Entanglement)</strong>: <ul> <li>두 양자 비트가 얽히면 각 상태를 독립적으로 설명할 수 없음</li> <li>한 쪽 비트 상태가 변하면, 물리적 거리에 관계없이 다른 비트의 상태도 함께 변화</li> <li>얽힘은 양자 암호와 분산 양자 컴퓨팅 같은 비고전적(non-classical) 응용의 핵심 요소</li> </ul> </li> </ul> <h3 id="4-양자-네트워크quantum-network">4. <strong>양자 네트워크(Quantum Network)</strong></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-12-22-quantum_vl/fig1-480.webp 480w,/assets/img/posts/2024-12-22-quantum_vl/fig1-800.webp 800w,/assets/img/posts/2024-12-22-quantum_vl/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-12-22-quantum_vl/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>양자 네트워크는 얽힘 상태를 분배하고 <strong>양자 비트(Qubit)</strong>를 교환할 수 있는 노드(node)들로 구성. <ul> <li>노드들은 <strong>광섬유(optical fiber)</strong> 또는 <strong>위성 레이저 링크(satellite laser link)</strong>로 연결.</li> </ul> </li> <li>얽힘 상태 설정: <ul> <li>두 인접 노드에 위치한 양자 비트 간 얽힘은 <strong>기본 링크(elementary link)</strong>를 구성.</li> <li>두 노드 간 얽힘 성공 확률(\(P_{e_{i,j}}\))은 거리 증가에 따라 지수적으로 감소 <ul> <li>Short-distance entanglements (like A-B, in Fig. 1) are more likely to succeed than long-distance entanglements (like A-C, in Fig. 1)</li> </ul> </li> </ul> </li> <li><strong>가상 링크(Virtual Link)</strong> 생성: <ul> <li><strong><code class="language-plaintext highlighter-rouge">얽힘 교환(Entanglement Swapping)</code></strong>을 통해 두 개의 기본 링크를 결합 <ul> <li>예) 기본 링크 A-B와 B-C를 소비하여 A-C라는 장거리 가상 링크 생성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">얽힘 교환</code>을 수행하는 중간 노드는 <strong>양자 리피터(Quantum Repeater)</strong></li> <li>양자 리피터는 기본 링크들을 <strong>양자 메모리(Quantum Memory)</strong>에 저장 후 사용 <ul> <li>예) B는 A-B와 B-C의 기본 링크를 양자 메모리에 저장 후 사용</li> </ul> </li> </ul> </li> </ul> <h3 id="5-양자-메모리-수명quantum-memory-lifetimes">5. <strong>양자 메모리 수명(Quantum Memory Lifetimes)</strong></h3> <ul> <li>양자 메모리에 저장된 얽힘 상태가 원래 상태를 유지할 확률(<strong>메모리 효율(memory efficiency), \(\eta\)</strong>)은 시간이 지남에 따라 감소 <ul> <li>이 과정은 <strong>데코히어런스(Decoherence)</strong>로 알려짐.</li> <li><code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률(<strong>\(P_s\)</strong>)은 가장 오래된 양자 메모리의 메모리 효율 \(\eta\)에 의존</li> </ul> </li> </ul> <h3 id="6-본-연구의-기여">6. 본 연구의 기여</h3> <ul> <li>양자 가상 링크 생성 과정을 <strong>고전적 MDP(Classical MDP)</strong>로 모델링하고, DRL 알고리즘을 사용하여 최적의 생성 정책(policy)을 도출</li> <li>본 연구는 기본 링크의 <strong>나이(age)</strong>를 추적하는 새로운 방법을 제안: <ul> <li>기존 연구에서는 링크 생성 성공 시점의 타임스탬프, 즉 링크의 나이를 고려하지 않음</li> </ul> </li> </ul> <table class="mbtablestyle table table-striped"> <tbody> <tr> <td><strong>심볼</strong></td> <td><strong>설명</strong></td> <td><strong>비고</strong></td> </tr> <tr> <td>\(P_{e_{i,j}}\)</td> <td>두 노드 간 얽힘 성공 확률</td> <td> </td> </tr> <tr> <td>\(\eta\)</td> <td>양자 메모리 효율</td> <td>양자 메모리에 저장된 얽힘 상태가 원래 상태를 유지할 확률 (저장 시간에 따라 감소, Mims 모델 따름 <a class="citation" href="#Ortu2022a">(Ortu et al., 2022)</a>)</td> </tr> <tr> <td>\(P_s\)</td> <td><code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률</td> <td>가장 오래된 양자 메모리의 메모리 효율, 즉, \(\eta\)에 의존</td> </tr> <tr> <td>\(t_c\)</td> <td>컷오프 시간</td> <td> </td> </tr> </tbody> </table> <hr/> <h2 id="related-works">Related Works</h2> <h3 id="1-quantum-decision-processqdp">1. Quantum Decision Process(QDP)</h3> <ul> <li>Quantum Decision Process(QDP)는 MDP의 양자적 일반화로, Khatri의 박사 논문<a class="citation" href="#10.5555/AAI29111215">(Khatri, 2021)</a>에서 제안됨.</li> <li>양자 컴퓨터의 사용을 전제로 하는 QDP의 주요 구성 요소 -양자 상태(Quantum State) - QDP에서는 상태를 <strong>양자 상태(Quantum State)</strong>로 표현 - 양자 상태는 상태 벡터로 나타나며, 중첩(Superposition)과 얽힘(Entanglement)을 포함할 수 있음 <ul> <li>양자 행동(Quantum Action) <ul> <li>QDP의 행동은 고전적 액션이 아닌 <strong>양자 연산자(Quantum Operator)</strong>로 표현됨</li> <li>이는 유니터리 연산이나 측정과 같은 양자역학적 연산으로 구현되며, 상태에 작용하여 새로운 상태를 생성함</li> </ul> </li> <li>양자 상태 전이 <ul> <li>전이 확률이 고전적 확률 분포 대신 양자 연산에 의해 결정됨</li> <li>양자 연산은 상태를 변환하면서 고전적 시스템과 달리 비선형적이고 중첩된 결과를 생성할 수 있음</li> </ul> </li> <li>보상 함수(Reward Function) <ul> <li>QDP의 보상 함수는 양자 상태를 기준으로 정의되며, 특정 양자 상태 또는 결과를 얻는 것에 대한 가치(value)를 표현함</li> <li>보상 함수는 상태와 행동의 조합에 따라 달라질 수 있음</li> </ul> </li> </ul> </li> </ul> <h3 id="2-이-논문의-접근-방식">2. 이 논문의 접근 방식</h3> <ul> <li>기존 QDP 모델과는 달리, 이 논문은 측정된 물리적 속성으로 기술된 상태와 거시적 수준의 액션으로 구성된 <strong>클래식 MDP</strong>로 모델링함.</li> <li>Khatri의 아이디어 중 현재 기술 수준에서 활용 가능한 것들을 도입, 양자 컴퓨터 개발을 기다릴 필요 없이 구현 가능성을 제시함.</li> </ul> <h3 id="3-논문-모델링-대상">3. 논문 모델링 대상</h3> <ul> <li>논문의 대상은 Khatri 논문의 부록 D에서 제시된 <strong><code class="language-plaintext highlighter-rouge">얽힘 교환</code>(entanglement swapping)을 이용한 가상 링크 생성</strong></li> <li>가상 링크 생성은 다음 두 가지 맥락에서 연구됨: <ul> <li><strong>양자 중계기(Quantum Repeaters) 체인</strong>에서의 장거리 얽힘 생성</li> <li><strong>양자 얽힘 라우팅(Quantum Entanglement Routing)</strong>에서의 Mesh 네트워크 기반 얽힘 생성</li> </ul> </li> </ul> <h3 id="4-기존-연구와의-차이점">4. 기존 연구와의 차이점</h3> <ul> <li>기존 연구는 링크 생성의 <strong>히스토리(타임스탬프)</strong>를 무시</li> <li>즉, 가상 링크 생성 시 무한 메모리 컷오프 시간 정책(infinity memory cutoff-time policy)을 따름 <ul> <li>초기 링크가 성공적으로 얽힌 이후 다음 링크가 성공적으로 얽힐 때까지 초기 링크는 원래 상태를 지속적으로 유지할 확률을 100% 로 설정</li> <li>오래된 링크의 더 큰 <code class="language-plaintext highlighter-rouge">열화(decoherence)</code>가 <code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률(\(P_s\))에 미치는 영향을 고려하지 않음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">열화(decoherence)</code> <ul> <li>큐비트 상태가 시간이 지남에 따라 점진적으로 손실되는 과정</li> <li>원인: 양자 메모리가 환경과 상호작용하면서 완벽히 고립될 수 없기 때문</li> </ul> </li> </ul> <h3 id="5-이-논문의-개선점">5. 이 논문의 개선점</h3> <ul> <li>기존 접근법의 단점을 보완하여, 히스토리와 <code class="language-plaintext highlighter-rouge">열화(decoherence)</code> 영향을 포함한 MDP 모델링을 제안.</li> </ul> <hr/> <h2 id="reinforcement-learning-for-virtual-link-generation">Reinforcement Learning for Virtual Link Generation</h2> <h3 id="1-문제-정의-가상-링크-생성-문제">1. 문제 정의: <code class="language-plaintext highlighter-rouge">가상 링크 생성 문제</code></h3> <ul> <li><code class="language-plaintext highlighter-rouge">얽힘 교환</code>을 통해 두 개의 기본 링크로부터 가상 링크를 생성할 때 <strong><u>시간당 성공적인 `얽힘 교환` 횟수(가상 링크 생성률)를 최대화</u></strong>하는 문제</li> <li><code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률(<strong>\(P_s\)</strong>): <ul> <li>두 개의 기본 링크가 성공적으로 생성되어야 <code class="language-plaintext highlighter-rouge">얽힘 교환</code> 시도 가능</li> <li>처음 생성된 기본 링크가 오래될수록 성공 확률 감소</li> </ul> </li> <li><strong>컷오프 시간(</strong>\(t_c\)<strong>)</strong> <ul> <li><u>얽힘 상태를 유지하다가 폐기하는 시점을 결정하는 시간 임계값</u></li> <li>얽힘 품질 관리 <ul> <li>얽힘 상태는 시간이 지남에 따라 <code class="language-plaintext highlighter-rouge">열화(decoherence)</code> 현상으로 인해 점차 품질이 저하됨</li> <li>품질이 낮아지면 <code class="language-plaintext highlighter-rouge">얽힘 교환</code>의 성공 확률이 감소하므로, 특정 시간 이후 품질 저하된 얽힘 상태를 폐기하는 것이 유리함</li> </ul> </li> <li>리소스 최적화 <ul> <li>얽힘 상태를 오래 유지하면 성공 확률은 감소하지만, 새로 생성하지 않으므로 리소스를 절약할 수 있음</li> <li>반대로, 얽힘 상태를 폐기하고 새로 생성하면 높은 성공 확률을 얻을 수 있지만, 생성 과정에서 비용과 시간이 소모되고 <code class="language-plaintext highlighter-rouge">얽힘 교환</code> 시도를 지연시킴</li> </ul> </li> <li>얽힘 폐기와 재생성을 통해 <code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률(<strong>\(P_s\)</strong>)을 높일 수 있음</li> </ul> </li> <li>컷오프 시간 설정의 중요성</li> </ul> <table class="mbtablestyle table table-striped"> <tbody> <tr> <td><strong>짧은 컷오프 시간</strong></td> <td><strong>긴 컷오프 시간</strong></td> </tr> <tr> <td>얽힘 상태를 빠르게 폐기하고 새로 생성함</td> <td>얽힘 상태를 오래 유지하며 새로운 생성 빈도를 줄임</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률은 높아질 수 있지만, 새로운 링크 생성의 반복으로 인해 전체 과정이 지연될 가능성이 높아짐</td> <td>리소스를 절약하며 빠르게 교환 시도를 진행할 수 있지만, <code class="language-plaintext highlighter-rouge">얽힘 교환</code>의 성공 확률은 낮아질 수 있음</td> </tr> <tr> <td>리소스 소모 증가</td> <td><code class="language-plaintext highlighter-rouge">열화(decoherence)</code> 영향이 크다면 교환 실패 가능성이 높아짐</td> </tr> </tbody> </table> <ul> <li>컷오프 시간 결정의 어려움 <ul> <li>얽힘 생성 성공 확률(\(P_{e_{i,j}}\))와 <code class="language-plaintext highlighter-rouge">얽힘 교환</code> 성공 확률(\(P_s\))은 다양한 환경적 요인(예: 광섬유 길이, 양자 메모리 특성)에 따라 변화</li> <li>최적의 컷오프 시간을 설정하려면 다음을 고려해야 함 <ul> <li>얽힘 상태의 품질 변화 속도(<code class="language-plaintext highlighter-rouge">열화(decoherence)</code> 영향)</li> <li>새로운 얽힘 생성의 성공 확률 및 소요 시간</li> <li>전체 가상 링크 생성률을 극대화하는 시간-성능 균형</li> </ul> </li> </ul> </li> </ul> <h3 id="2-양자-얽힘-관리-문제에-대한-mdp-정의">2. 양자 얽힘 관리 문제에 대한 MDP 정의</h3> <ul> <li><strong>타임 스텝(time step) \(t\)</strong> : <ul> <li>제어 에이전트는 현재 상태 \(s_t\)를 관찰한 후 특정 행동 \(a_t\)를 적용</li> <li>이 행동의 실행은 상태 전이 확률 \(p(s_t, a_t, s_{t+1})\)에 의하여 새로운 상태 \(s_{t+1}\)로의 전환을 유발</li> <li>에이전트는 상태-행동 쌍 \((s_t, a_t)\)의 평가에 따라 보상 \(r_{t+1}\)을 받음</li> <li>에이전트는 새로운 상태 \(s_{t+1}\)를 관찰하고 이 과정을 반복</li> </ul> </li> <li><strong>MDP의 경로(trajectory):</strong> <ul> <li>초기 상태 \(s_0\)에서 시작하여 다음과 같은 경로를 생성: \(s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, \dots\)</li> <li>이 경로는 에이전트 정책(policy) \(\pi(s, a)\)에 따라 생성</li> </ul> </li> <li><strong>시스템 상태:</strong> <ul> <li>두 기본 링크(elementary links)와 가상 링크(virtual link)의 상태(행동)를 연결(concatenate)하여 구성</li> <li>각 링크의 상태는 벡터 \(s = [x, m]\)로 표현: <ul> <li>\(x\): 얽힘(entanglement) 상태 <ul> <li>\(x = 1\): 얽힘이 활성 상태</li> <li>\(x = 0\): 얽힘이 비활성 상태</li> </ul> </li> <li>\(m\): 얽힘의 나이(age) <ul> <li>얽힘이 비활성 상태(즉, \(x = 0\))인 경우 \(m = -1\)</li> </ul> </li> </ul> </li> </ul> </li> <li><strong>각 링크별 가능한 행동:</strong> <ul> <li>각 기본 또는 가상 링크에 대해 다음 두 가지 행동 중 하나 선택: <ol> <li><strong>재설정(reset):</strong> <ul> <li>링크 생성 재시도</li> <li>확률 \(P_{e_{i,j}}\) 및 \(P_s\)에 따라 확률적 전환 유발</li> <li>해당 링크 상태 \(s\)에 대해 <ul> <li>\(x\)가 1 또는 0으로 변경</li> <li>\(x=1\)이라면 \(m\)은 0으로 변경</li> </ul> </li> </ul> </li> <li><strong>대기(wait):</strong> <ul> <li>링크 생성이 재시도되지 않으며, 현재 링크 상태는 그대로 유지</li> <li>해당 링크 상태 \(s\)에 대해 <ul> <li>\(x\)는 변환없고</li> <li>\(x=1\)이라면 \(m\) 증가 유발</li> </ul> </li> </ul> </li> </ol> </li> <li>가상 링크에 대한 <strong>재설정(reset)</strong> <ul> <li>두 기본 링크에 대하여 <code class="language-plaintext highlighter-rouge">얽힘 교환</code>을 통해 하나의 가상 링크 생성</li> </ul> </li> <li>행동 공간 크기 \(2^3 = 8\)</li> </ul> </li> <li><strong>보상(reward):</strong> <ul> <li><code class="language-plaintext highlighter-rouge">얽힘 교환</code>이 성공하면 보상 값은 1</li> <li>그렇지 않으면 보상 값은 0</li> </ul> </li> </ul> <h3 id="3-양자-환경-모델에-대한-가정">3. 양자 환경 모델에 대한 가정</h3> <ul> <li>얽힘 생성 및 저장 방식 <ul> <li>얽힘은 <code class="language-plaintext highlighter-rouge">신호(herald)</code> 방식으로 생성됨 <ul> <li>얽힘 상태가 성공적으로 생성되었을 때, 해당 성공 여부를 외부에서 확인할 수 있도록 신호(herald)를 제공하는 얽힘 생성 기술을 지칭함</li> <li>양자 얽힘 생성 과정에서 성공 여부를 실시간으로 확인할 수 있음</li> </ul> </li> <li>얽힘 생성 시각과 컷오프 시간 \(t_c\)와의 연관 <ul> <li><code class="language-plaintext highlighter-rouge">신호(herald)</code> 방식에 의해 컷오프 시간 \(t_c\)를 정밀하게 측정하여 얽힘 품질 관리에 활용 <ul> <li>얽힘 생성 시점부터 시간이 경과함에 따라 얽힘 상태는 <code class="language-plaintext highlighter-rouge">열화(decoherence)</code>로 인해 품질이 저하되므로, 컷오프 시간 \(t_c\)이 네트워크 효율 관리에 중요함</li> </ul> </li> </ul> </li> <li>얽힘 상태 저장 <ul> <li>생성된 얽힘은 <strong>양자 메모리</strong>에 저장되며, 가상 링크 생성을 위해 얽힘 교환에 사용됨</li> </ul> </li> <li>얽힘 생성 방식으로 <strong>DLCZ 기반 프로토콜</strong> 사용 <ul> <li>광자 방출과 검출을 통해 얽힘 생성 성공 여부를 확인</li> <li>성공 여부를 기반으로 얽힘 상태를 메모리에 저장하고 컷오프 시간 \(t_c\) 관리</li> </ul> </li> </ul> </li> <li>시간 슬롯 모델 <ul> <li>전체 시간을 슬롯 단위로 나눔 <ul> <li>슬롯 길이: \(\frac{L_0}{v}\)</li> <li>\(L_0\): 기본 링크(광섬유)의 길이</li> <li>\(v\): 광섬유에서 빛의 전파 속도</li> </ul> </li> <li>슬롯 길이는 새로운 상태를 관찰하고 링크 생성을 재시도하는데 요구되는 시간보다는 길어야 함</li> </ul> </li> <li>기본 링크 얽힘 성공 확률 \(P_{e_{i,j}}\) <ul> <li>광섬유 거리 \(L_0\)에 따라 지수적으로 감소 <ul> <li><a class="citation" href="#sangouard2009">(Sangouard et al., 2009)</a>, (missing reference) 또는 <a class="citation" href="#Uphoff_2016">(Uphoff et al., 2016)</a>에 제시된 결과에 기반함</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Entanglement Generation Probability Model</p> </blockquote> <blockquote> <p>Once a heralded local entanglement is generated at each node, the two photons must be sent to the BSM and must be measured The entanglement generation probability for an elementary link \(e_{i,j}\) is equal to:</p> </blockquote> \[P_{e_{i,j}} = \frac{1}{2} \nu^o \left( p e^{-\frac{d_{i,j}}{2L_0}} \right)^2 = \frac{1}{2} \nu^o p^2 e^{-\frac{d_{i,j}}{L_0}}\] <blockquote> <p>where \(\nu^o\) denotes the optical BSM efficiency (assumed constant at each node, \(\nu^o=0.39\)), \(d_{i,j}\) denotes the length of elementary link \(e_{i,j}\), \(p\) indicates the success probability of detecting a pair of photons during the entanglement generation process (\(p≈0.05\sim0.1\)), \(L_0\)￼denotes the attenuation length of the optical fiber (\(L_0 = 22 \, \mathrm{km}\)), and the term \(\frac{1}{2}\) accounts for the optical BSM capability of unambiguously identifying only two out of four bell states</p> </blockquote> <ul> <li>양자 메모리 효율 \(\eta\) <ul> <li>저장 시간에 따라 감소 <ul> <li><a class="citation" href="#Ortu2022a">(Ortu et al., 2022)</a>에서 설명된 Mims 모델에 따름</li> </ul> </li> <li>이는 얽힘 교환 성공 확률 \(P_s\)에 주요 영향을 미침</li> </ul> </li> </ul> <blockquote> <p>Quantum Memory Efficiency Model</p> </blockquote> <blockquote> <p>The efficiency of a quantum memory, denoted as \(\eta(t)\) (the probability that the qubit remains in its original state at time \(t\)), can be expressed as:</p> </blockquote> \[\eta(t) = \eta(0) \cdot e^{-\frac{t}{T_o}}\] <blockquote> <p>where \(\eta(0)\) represents the probability that the qubit remains in its original state at time \(t=0\). Typically, \(\eta(0)=1\) for ideal systems but may be less than 1 in practical cases due to initialization imperfections. \(t\) is the time for which the qubit is stored in the quantum memory. \(T_o\) indicates the characteristic memory lifetime or decoherence time, representing the time scale over which the memory retains its original state.</p> </blockquote> <ul> <li>\(T_o\) (양자 메모리 수명)의 일반적인 값 <ul> <li>물리적 시스템별 <ul> <li>원자 집합(Atomic Ensembles): 수백 밀리초 ~ 몇 초</li> <li>이온 트랩(Ion Traps): 진공 상태에서 1초 ~ 100초</li> <li>고체 상태 양자 메모리(Solid-State Quantum Memory): <ul> <li>희토류 이온: 1~10밀리초</li> <li>NV 센터: 최대 수백 밀리초</li> </ul> </li> <li>초전도 큐비트(Superconducting Qubits): 10~500마이크로초</li> </ul> </li> <li>\(T_o\)에 영향을 미치는 요인 <ul> <li>환경 잡음: \(T_o\) 감소</li> <li>온도: 극저온 조건에서 \(T_o\) 증가</li> <li>오류 보정: 다이나믹 디커플링(Dynamic Decoupling) 등의 기술로 \(T_o\) 연장 가능</li> </ul> </li> <li>양자 네트워크 설계 목표 <ul> <li>실용적인 양자 네트워크에서는 최소 \(T_o\)이 1~10초 이상 필요</li> </ul> </li> </ul> </li> <li>교환 성공 확률 \(P_s\) <ul> <li>얽힘 교환 과정에서 가장 오래된 메모리의 효율 \(\eta(t)\) 에 따라 결정</li> </ul> </li> </ul> \[P_s = \eta(t_{oldest}) = \eta(0) \cdot e^{-\frac{t_{oldest}}{T_o}}\] <h3 id="4-강화학습-기반-문제-해결-접근-방법">4. 강화학습 기반 문제 해결 접근 방법</h3> <ul> <li>확률 모델의 부재 <ul> <li>기본 링크 생성 확률 \(P_{e_{i,j}}\)와 메모리 효율 \(\eta\) (따라서, 얽힘 교환 성공 확률 \(P_s\))에 대한 정확한 모델을 알 수 없음</li> <li>따라서, 당연히 상태 전이 확률 \(p(s_t, a_t, s_{t+1})\)도 알 수 없음</li> </ul> </li> <li>깊은 강화학습(DRL)의 적용 <ul> <li>가상 링크 생성률을 극대화하는 정책을 찾기 위해 DRL 적용</li> <li>이 과정에서 \(\pi\)를 따르는 \(Q\)-value 함수인 \(Q^\pi(s, a)\)를 정의하여 활용</li> </ul> </li> <li>\(Q\)-value 함수 정의 <ul> <li>\(Q^\pi(s, a)\): 주어진 상태-행동 쌍 \((s, a)\)에 대해 정책 \(\pi\)를 따랐을 때 할인된 누적 보상 기대값 (expected discounted return)</li> <li>할인된 누적 보상은 궤적(trajectory)에서 미래 보상의 합으로 정의됨: \(\sum_{k=0}^\infty \gamma^k r_{t+k+1}, \quad \gamma \in [0, 1]\)</li> <li>에이전트는 \(Q\)-value 함수를 극대화하는 최적 정책 \(\pi^*\)를 탐색</li> </ul> </li> <li>정책의 구성 <ul> <li>본 문제에서 정책은 가장 적절한 컷오프 시간 \(t_c\)를 결정하는 것</li> </ul> </li> <li>DQN 알고리즘을 활용한 학습 <ul> <li>본 연구에서는 <strong>Deep Q-Network(DQN)</strong> 알고리즘을 기반으로 <code class="language-plaintext highlighter-rouge">가상 링크 생성 문제</code>에 적합하도록 학습 루틴을 구성</li> <li>학습 루틴은 기존의 DQN 방식을 따르면서도 본 연구 문제에 맞게 조정됨</li> </ul> </li> </ul> <h2 id="experimental-results">Experimental Results</h2> <h3 id="1-실험-세팅">1. 실험 세팅</h3> <ul> <li>기본 설정 <ul> <li>광섬유 길이 \(L_0 = 100 \, \mathrm{km}\), 빛의 속도(light speed) \(200,000 \, \mathrm{km/s}\) <ul> <li>타임 슬롯 길이 (duration of a time step): \(0.5 \, \mathrm{ms}\)</li> </ul> </li> <li>광섬유 손실은 \(0.2 \, \mathrm{dB/km}\) <ul> <li>광섬유를 통해 빛이 \(1 \, \mathrm{km}\)를 갈 때, 빛의 세기가 조금 줄어드는 정도를 $0.2$로 가정</li> <li>이 줄어드는 정도는 빛 파장이 \(1,550 \, \mathrm{nm}\) 때 가능</li> </ul> </li> <li>Mims 방정식을 사용하여 손실 모델링 수행</li> <li>zero-time efficiency 가정 <ul> <li>얽힘 생성, 전송, 또는 교환 과정에서 추가적인 시간 지연(예: 신호 전파 시간, 연산 시간 등)이 없다고 가정</li> <li>양자 메모리에서 Swap을 수행할 때, 시간이 소모되지 않고 즉각적으로 이루어진다고 간주</li> </ul> </li> </ul> </li> <li>세 계의 계층으로 이루어진 \(Q\)-value 함수 구현 <ul> <li>두 개의 Dense Layer에는 각각 32개의 뉴런 존재</li> <li>각 층은 \(\tanh\) 활성화 함수를 사용</li> <li>출력층에는 활성화 함수가 없으며, 액션 수와 동일한 뉴런 수를 가짐 <ul> <li>액션 수는 8 (두 개의 기본 링크와 하나의 가상 링크를 고려)</li> </ul> </li> </ul> </li> <li>학습 알고리즘 <ul> <li>OpenAI Baselines 라이브러리에서 제공하는 DQN 알고리즘 사용</li> <li>DRL 알고리즘으로 신경망 학습</li> </ul> </li> <li>보상 평가 <ul> <li>에피소드 보상은 학습 에피소드 동안 발생한 모든 단계의 보상 \(r\)의 합으로 계산</li> <li>에피소드는 10,000 스텝으로 구성</li> <li>평균 에피소드 보상이 학습 시간에 따라 증가하며, 600 에피소드 이후 안정화됨</li> </ul> </li> </ul> <h3 id="2-실험-결과">2. 실험 결과</h3> <ul> <li>Benchmark 1: <strong>Inf-cutoff-time policy</strong> <ul> <li>컷오프 시간 무한대 설정</li> <li>즉, 첫 번째 기본 링크가 성공적으로 생성되면, 두 번째 기본 링크가 성공할 때까지 첫 번째 링크를 계속 유지 <ul> <li><code class="language-plaintext highlighter-rouge">열화(decoherence)</code>로 인해 첫 번째 링크의 상태가 저하되더라도 폐기하지 않고 그대로 사용</li> </ul> </li> <li>이 방법은 최적 정책 성능의 하한선으로 활용됨</li> <li>장점 <ul> <li>첫 번째 링크를 폐기하지 않고 유지하므로, 재생성에 따른 추가 비용과 시간을 절약</li> </ul> </li> <li>단점 <ul> <li>첫 번째 링크가 오래 유지되면서 품질이 저하될 가능성이 큼 <ul> <li>저하된 품질로 인해 얽힘 교환의 성공 확률이 크게 낮아질 수 있음</li> </ul> </li> </ul> </li> </ul> </li> <li>Benchmark 2: <strong>Opt-cutoff-time policy</strong> <ul> <li>문제의 모든 가능한 컷오프 시간을 시도하여 가장 좋은 결과를 내는 값을 선택</li> <li>즉, 가장 오래된 기본 링크의 컷오프 시간을 여러 값으로 설정하여 모두 시뮬레이션 <ul> <li>각 시뮬레이션 결과를 비교하여 가장 높은 성능(예: 최대 얽힘 교환 성공률)을 내는 컷오프 시간을 선택</li> </ul> </li> <li>이 방법은 최적 정책 성능의 상한선으로 활용됨</li> </ul> </li> <li>실험 결과 설명</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-12-22-quantum_vl/fig2-480.webp 480w,/assets/img/posts/2024-12-22-quantum_vl/fig2-800.webp 800w,/assets/img/posts/2024-12-22-quantum_vl/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-12-22-quantum_vl/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>정책 테스트 방법 <ul> <li>MDP 프로세스를 1,000번 시뮬레이션하여 정책을 테스트</li> <li>하나의 에피소드는 10,000 스텝으로 구성됨.</li> </ul> </li> <li>성능 비교 결과 <ul> <li>DRL 에이전트는 Inf-cutoff-time 정책보다 명확히 더 좋은 성능을 보임</li> <li>DRL 에이전트의 성능은 opt-cutoff-time 정책에 매우 근접함.</li> </ul> </li> <li>컷오프 시간 결과 <ul> <li>DRL 정책에서 발견된 최적 컷오프 시간: 146.0 스텝</li> <li>opt-cutoff-time 정책에서 발견된 최적 컷오프 시간: 108 스텝</li> </ul> </li> <li>의미 <ul> <li>DRL 정책은 최적 정책(opt-cutoff-time)에 근접하면서도 이 정책에서 사용하고 있는 Brute-force 방법보다 효율적으로 작동함</li> <li>Inf-cutoff-time 정책보다 더 나은 컷오프 시간을 찾음으로써 성능 향상을 입증</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Quantum"/><category term="Networks"/><category term="Quantum,"/><category term="Reinforcement"/><category term="Learning"/><summary type="html"><![CDATA[Quantum Virtual Link Generation via Reinforcement Learning]]></summary></entry><entry><title type="html">UDC</title><link href="https://optreal.github.io/blog/2024/tabs-ttt/" rel="alternate" type="text/html" title="UDC"/><published>2024-11-26T00:32:13+00:00</published><updated>2024-11-26T00:32:13+00:00</updated><id>https://optreal.github.io/blog/2024/tabs-ttt</id><content type="html" xml:base="https://optreal.github.io/blog/2024/tabs-ttt/"><![CDATA[<h3 id="abstract">Abstract</h3> <p><strong>Single-Stage Neural Combinatorial Optimization Solvers</strong></p> <ul> <li>Exhibit significant performance degradation when applied to large-scale combinatorial optimization (CO) problems.</li> </ul> <p><strong>Two-Stage Neural Methods</strong></p> <ul> <li>Inspired by Divide-and-Conquer strategies.</li> <li>Efficient in addressing large-scale CO problems but rely heavily on problem-specific heuristics in either the dividing or conquering phase, limiting general applicability.</li> <li>Typically, employ separate training schemes, overlooking interdependencies between the two phases, often leading to convergence to suboptimal solutions.</li> </ul> <p>Unified Neural Divide-and-Conquer Framework (UDC)</p> <ul> <li>Introduces the Divide-Conquer-Reunion (DCR) training method to address issues arising from suboptimal dividing policies.</li> <li>Utilizes a lightweight Graph Neural Network (GNN) to decompose large-scale CO instances.</li> <li>Employs a constructive solver to conquer the divided sub-problems effectively. Demonstrates extensive applicability to diverse CO problems.</li> <li>Achieves superior performance across 10 representative large-scale CO problems.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_1-480.webp 480w,/assets/img/table_1-800.webp 800w,/assets/img/table_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h3 id="introduction">Introduction</h3> <p><strong>Combinatorial Optimization (CO) Applications</strong></p> <ul> <li>Route planning</li> <li>Circuit design</li> <li>Biology</li> </ul> <p><strong>Reinforcement Learning (RL)-based Constructive Neural Combinatorial Optimization (NCO) Methods</strong></p> <ul> <li>Generate near-optimal solutions for small-scale instances (e.g., TSP instances with up to 200 nodes) without requiring expert knowledge.</li> <li>Construct solutions in an end-to-end manner, node-by-node.</li> <li>Limited capability when applied to large-scale instances.</li> </ul> <p><strong>Categories of NCO Methods</strong></p> <ol> <li><strong>Modified Single-Stage Solvers</strong> <ul> <li>Methods like BQ-NCO and LEHD develop sub-path construction using heavy decoders.</li> <li>Require supervised learning (SL), limiting applicability when high-quality labeled solutions are unavailable.</li> </ul> </li> <li><strong>Auxiliary Information for RL-Based Solvers</strong> <ul> <li>Methods like ELG, ICAM, and DAR use auxiliary information to guide solvers.</li> <li>Problem-specific auxiliary designs limit general applicability.</li> <li>Complexity issues arise, particularly with self-attention mechanisms (e.g., \(O(N^2)\) complexity).</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Inspired by traditional heuristic divide-and-conquer methods.</li> <li>Use a two-stage approach: dividing the instance and conquering sub-problems.</li> <li>Methods like TAM, H-TSP, and GLOP show improved efficiency in large-scale TSP and CVRP problems.</li> </ul> </li> </ol> <p><strong>Challenges in Large-Scale NCO</strong></p> <ul> <li>Heavy models requiring SL are limited by the availability of labeled solutions.</li> <li>Self-attention complexity \(O(N^2)\) hinders scalability.</li> <li>Problem-specific auxiliary information limits general applicability.</li> </ul> <h3 id="shortcomings-of-neural-divide-and-conquer-approaches">Shortcomings of Neural Divide-and-Conquer Approaches</h3> <p><strong>Limitations in Applicability and Solution Quality</strong></p> <ul> <li>Rely on problem-specific heuristics in either the dividing (e.g., GLOP, SO) or conquering (e.g., L2D, RBG) stages, which limits generalizability.</li> </ul> <p><strong>Issues with Separate Training Process</strong></p> <ul> <li>Dividing and conquering policies are trained separately, which fails to consider their interdependencies, often resulting in convergence to local optima.</li> </ul> <p><strong>Importance of Mitigating Sub-Optimal Dividing Impact</strong></p> <ul> <li>Addressing suboptimal dividing is crucial for achieving high-quality solutions.</li> </ul> <h3 id="proposed-approach">Proposed Approach</h3> <p><strong>Divide-Conquer-Reunion (DCR)</strong></p> <ul> <li>A novel RL-based training method designed to consider interdependencies between dividing and conquering stages.</li> </ul> <p><strong>Unified Neural Divide-and-Conquer Framework (UDC)</strong></p> <ul> <li>Incorporates DCR in a unified training scheme.</li> <li>Uses a lightweight GNN to efficiently decompose large-scale instances into manageable sub-problems.</li> <li>Constructive solvers then effectively solve these sub-problems.</li> </ul> <p><strong>Contributions</strong></p> <ul> <li>Propose DCR to mitigate the impact of suboptimal dividing policies.</li> <li>Achieve a unified training scheme in UDC, leading to improved performance.</li> <li>Demonstrate UDC’s applicability across various CO problems.</li> </ul> <hr/> <h3 id="preliminaries-neural-divide-and-conquer">Preliminaries: Neural Divide-and-Conquer</h3> <p><strong>CO Problem Definition</strong></p> <ul> <li>Involves $N$ decision variables.</li> <li>Objective: Minimize function \(f(x, \mathcal{G})\), where $G$ is the CO instance, and \(\Omega\) is the set of feasible solutions.</li> </ul> \[\text{minimize}_ f(x, \mathcal{G})\] <p><strong>Divide-and-Conquer in CO</strong></p> <ul> <li><strong>Traditional Methods</strong> <ul> <li>Use heuristic algorithms like large-neighborhood-search to divide and conquer.</li> <li>Dividing stage selects sub-problems, and conquering stage repairs sub-problems.</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Dividing policy \(\pi_d(\mathcal{G})\) decomposes instance $G$ into sub-problems.</li> <li>Conquering policy \(\pi_c\) solves each sub-problem, and the total solution is obtained by concatenating sub-solutions.</li> </ul> </li> </ul> <h3 id="constructive-neural-solver">Constructive Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Efficient for small-scale CO problems.</li> <li>Uses an attention-based encoder-decoder network to construct solutions.</li> </ul> <p><strong>Training Process</strong></p> <ul> <li>Modeled as a Markov Decision Process (MDP).</li> <li>Trained using Deep Reinforcement Learning (DRL) without expert experience.</li> </ul> <p><strong>Solution Generation</strong></p> <ul> <li>Constructs solutions step-by-step using a trained policy \(\pi\).</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = \prod_{t=1}^{\tau} p_{\theta}(x_t \mid x_{1:t-1}, \mathcal{G}, \Omega)\] <h3 id="heatmap-based-neural-solver">Heatmap-Based Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Uses lightweight GNNs for problem-solving, especially for large-scale CO problems like VRPs.</li> </ul> <p><strong>Limitations</strong></p> <ul> <li><strong>Non-Autoregressive Generation</strong>: Lacks partial solution order information, which can lead to poor solution quality.</li> <li><strong>Search Algorithm Dependence</strong>: Relies on search algorithms for high-quality solutions.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = p_{\theta}(\mathcal{H} \mid \mathcal{G}, \Omega) p(x_1) \prod_{t=2}^{\tau} \frac{\exp(\mathcal{H}_{x_{t-1}, x_t})}{\sum_{i=t}^{N} \exp(\mathcal{H}_{x_{t-1}, x_i})},\] <hr/> <h3 id="methodology-unified-divide-and-conquer-udc">Methodology: Unified Divide-and-Conquer (UDC)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_1-480.webp 480w,/assets/img/figure_1-800.webp 800w,/assets/img/figure_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="general-framework">General Framework</h4> <ul> <li><strong>Two Stages</strong>: Dividing and Conquering.</li> <li><strong>Dividing Stage</strong>: Generates initial solutions using an Anisotropic GNN (AGNN).</li> <li><strong>Conquering Stage</strong>: Decomposes the original instance into sub-problems and solves them using constructive neural solvers.</li> </ul> <p><strong>Solver Integration</strong></p> <ul> <li>Different solvers are used based on the type of CO problem.</li> <li><strong>AGNN</strong> for Maximum Independent Set (MIS).</li> <li><strong>POMO</strong> for Vehicle Routing Problem (VRP).</li> <li><strong>ICAM</strong> for 0-1 Knapsack Problem (KP).</li> </ul> <p><strong>Dividing Stage</strong></p> \[\pi_d(x_0|\mathcal{G}_D, \Omega, \phi) = \begin{cases} p(\mathcal{H}|\mathcal{G}_D, \Omega, \phi) p(x_{0,1}) \prod_{t=2}^\tau \frac{\exp(\mathcal{H}_{x_0, t-1, x_0, t})}{\sum_{i=t}^N \exp(\mathcal{H}_{x_0, t-1, x_0, i})}, &amp; \text{if } x_0 \in \Omega \\ 0, &amp; \text{otherwise} \end{cases}\] <ul> <li>original CO instance \(\mathcal{G}\)</li> <li>sparse graph \(\mathcal{G}_D = \{ \mathbb{V}, \mathbb{E} \}\)</li> <li>parameter $$\phi$ of Anisotropic Graph Neural Networks (AGNN)</li> <li>heatmap \(\mathcal{H}\) (e.g For \(N\)-node VRPs, the heatmap \(\mathcal{H} \in \mathbb{R}^{N×N}\) )</li> <li>initial solution \(x_0 = (x_{0,1},...,x_{0,\tau})\), \(\tau\) is length</li> </ul> <p><strong>Conquering Stage: Sub-problem Preparation</strong></p> <ul> <li>sub-problems \(\{ \mathcal{G}_1,..., \mathcal{G}_{\lfloor \frac{N}{n} \rfloor} \}\)</li> <li>\(\{ \Omega_1,..., \Omega_{\lfloor \frac{N}{n} \rfloor} \}\) constraints of sub-problems (e.g no self-loop in sub-TSPs)</li> </ul> <p><strong>Conquering Stage: Constructive Neural Conquering</strong></p> <p>\(\pi_c(s_k|\mathcal{G}_k, \Omega_k, \theta) = \begin{cases} \prod_{t=1}^n p(s_{k,t} | s_{k,1:t-1}, \mathcal{G}_k, \Omega_k, \theta), &amp; \text{if } s_k \in \Omega_k \\ 0, &amp; \text{otherwise} \end{cases}\)</p> <ul> <li>utilize constructive solvers with parameter $$\theta$ for most involved sub-CO problems.</li> <li>sub-solution \(s_{k} = (s_{k,1},...,s_{k,n})\), $k \in { 1,…, \lfloor \frac{N}{n} \rfloor}$$</li> <li>conquering policy \(\pi_c\)</li> <li>Replacement of original solution fragments in the final conquering stage: sub-solutions with improvements on the objective function replace the original solution fragment in \(x_0\)</li> <li>Formation of merged solution: the merged solution becomes \(x_1\)</li> <li>Repeated execution of conquering stage: conquering stage can be executed repeatedly on the new merged solution</li> <li>Gradual improvement in solution quality: the solution after \(r\) conquering stages is noted as \(x_r\)</li> </ul> <h3 id="training-method-divide-conquer-reunion-dcr">Training Method: Divide-Conquer-Reunion (DCR)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_2-480.webp 480w,/assets/img/figure_2-800.webp 800w,/assets/img/figure_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dividing and conquering stages modeled as MDPs.</li> <li>Separate training for conquering and dividing policies.</li> <li>Need for problem-specific datasets.</li> <li>Lack of collaboration in optimizing policies.</li> <li>Impact of sub-optimal sub-problem decomposition.</li> <li>Divide-Conquer-Reunion (DCR) process introduction.</li> <li>Additional Reunion step for better integration of sub-problems.</li> <li>Improved stability and convergence in training.</li> <li>Use of REINFORCE algorithm for unified training.</li> <li>Baseline calculation for both dividing and conquering policies.</li> </ul> <p>\(\nabla \mathcal{L}d(\mathcal{G}) = \frac{1}{\alpha} \sum_{i=1}^\alpha\) \(\left( f(x_2^i, \mathcal{G}) - \frac{1}{\alpha} \sum_{j=1}^\alpha f(x_2^j, \mathcal{G}) \right) \nabla \log \pi_d(x_2^i|\mathcal{G}_D, \Omega, \phi)\) \(\nabla \mathcal{L}{c1}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{1,i}, \mathcal{G}_{c}^0) - \frac{1}{\beta} \sum_{j=1}^\beta f{\prime}(s_{c}^{1,j}, \mathcal{G}_{c}^0) \right) \nabla \log \pi_c(s_{c}^{1,j}|\mathcal{G}_{c}^0, \Omega_{c}, \theta) \right)\) \(\nabla \mathcal{L}{c2}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{2,i}, \mathcal{G}_{c}^1) - \frac{1}{\beta} \sum_{j=1}^\beta f{\prime}(s_{c}^{2,j}, \mathcal{G}_{c}^1) \right) \nabla \log \pi_c(s_{c}^{2,j}|\mathcal{G}_{c}^1, \Omega_{c}, \theta) \right)\)</p> <ul> <li>\(\{ x_2^1, ..., x_{2}^{\alpha} \}\) represents the \(\alpha\) sampled solutions.</li> <li>there are \(\alpha \lfloor \frac{N}{n} \rfloor$ sub-problems\)\mathcal{G}^{0}<em>{c},c \in { 1, …, \lfloor \frac{N}{n} \rfloor, …, \alpha \lfloor \frac{N}{n} \rfloor}\(generated based on\){ x_0^1, …, x</em>{0}^{\alpha} }$$ in the first conquering stage</li> <li>\(\alpha \lfloor \frac{N}{n} \rfloor\) can be regarded as the batch size of sub-problems</li> <li>The \(\beta\) sampled sub-solutions for sub-problem \(\mathcal{G}_{c}^{0}, \mathcal{G}_{c}^{1},c \in \{1,..., \alpha \lfloor \frac{N}{n} \rfloor\}\) are noted as \(\{s_{c}^{1,i},...,s_{c}^{1,\beta}\},\{s_{c}^{2,i},...,s_{c}^{2,i}\}\).</li> </ul> <p><strong>Challenges and Proposed Solution</strong></p> <ul> <li>Existing methods fail to train dividing and conquering policies simultaneously, leading to unsolvable antagonisms.</li> <li><strong>Unified Training Requirement</strong>: DCR enables collaborative optimization of dividing and conquering policies by treating connections between sub-problems as new problems to reconquer.</li> </ul> <p><strong>Training Process with REINFORCE</strong></p> <ul> <li>Uses the REINFORCE algorithm to train both dividing and conquering policies, ensuring better reward estimation and improved convergence.</li> </ul> <h3 id="application-general-co-problems">Application: General CO Problems</h3> <p><strong>Conditions for Applicability</strong></p> <ol> <li><strong>Decomposable Objective Functions</strong>: The objective function must contain decomposable aggregate functions (i.e., no functions like Rank or Top-k).</li> <li><strong>Feasibility of Initial and Sub-Solutions</strong>: Ensured using feasibility masks.</li> <li><strong>Non-Uniqueness of Sub-Problem Solutions</strong>: Solutions for sub-problems should not be unique to ensure flexibility in merging sub-solutions.</li> </ol> <p><strong>Limitations</strong></p> <ul> <li>Complex CO problems may face issues where solutions cannot be guaranteed as legal through the process, limiting applicability.</li> <li>Problems such as TSPTW may have constraints that make ensuring legal initial and sub-solutions difficult.</li> </ul> <hr/> <h3 id="experiment">Experiment</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_2-480.webp 480w,/assets/img/table_2-800.webp 800w,/assets/img/table_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_3-480.webp 480w,/assets/img/table_3-800.webp 800w,/assets/img/table_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table_4-480.webp 480w,/assets/img/table_4-800.webp 800w,/assets/img/table_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/table_4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/figure_3-480.webp 480w,/assets/img/figure_3-800.webp 800w,/assets/img/figure_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/figure_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Overview</strong></p> <ul> <li>To verify the applicability and efficiency of UDC, experiments were conducted across 10 different CO problems, including TSP, CVRP, KP, MIS, and more.</li> <li>UDC was compared to both classical and neural solvers.</li> </ul> <p><strong>Performance Evaluation</strong></p> <ul> <li>UDC demonstrated superior performance in terms of solution quality and computational efficiency across large-scale CO instances, ranging from 500-node to 2,000-node problems.</li> </ul> <p><strong>Comparison to Baselines</strong></p> <ul> <li>Classical solvers like LKH and other neural methods (e.g., ELG, GLOP) were used as baselines.</li> <li>UDC consistently outperformed other methods, particularly in large-scale settings where scalability is critical.</li> </ul> <hr/> <h3 id="conclusion">Conclusion</h3> <p><strong>Summary</strong></p> <ul> <li>UDC, with its novel DCR training mechanism, successfully addresses the limitations of existing neural divide-and-conquer methods for large-scale CO problems.</li> <li>The unified training scheme ensures that both dividing and conquering stages work in synergy, thereby achieving better overall optimization.</li> </ul> <p><strong>Future Work</strong></p> <ul> <li>Further improvements can be made by designing better loss functions for training.</li> <li>Extending UDC’s applicability to other complex CO problems not covered in the current study is another promising direction for future research.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</title><link href="https://optreal.github.io/blog/2024/udc/" rel="alternate" type="text/html" title="A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems"/><published>2024-11-26T00:00:00+00:00</published><updated>2024-11-26T00:00:00+00:00</updated><id>https://optreal.github.io/blog/2024/udc</id><content type="html" xml:base="https://optreal.github.io/blog/2024/udc/"><![CDATA[<h3 id="abstract">Abstract</h3> <p><strong>Single-Stage Neural Combinatorial Optimization Solvers</strong></p> <ul> <li>Exhibit significant performance degradation when applied to large-scale combinatorial optimization (CO) problems.</li> </ul> <p><strong>Two-Stage Neural Methods</strong></p> <ul> <li>Inspired by Divide-and-Conquer strategies.</li> <li>Efficient in addressing large-scale CO problems but rely heavily on problem-specific heuristics in either the dividing or conquering phase, limiting general applicability.</li> <li>Typically, employ separate training schemes, overlooking interdependencies between the two phases, often leading to convergence to suboptimal solutions.</li> </ul> <p>Unified Neural Divide-and-Conquer Framework (UDC)</p> <ul> <li>Introduces the Divide-Conquer-Reunion (DCR) training method to address issues arising from suboptimal dividing policies.</li> <li>Utilizes a lightweight Graph Neural Network (GNN) to decompose large-scale CO instances.</li> <li>Employs a constructive solver to conquer the divided sub-problems effectively. Demonstrates extensive applicability to diverse CO problems.</li> <li>Achieves superior performance across 10 representative large-scale CO problems.</li> </ul> <p><img src="assets/udc_img/table_1.png" alt="poster"/></p> <hr/> <h3 id="introduction">Introduction</h3> <p><strong>Combinatorial Optimization (CO) Applications</strong></p> <ul> <li>Route planning</li> <li>Circuit design</li> <li>Biology</li> </ul> <p><strong>Reinforcement Learning (RL)-based Constructive Neural Combinatorial Optimization (NCO) Methods</strong></p> <ul> <li>Generate near-optimal solutions for small-scale instances (e.g., TSP instances with up to 200 nodes) without requiring expert knowledge.</li> <li>Construct solutions in an end-to-end manner, node-by-node.</li> <li>Limited capability when applied to large-scale instances.</li> </ul> <p><strong>Categories of NCO Methods</strong></p> <ol> <li><strong>Modified Single-Stage Solvers</strong> <ul> <li>Methods like BQ-NCO and LEHD develop sub-path construction using heavy decoders.</li> <li>Require supervised learning (SL), limiting applicability when high-quality labeled solutions are unavailable.</li> </ul> </li> <li><strong>Auxiliary Information for RL-Based Solvers</strong> <ul> <li>Methods like ELG, ICAM, and DAR use auxiliary information to guide solvers.</li> <li>Problem-specific auxiliary designs limit general applicability.</li> <li>Complexity issues arise, particularly with self-attention mechanisms (e.g., $O(N^2)$ complexity).</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Inspired by traditional heuristic divide-and-conquer methods.</li> <li>Use a two-stage approach: dividing the instance and conquering sub-problems.</li> <li>Methods like TAM, H-TSP, and GLOP show improved efficiency in large-scale TSP and CVRP problems.</li> </ul> </li> </ol> <p><strong>Challenges in Large-Scale NCO</strong></p> <ul> <li>Heavy models requiring SL are limited by the availability of labeled solutions.</li> <li>Self-attention complexity $O(N^2)$ hinders scalability.</li> <li>Problem-specific auxiliary information limits general applicability.</li> </ul> <h3 id="shortcomings-of-neural-divide-and-conquer-approaches">Shortcomings of Neural Divide-and-Conquer Approaches</h3> <p><strong>Limitations in Applicability and Solution Quality</strong></p> <ul> <li>Rely on problem-specific heuristics in either the dividing (e.g., GLOP, SO) or conquering (e.g., L2D, RBG) stages, which limits generalizability.</li> </ul> <p><strong>Issues with Separate Training Process</strong></p> <ul> <li>Dividing and conquering policies are trained separately, which fails to consider their interdependencies, often resulting in convergence to local optima.</li> </ul> <p><strong>Importance of Mitigating Sub-Optimal Dividing Impact</strong></p> <ul> <li>Addressing suboptimal dividing is crucial for achieving high-quality solutions.</li> </ul> <h3 id="proposed-approach">Proposed Approach</h3> <p><strong>Divide-Conquer-Reunion (DCR)</strong></p> <ul> <li>A novel RL-based training method designed to consider interdependencies between dividing and conquering stages.</li> </ul> <p><strong>Unified Neural Divide-and-Conquer Framework (UDC)</strong></p> <ul> <li>Incorporates DCR in a unified training scheme.</li> <li>Uses a lightweight GNN to efficiently decompose large-scale instances into manageable sub-problems.</li> <li>Constructive solvers then effectively solve these sub-problems.</li> </ul> <p><strong>Contributions</strong></p> <ul> <li>Propose DCR to mitigate the impact of suboptimal dividing policies.</li> <li>Achieve a unified training scheme in UDC, leading to improved performance.</li> <li>Demonstrate UDC’s applicability across various CO problems.</li> </ul> <hr/> <h3 id="preliminaries-neural-divide-and-conquer">Preliminaries: Neural Divide-and-Conquer</h3> <p><strong>CO Problem Definition</strong></p> <ul> <li>Involves $N$ decision variables.</li> <li>Objective: Minimize function $f(x, G)$, where $G$ is the CO instance, and $\Omega$ is the set of feasible solutions. \(\text{minimize}_ f(x, \mathcal{G})\)</li> </ul> <p><strong>Divide-and-Conquer in CO</strong></p> <ul> <li><strong>Traditional Methods</strong> <ul> <li>Use heuristic algorithms like large-neighborhood-search to divide and conquer.</li> <li>Dividing stage selects sub-problems, and conquering stage repairs sub-problems.</li> </ul> </li> <li><strong>Neural Divide-and-Conquer Methods</strong> <ul> <li>Dividing policy $\pi_d(G)$ decomposes instance $G$ into sub-problems.</li> <li>Conquering policy $\pi_c$ solves each sub-problem, and the total solution is obtained by concatenating sub-solutions.</li> </ul> </li> </ul> <h3 id="constructive-neural-solver">Constructive Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Efficient for small-scale CO problems.</li> <li>Uses an attention-based encoder-decoder network to construct solutions.</li> </ul> <p><strong>Training Process</strong></p> <ul> <li>Modeled as a Markov Decision Process (MDP).</li> <li>Trained using Deep Reinforcement Learning (DRL) without expert experience.</li> </ul> <p><strong>Solution Generation</strong></p> <ul> <li>Constructs solutions step-by-step using a trained policy $\pi$.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = \prod_{t=1}^{\tau} p_{\theta}(x_t \mid x_{1:t-1}, \mathcal{G}, \Omega)\] <h3 id="heatmap-based-neural-solver">Heatmap-Based Neural Solver</h3> <p><strong>Overview</strong></p> <ul> <li>Uses lightweight GNNs for problem-solving, especially for large-scale CO problems like VRPs.</li> </ul> <p><strong>Limitations</strong></p> <ul> <li><strong>Non-Autoregressive Generation</strong>: Lacks partial solution order information, which can lead to poor solution quality.</li> <li><strong>Search Algorithm Dependence</strong>: Relies on search algorithms for high-quality solutions.</li> </ul> \[\pi(x \mid \mathcal{G}, \Omega, \theta) = p_{\theta}(\mathcal{H} \mid \mathcal{G}, \Omega) p(x_1) \prod_{t=2}^{\tau} \frac{\exp(\mathcal{H}_{x_{t-1}, x_t})}{\sum_{i=t}^{N} \exp(\mathcal{H}_{x_{t-1}, x_i})},\] <hr/> <h3 id="methodology-unified-divide-and-conquer-udc">Methodology: Unified Divide-and-Conquer (UDC)</h3> <p><img src="udc_img/figure_1.png" alt="poster"/></p> <h4 id="general-framework">General Framework</h4> <ul> <li><strong>Two Stages</strong>: Dividing and Conquering.</li> <li><strong>Dividing Stage</strong>: Generates initial solutions using an Anisotropic GNN (AGNN).</li> <li><strong>Conquering Stage</strong>: Decomposes the original instance into sub-problems and solves them using constructive neural solvers.</li> </ul> <p><strong>Solver Integration</strong></p> <ul> <li>Different solvers are used based on the type of CO problem.</li> <li><strong>AGNN</strong> for Maximum Independent Set (MIS).</li> <li><strong>POMO</strong> for Vehicle Routing Problem (VRP).</li> <li><strong>ICAM</strong> for 0-1 Knapsack Problem (KP).</li> </ul> <p><strong>Dividing Stage</strong></p> \[\pi_d(x_0|\mathcal{G}_D, \Omega, \phi) = \begin{cases} p(\mathcal{H}|\mathcal{G}_D, \Omega, \phi) p(x_{0,1}) \prod_{t=2}^\tau \frac{\exp(\mathcal{H}_{x_0, t-1, x_0, t})}{\sum_{i=t}^N \exp(\mathcal{H}_{x_0, t-1, x_0, i})}, &amp; \text{if } x_0 \in \Omega \\ 0, &amp; \text{otherwise} \end{cases}\] <ul> <li>original CO instance $\mathcal{G}$</li> <li>sparse graph $\mathcal{G}_D = { \mathbb{V}, \mathbb{E} }$</li> <li>parameter $\phi$ of Anisotropic Graph Neural Networks (AGNN)</li> <li>heatmap $\mathcal{H}$ (e.g For $N$-node VRPs, the heatmap $\mathcal{H} \in \mathbb{R}^{N×N}$ )</li> <li>initial solution $x_0 = (x_{0,1},…,x_{0,\tau})$, $\tau$ is length</li> </ul> <p><strong>Conquering Stage: Sub-problem Preparation</strong></p> <ul> <li>sub-problems ${ \mathcal{G}<em>1,…, \mathcal{G}</em>{\lfloor \frac{N}{n} \rfloor} }$</li> <li>${ \Omega_1,…, \Omega_{\lfloor \frac{N}{n} \rfloor} }$ constraints of sub-problems (e.g no self-loop in sub-TSPs)</li> </ul> <p><strong>Conquering Stage: Constructive Neural Conquering</strong></p> <p>\(\pi_c(s_k|\mathcal{G}_k, \Omega_k, \theta) = \begin{cases} \prod_{t=1}^n p(s_{k,t} | s_{k,1:t-1}, \mathcal{G}_k, \Omega_k, \theta), &amp; \text{if } s_k \in \Omega_k \\ 0, &amp; \text{otherwise} \end{cases}\)</p> <ul> <li>utilize constructive solvers with parameter $\theta$ for most involved sub-CO problems.</li> <li>sub-solution $s_{k} = (s_{k,1},…,s_{k,n})$, $k \in { 1,…, \lfloor \frac{N}{n} \rfloor}$</li> <li>conquering policy $\pi_c$</li> <li>Replacement of original solution fragments in the final conquering stage: sub-solutions with improvements on the objective function replace the original solution fragment in $x_0$</li> <li>Formation of merged solution: the merged solution becomes $x_1$</li> <li>Repeated execution of conquering stage: conquering stage can be executed repeatedly on the new merged solution</li> <li>Gradual improvement in solution quality: the solution after $r$ conquering stages is noted as $x_r$</li> </ul> <h3 id="training-method-divide-conquer-reunion-dcr">Training Method: Divide-Conquer-Reunion (DCR)</h3> <p><img src="udc_img/figure_2.png" alt="poster"/></p> <ul> <li>Dividing and conquering stages modeled as MDPs.</li> <li>Separate training for conquering and dividing policies.</li> <li>Need for problem-specific datasets.</li> <li>Lack of collaboration in optimizing policies.</li> <li>Impact of sub-optimal sub-problem decomposition.</li> <li>Divide-Conquer-Reunion (DCR) process introduction.</li> <li>Additional Reunion step for better integration of sub-problems.</li> <li>Improved stability and convergence in training.</li> <li>Use of REINFORCE algorithm for unified training.</li> <li>Baseline calculation for both dividing and conquering policies.</li> </ul> <table> <tbody> <tr> <td>$$\nabla \mathcal{L}d(\mathcal{G}) = \frac{1}{\alpha} \sum_{i=1}^\alpha \left( f(x_2^i, \mathcal{G}) - \frac{1}{\alpha} \sum_{j=1}^\alpha f(x_2^j, \mathcal{G}) \right) \nabla \log \pi_d(x_2^i</td> <td>\mathcal{G}_D, \Omega, \phi) $$</td> </tr> <tr> <td>$$\nabla \mathcal{L}{c1}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{1,i}, \mathcal{G}<em>{c}^0) - \frac{1}{\beta} \sum</em>{j=1}^\beta f{\prime}(s_{c}^{1,j}, \mathcal{G}<em>{c}^0) \right) \nabla \log \pi_c(s</em>{c}^{1,j}</td> <td>\mathcal{G}<em>{c}^0, \Omega</em>{c}, \theta) \right)$$</td> </tr> <tr> <td>$$\nabla \mathcal{L}{c2}(\mathcal{G}) = \frac{1}{\alpha \beta \lfloor \frac{N}{n} \rfloor} \sum_{c=1}^{\alpha \lfloor \frac{N}n \rfloor} \sum_{i=1}^\beta \left( \left( f{\prime}(s_{c}^{2,i}, \mathcal{G}<em>{c}^1) - \frac{1}{\beta} \sum</em>{j=1}^\beta f{\prime}(s_{c}^{2,j}, \mathcal{G}<em>{c}^1) \right) \nabla \log \pi_c(s</em>{c}^{2,j}</td> <td>\mathcal{G}<em>{c}^1, \Omega</em>{c}, \theta) \right)$$</td> </tr> </tbody> </table> <ul> <li>${ x_2^1, …, x_{2}^{\alpha} }$ represents the $\alpha$ sampled solutions.</li> <li>there are $\alpha \lfloor \frac{N}{n} \rfloor$ sub-problems $\mathcal{G}^{0}<em>{c},c \in { 1, …, \lfloor \frac{N}{n} \rfloor, …, \alpha \lfloor \frac{N}{n} \rfloor}$ generated based on ${ x_0^1, …, x</em>{0}^{\alpha} }$ in the first conquering stage</li> <li>$\alpha \lfloor \frac{N}{n} \rfloor$ can be regarded as the batch size of sub-problems</li> <li>The $\beta$ sampled sub-solutions for sub-problem $\mathcal{G}<em>{c}^{0}, \mathcal{G}</em>{c}^{1},c \in {1,…, \alpha \lfloor \frac{N}{n} \rfloor}$ are noted as ${s_{c}^{1,i},…,s_{c}^{1,\beta}},{s_{c}^{2,i},…,s_{c}^{2,i}}$.</li> </ul> <p><strong>Challenges and Proposed Solution</strong></p> <ul> <li>Existing methods fail to train dividing and conquering policies simultaneously, leading to unsolvable antagonisms.</li> <li><strong>Unified Training Requirement</strong>: DCR enables collaborative optimization of dividing and conquering policies by treating connections between sub-problems as new problems to reconquer.</li> </ul> <p><strong>Training Process with REINFORCE</strong></p> <ul> <li>Uses the REINFORCE algorithm to train both dividing and conquering policies, ensuring better reward estimation and improved convergence.</li> </ul> <h3 id="application-general-co-problems">Application: General CO Problems</h3> <p><strong>Conditions for Applicability</strong></p> <ol> <li><strong>Decomposable Objective Functions</strong>: The objective function must contain decomposable aggregate functions (i.e., no functions like Rank or Top-k).</li> <li><strong>Feasibility of Initial and Sub-Solutions</strong>: Ensured using feasibility masks.</li> <li><strong>Non-Uniqueness of Sub-Problem Solutions</strong>: Solutions for sub-problems should not be unique to ensure flexibility in merging sub-solutions.</li> </ol> <p><strong>Limitations</strong></p> <ul> <li>Complex CO problems may face issues where solutions cannot be guaranteed as legal through the process, limiting applicability.</li> <li>Problems such as TSPTW may have constraints that make ensuring legal initial and sub-solutions difficult.</li> </ul> <hr/> <h3 id="experiment">Experiment</h3> <p><img src="udc_img/table_2.png" alt="poster"/> <img src="udc_img/table_3.png" alt="poster"/> <img src="udc_img/table_4.png" alt="poster"/> <img src="udc_img/figure_3.png" alt="poster"/></p> <p><strong>Overview</strong></p> <ul> <li>To verify the applicability and efficiency of UDC, experiments were conducted across 10 different CO problems, including TSP, CVRP, KP, MIS, and more.</li> <li>UDC was compared to both classical and neural solvers.</li> </ul> <p><strong>Performance Evaluation</strong></p> <ul> <li>UDC demonstrated superior performance in terms of solution quality and computational efficiency across large-scale CO instances, ranging from 500-node to 2,000-node problems.</li> </ul> <p><strong>Comparison to Baselines</strong></p> <ul> <li>Classical solvers like LKH and other neural methods (e.g., ELG, GLOP) were used as baselines.</li> <li>UDC consistently outperformed other methods, particularly in large-scale settings where scalability is critical.</li> </ul> <hr/> <h3 id="conclusion">Conclusion</h3> <p><strong>Summary</strong></p> <ul> <li>UDC, with its novel DCR training mechanism, successfully addresses the limitations of existing neural divide-and-conquer methods for large-scale CO problems.</li> <li>The unified training scheme ensures that both dividing and conquering stages work in synergy, thereby achieving better overall optimization.</li> </ul> <p><strong>Future Work</strong></p> <ul> <li>Further improvements can be made by designing better loss functions for training.</li> <li>Extending UDC’s applicability to other complex CO problems not covered in the current study is another promising direction for future research.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="Divide-and-Conquer,"/><category term="Optimization"/><summary type="html"><![CDATA[A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems]]></summary></entry><entry><title type="html">a post with tabs2</title><link href="https://optreal.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs2"/><published>2024-05-02T00:32:13+00:00</published><updated>2024-05-02T00:32:13+00:00</updated><id>https://optreal.github.io/blog/2024/tabs</id><content type="html" xml:base="https://optreal.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="94697008-31ef-4fdf-aa69-efaeb7ee2ac8" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="94697008-31ef-4fdf-aa69-efaeb7ee2ac8" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="eceeb165-fe16-41d7-bb1e-a6271591f79c" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="eceeb165-fe16-41d7-bb1e-a6271591f79c" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="62e40559-e935-4c19-aa59-a29def17852b" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="62e40559-e935-4c19-aa59-a29def17852b" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://optreal.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://optreal.github.io/blog/2024/tabs</id><content type="html" xml:base="https://optreal.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="d7ba3828-50dc-413f-a555-e5902d200cf6" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="d7ba3828-50dc-413f-a555-e5902d200cf6" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="ad931547-2025-4099-bf24-d1ee81df0fce" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="ad931547-2025-4099-bf24-d1ee81df0fce" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="8ea245e1-bc6b-41e4-890f-81d043fb6bd4" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="8ea245e1-bc6b-41e4-890f-81d043fb6bd4" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://optreal.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://optreal.github.io/blog/2024/typograms</id><content type="html" xml:base="https://optreal.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="rouge-code"><pre><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://optreal.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://optreal.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://optreal.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry></feed>