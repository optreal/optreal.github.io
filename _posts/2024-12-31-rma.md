---
layout: post
title: "RMA: Rapid Motor Adaptation for Legged Robots"
description: "RMA: Rapid Motor Adaptation for Legged Robots"
date: 2024-12-30 16:00:00
tags: Robotics, Reinforcement-Learning
categories: Robotics
tabs: true
toc:
  sidebar: left
---

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/title.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

## I Introduction

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/fig1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/fig2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

### 1. Legged Robotics의 발전

- 물리적 역학 모델링 및 제어 이론 도구 활용
- 인간 설계자의 전문성 요구
- 강화 학습 및 모방 학습 기법 적용
  - 설계 부담 완화
  - 성능 향상 가능성

***

### 2. 강화 학습 기반 컨트롤러의 표준 패러다임

- 물리 시뮬레이션 환경에서 RL 기반 컨트롤러 훈련
- 시뮬레이션-현실 간 전이 (sim-to-real) 기술 적용
  - 물리 로봇과 시뮬레이터 모델의 차이
  - 현실 세계 지형의 다양성
  - 시뮬레이터의 물리적 한계
    - 접촉력, 변형 가능한 표면 등 복잡한 물리 현상

***

### 3. Quadruped Locomotion Challenge Solving Approach

- **A1 로봇 (Unitree)의 실험 플랫폼 사용**
- **실제 인간 보행의 특징**
  - 다양한 지형 적응 (흙, 언덕, 하중 등)
  - 피로 및 부상 대응 능력
- **RMA (Rapid Motor Adaptation)**
  - 실시간 온라인 적응 필요 (초 단위 적응)
  - 물리 세계에서 다중 실험 및 최적화 불가능
  - 실제 환경에서의 데이터 수집 어려움
    - 3-5분 데이터 수집조차 비현실적

***

### 4. 전략

- 기본 보행 정책 및 RMA를 시뮬레이션에서 훈련
- 직접 현실 세계에 배포

***

### 5. RMA (Rapid Motor Adaptation)

- **두 가지 하위 시스템**
  - 기본 정책 (Base Policy, $$\pi$$)
  - 적응 모듈 (Adaptation Module, $$\phi$$)
- **실시간 온라인 적응 지원**
  - 다양한 환경 구성에 적응

***

#### 5.1 Base Policy, $$\pi$$

- **훈련 방식**
  - 강화 학습(RL)을 통한 시뮬레이션 훈련
  - 환경 구성 벡터($$e_t$$)의 privileged information 활용 (마찰, 하중 등)
- **환경 구성 벡터($$e_t$$) 활용 과정**
  - 인코더 네트워크($$\mu$$)를 통해 잠재 특징 공간($$z_t$$)으로 인코딩
  - 잠재 벡터($$z_t$$) → 기본 정책($$π$$) 입력
  - 로봇의 현재 상태($$x_t$$) 및 이전 행동($$a_{t-1}$$)과 함께 사용
  - 목표 관절 위치($$a_t$$) 예측
- **End-to-End Training**
  - 정책($$π$$)과 인코더($$\mu$$)의 End-to-End 강화 학습

***

#### 5.2 Adaptation Module, $$\phi$$

- **목표**
  - 실시간으로 잠재 벡터($$z_t$$) 추정
  - Privileged information($$e_t$$) 없이 상태 및 행동 history를 통해 추정
- **원리**
  - 로봇 관절의 명령된 움직임과 실제 움직임의 차이를 기반으로 환경 특성 추정
  - 칼만 필터와 유사한 접근법
- **훈련 과정**
  - 시뮬레이션에서 감독 학습으로 훈련
  - 상태 이력과 잠재 벡터($$z_t$$) 계산 가능
- **실행 시 동작**
  - 실시간 추정으로 정책($$π$$)에 환경 특성 벡터($$z_t$$) 제공
  - 비동기 병렬 실행
    - Base policy($$π$$): 100Hz 실행
    - Adaptation module($$φ$$): 10Hz 실행
    - 중앙 클럭 없이 독립 실행
  - 잠재 벡터($$z_t$$)를 기본 정책에 전달하여 행동($$a_t$$) 예측 지원

***

### 6. 기존 접근법과의 비교

- **기존 접근법**
  - 새로운 환경에서 소규모 데이터셋 수집 후 정책 적응
  - 물리적 매개변수 (마찰 등) 또는 잠재 인코딩 활용
- **기존 접근법의 문제점**
  - 초기 데이터셋 수집 중 낙상 및 로봇 손상 위험
- **RMA 접근법의 장점**
  - 잠재 벡터($$z_t$$)의 빠른 추정
  - 즉각적 정책 적응을 통해 낙상 방지

***

### 7. RMA의 주요 특징 및 핵심 원리

***

#### 7.1 Base Policy

- **기존 접근법과의 유사점**
  - 환경 매개변수를 추가 인자로 활용한 강화 학습(RL) 훈련
- **새로운 측면**
  - 다양한 지형 생성기 사용
  - 생체에너지학에서 영감을 받은 자연스러운 보상 함수 사용
  - Reference demonstrations 없이 보행 정책 학습 가능

***

#### 7.2 Adaptation Module의 작동 원리

- **System Identification의 원리 활용**
  - 최적화 문제로서의 시스템 식별 접근
  - 신경망을 사용한 입력-출력 관계 근사
- **완벽한 시스템 식별의 불필요성**
  - Extrinsics Vector($$z_t$$): 환경 매개변수의 저차원 비선형 투영
  - 정확한 '정답' 벡터가 아닌 '올바른 행동'을 유도하는 벡터로 최적화
- **다양한 훈련 상황 제공**
  - 프랙탈 지형 생성기 사용
  - 질량, 마찰 등 매개변수 무작위화
  - 다양한 물리적 맥락에서의 보행 반응 훈련

***

### 8. 실제 환경에서의 성능 평가

- **다양한 지형에서의 검증**
  - 미끄러운 표면
  - 불규칙한 지형
  - 변형 가능한 표면 (스펀지, 매트리스 등)
  - 자연 지형 (잔디, 긴 식물, 콘크리트, 자갈, 바위, 모래 등)

***

### 9. RMA의 주요 기여

- 실시간 적응 모듈을 통한 환경 변화 대응
- 데이터셋 의존 없이 즉각적인 정책 조정
- 강건한 보행 정책의 다양한 환경 적용 가능성

***

## II Related Work

***

### 1. 전통적인 제어 기반 접근법 (Control-Based Methods)

- **주요 로봇 사례**
  - MIT Cheetah 3
    - 정규화된 **모델 예측 제어 (MPC)** 사용
    - 단순화된 동역학 활용
    - 고속 이동 및 장애물 점프 가능
  - ANYmal 로봇
    - 매개변수화된 제어기 최적화
    - 역진자 모델 기반 계획 수행
- **한계점**
  - 정확한 실제 동역학 모델 요구
  - 로봇에 대한 사전 지식 필요
  - 보행 및 행동 수동 튜닝 요구
- **개선 방안**
  - 제어기 최적화 및 MPC 결합
    - 문제 일부 완화
    - 여전히 과제별 특징 엔지니어링 필요

***

### 2. 학습 기반 보행 접근법 (Learning for Legged Locomotion)

- **초기 시도**
  - DARPA Learning Locomotion Program
- **최근 동향**
  - 심층 강화 학습(RL)의 도입
    - 인간 전문성 의존도 감소
    - 시뮬레이션에서 우수한 결과 도출
- **한계점**
  - 정책의 현실 세계 전이 어려움
- **해결책**
  - 현실 세계에서 직접 훈련
    - 단순한 환경에 한정됨
    - 복잡한 환경에서는 비효율적 및 안전 문제 발생

***

### 3. Sim-to-Real 강화 학습 (Sim-to-Real Reinforcement Learning)

- **Domain Randomization**
  - 다양한 환경 매개변수 및 센서 노이즈 사용
  - 강건한 정책 학습 가능
  - 최적성(Optimality) 희생 → 과도하게 보수적인 정책 생성
- **Simulation Accuracy Improvement**
  - 모터 모델 개선
    - Tan et al.: 선형 함수로 모터 데이터 피팅
    - Hwangbo et al.: 신경망으로 액추에이터 모델 매개변수화
- **Limitations**
  - 초기 데이터 수집 필요
  - 새로운 환경마다 재조정 필요

***

### 4. 시스템 식별 및 적응 (System Identification and Adaptation)

- **Online System Identification**
  - 시뮬레이션에서 훈련된 모듈을 통해 매개변수 추정
  - 진화 알고리즘을 사용한 직접 최적화
- **Latent Embedding 활용**
  - 저차원 잠재 임베딩을 통해 시스템 매개변수 표현
  - 실제 환경 롤아웃 기반 최적화
    - 정책 그래디언트 방법 사용
    - 베이지안 최적화 적용
    - 무작위 탐색 활용
- **Meta-Learning**
  - 빠른 온라인 적응을 위한 정책 네트워크 초기화 학습
- **Limitations**
  - 여러 실제 환경 롤아웃 필요
  - 현실 세계에서의 최적화 비용 및 시간 문제

***

### 5. 요약 및 비교

- **전통적 제어 접근법:** 정확한 모델과 수작업 튜닝 요구
- **학습 기반 접근법:** 시뮬레이션 성능 우수, 현실 전이 어려움
- **Sim-to-Real 접근법:** 도메인 무작위화와 시뮬레이션 정확도 향상
- **시스템 식별 및 적응:** 저차원 잠재 임베딩과 메타 학습 활용

**결론:** RMA는 기존 접근법의 한계를 극복하며 다양한 환경에서 강건한 보행 정책을 실시간으로 적응 가능하게 함.

***

## III Rapid Motor Adaptation

***

### 1. Base Policy

***

#### 1.1 Base Policy

$$
a_t = \pi(x_t, a_{t-1}, z_t)
$$

- **Input**
  - current state: $$x_t \in \mathbb{R}^{30}$$
  - previous action: $$a_{t-1} \in \mathbb{R}^{12}$$
  - extrinsics vector: $$z_t \in \mathbb{R}^8$$
    - form: $$z_t=\mu(e_t)$$
    - environment vector $$e_t \in \mathbb{R}^{17}$$
    - environment factor endoder: $$\mu$$
- **Ouput**
  - next action: $$a_{t} \in \mathbb{R}^{12}$$
  - $$a_t$$는 12개의 로봇 관절이 원하는 위치로 설정된 값으로, PD 제어기를 사용하여 토크로 변환

***

#### 1.2 정책 학습

- **네트워크 구조:**
  - 정책 네트워크($$\pi$$) 및 인코더($$\mu$$): 다층 퍼셉트론(MLP)으로 구현
- **End-to-End 훈련:** Model-free RL
- **목표:** 정책의 기대 반환($$J(\pi)$$) 최대화

$$
J(\pi) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[\sum_{t=0}^{T-1} \gamma^t r_t \right]
$$

- **경로($$\tau$$):** 정책($$\pi$$)을 따르는 에이전트의 상태, 행동, 보상 시퀀스

***

#### 1.3 자연적 제약을 통한 안정적인 보행 (Stable Gait through Natural Constraints)

- **자연적 제약 활용:**
  - **생체에너지학 기반 보상 함수:** 작업 최소화, 지면 충격 감소 목표
  - **고르지 않은 지형에서 훈련:** 추가 보상 없이 보행 강건성 확보
- **학습 환경:**
  - 인공적인 시뮬레이션 노이즈 대신 자연적 제약 적용
- **정책 전이:**
  - 단순 환경(콘크리트, 나무 바닥)으로 자연스럽게 전이
  - 추가적인 미세 조정(finetuning) 불필요

***

#### 1.4 기존 Sim-to-Real 접근법과의 차이점

- **기존 접근법:**
  - 시뮬레이션과 현실 간 보정(calibration) 수행 [51, 23]
  - 현실 세계에서 정책 미세 조정(finetuning) 필요 [41]
- **RMA 접근법:**
  - 적응 모듈(Adaptation Module)을 통해 단순 환경에서 복잡한 지형으로 확장 가능

***

#### 1.5 강화 학습 보상 (RL Rewards)

보상 함수는 에이전트가 최대 0.35 m/s 속도로 전진하도록 장려 및 불규칙하고 비효율적인 움직임에 패널티 부여.

- **선형 속도: $$\mathbf{v}$$**
- **자세: $$\mathbf{\theta}$$**
- **각속도: $$\omega$$**
- **관절 각도: $$\mathbf{q}$$**
- **관절 속도: $$\dot{\mathbf{q}}$$**
- **관절 토크: $$\tau$$**
- **발의 지면 반력: $$\mathbf{f}$$**
- **발의 속도: $$\mathbf{v}_\mathbf{f}$$**
- **발 접촉 이진 벡터: $$\mathbf{g}$$**

시간 $$t$$에서의 보상은 다음 요소들의 합으로 정의됩니다:

1. **전진:** $$\operatorname{min}(v^t_x, 0.35)$$
2. **측면 이동 및 회전:** $$- \| v_{y}^{t} \|^2 - \| \omega_{\text{yaw}}^{t} \|^2$$
3. **작업:** $$- \| \boldsymbol{\tau}^{T} \cdot (\mathbf{q}^{t} - \mathbf{q}^{t-1}) \|$$
4. **지면 충격:** $$- \| \mathbf{f}^{t} - \mathbf{f}^{t-1} \|^2$$
5. **부드러움:** $$- \| \mathbf{\tau}^{t} - \mathbf{\tau}^{t-1} \|^2$$
6. **행동 크기:** $$- \| \mathbf{a}^{t} \|^2$$
7. **관절 속도:** $$- \| \dot{\mathbf{q}}^{t} \|^2$$
8. **자세 안정성:** $$- \| \boldsymbol{\theta}^{t}_{\text{roll, pitch}} \|^2$$
9. **Z축 가속도:** $$- \| v^t_z \|^2$$
10. **발 미끄러짐:** $$- \| \operatorname{diag}(\mathbf{g}^{t}) \cdot \mathbf{v}_{\mathbf{f}}^{t} \|^2$$

각 보상 항목의 스케일링 계수: 20, 21, 0.002, 0.02, 0.001, 0.07, 0.002, 1.5, 2.0, 0.8.

***

#### 1.6 Training Curriculum

위 보상 함수로 학습 시, 움직직에 대한 패널티로 인해 제자리에서 머무르는 현상 발생 가능. 이를 방지하기 위해 다음 전략 사용:

1. **패널티 계수:** 초기에는 매우 작은 패널티 계수로 시작, 훈련이 진행됨에 따라 점진적으로 계수 증가.
2. **환경 변화:** 질량, 마찰, 모터 힘 등의 난이도를 선형적으로 증가.
3. **지형 난이도:** 지형에 대한 별도의 커리큘럼 없음, 고정된 난이도에서 무작위 지형 프로필 샘플링.

***

### 2. Adaptation Module

***

#### 2.1 Adaptation Module

$$
\hat{z_t} = \phi(x_{t-k:t-1}, a_{t-k:t-1})
$$

- **역할**
  - 실제 환경에서 $$z_t$$를 온라인 추정
  - privileged environment configuration, **$$e_t$$** 없이 작동
- **Input**
  - history of robot’s states: $$x_{t-k:t-1}$$
  - history of robot’s actions($$a_{t-k:t-1}$$)
- **Output**
  - predicted extrinsics vector: $$\hat{z_t}$$
- **Hyperparameter**
  - k=50 사용 (약 0.5초에 해당)

***

#### 2.2 모델 학습

$$
\operatorname{MSE}(\hat{z}_{t}, z_{t}) = \| \hat{z}_{t} - z_{t} \|^2
$$

- **네트워크 구조**:
  - **1D CNN** 사용: 시간적 상관관계 포착

***

#### 2.3 데이터 수집 방식

- **문제점**
  - Base policy($$\pi$$)을 기반으로 한 데이터셋은 최적 경로만 포함
  - 실제 배포 시 발생할 편차를 충분히 다루지 못함
- **해결책: On-Policy 데이터 사용**
  - 무작위로 초기화된 **Adaptation module($$\phi$$)** 사용하여 데이터 수집
  - 상태-행동 이력과 목표 잠재 벡터($$z_t$$) 쌍으로 학습 진행
- **훈련 절차**
  - Base policy($$\pi$$)을 $$\hat{z_t}$$로 롤아웃
  - state action history과 ground truth $$z_t$$ 생성
  - 이 과정을 반복하여 수렴 유도
- **강건성 확보 메커니즘**
  - 랜덤 초기화된 **Adaptation module**($$\phi$$) 사용
  - **imperfect prediction**($$\hat{z_t}$$) 수용
  - 훈련 중 충분한 탐색 경로 확보

***

### 3. Asynchronous Deployment

***

#### 3.1 비동기식 배포

- **훈련 및 배포**
  - 완전한 시뮬레이션 기반 훈련
  - 현실 세계로 직접 배포 (수정 및 미세 조정 불필요)
- **비동기식 실행 구조**
  - 두 하위 시스템의 비동기적 실행
  - 서로 다른 주기로 작동 → 온보드 컴퓨팅 부담 최소화

***

#### 3.2 Adaptation Module

- **작동 주기:** 10Hz
- **입력:** 최근 50 타임 스텝의 상태 및 행동 이력
- **출력:** Extrinsics Vector, $$\hat{z_t}$$
- **특징:** 비교적 느리게 업데이트되나 성능에 영향 없음

***

#### 3.3 Base Policy

- **작동 주기:** 100Hz
- **입력:**
  - the most recent $$\hat{z_t}$$ generated by the adaptation module
  - current state: $$x_t \in \mathbb{R}^{30}$$
  - previous action: $$a_{t-1} \in \mathbb{R}^{12}$$
- **Output**
  - next action: $$a_{t} \in \mathbb{R}^{12}$$
- **특징:**
  - 빠른 주기로 작동
  - 적응 모듈의 비동기적 업데이트 수용 가능

***

#### 3.4 단일 정책 설계의 한계점

- **상태 및 행동 이력을 직접 입력받는 단일 정책 접근법의 문제점**
  - **비자연스러운 보행 패턴 발생**
  - **시뮬레이션 성능 저하**
  - **온보드 컴퓨팅 한계:** 10Hz에서만 작동 가능
  - **비동기적 설계 불가능:** 하위 시스템 간 동기화 및 보정 필요

***

### 3.5 비동기 설계의 장점

- 상대적으로 느리게 변하는 Extrinsics Vector($$\hat{z_t}$$)와 빠르게 변하는 로봇 상태($$x_t$$)의 분리
- 효율적인 온보드 컴퓨팅 자원 활용 가능
- 동기화 및 추가 보정 불필요 → 원활한 Real-world 배포 가능

***

## IV Experimental Setup

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/table1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

***

### 1. Environment Details

***

#### 1.1 Hardware Details

- **로봇 플랫폼:** Unitree의 A1 로봇
- **특징:**
  - 중형 크기, 저비용 사족 보행 로봇
  - **자유도:** 총 18자유도 (12자유도는 액추에이터 작동, 다리당 3개 모터)
  - **무게:** 약 12kg
- **센서 입력:**
  - **모터 인코더:** 관절 위치 및 속도 측정
  - **IMU 센서:** 롤(Roll), 피치(Pitch) 측정
  - **발 센서:** 이진화된 발 접촉 상태
- **제어 방식:**
  - **관절 위치 제어 사용**
  - **PD 컨트롤러로 토크 변환**
    - Gain: $$K_p = 55, K_d = 0.8$$

***

#### 1.2 Simulation Setup

- **시뮬레이터:** RaiSim 사용
- **로봇 모델:** Unitree A1 URDF 파일 사용
- **지형 생성기:** 프랙탈 지형 생성기
  - fractal octaves: 2
  - fractal lacunarity: 2.0
  - fractal gain: 0.25
  - Z-scale: 0.27
- **에피소드 길이:** 최대 1000 steps
  - 조기 종료 조건:
    - 높이 < 0.28m
    - 롤 각도 > 0.4 radians
    - 피치 각도 > 0.2 radians
- **제어 주파수:** 100 Hz
- **시뮬레이션 시간 간격:** 0.025s = 1/40

***

#### 1.3 State-Action Space

- **상태 벡터(**$$x_t \in \mathbb{R}^{30}$$**)**
  - **관절 위치:** 12개 값
  - **관절 속도:** 12개 값
  - **몸체 롤 및 피치:** 2개 값
  - **발 접촉 상태:** 4개 값
- **행동 벡터(**$$a \in \mathbb{R}^{12}$$**)**
  - **관절 목표 위치 예측:** $$a = \hat{q} \in \mathbb{R}^{12}$$
  - **토크 변환:** PD 컨트롤러 사용
    - 수식: $$\tau = K_p (\hat{q} - q) + K_d (\hat{\dot{q}} - \dot{q})$$
  - **이득 값:** $$K_p$$ 및 $$K_d$$ (수동 설정)
  - **목표 관절 속도:** $$\hat{\dot{q}} = 0$$

***

#### 1.4 Environmental Variations

- **환경 벡터($$e_t \in \mathbb{R}^{17}$$)**
  - **질량 및 로봇 내 위치:** 3차원
  - **모터 강도:** 12차원
  - **마찰 계수:** 스칼라 값
  - **지형 높이:** 스칼라 값
- **지형 높이 측정 방법:**
  - 각 발 아래 지형 높이 값을 소수점 첫째 자리로 이산화
  - 네 발 중 최대값 사용
- **지형 프로파일 특성:**
  - 고정된 난이도
  - 로컬 지형 높이의 동적 변화

***

### 2. Training Details

***

#### 2.1 Base Policy and Environment Factor Encoder Architecture

- **Base Policy**
  - **구조:** 3-layer MLP
  - **Hidden layer sizes:** 128
- **Environment Factor Encoder**
  - **구조:** 3층 다층 퍼셉트론(MLP)
  - **Hidden layer sizes:** 256, 128

***

#### 2.2 Adaptation Module Architecture

- **1단계: 상태 및 행동 임베딩**
  - **구조:** 2-layer MLP
- **2단계: 시간적 상관관계 학습**
  - **구조:** 3-layer 1D CNN
  - **입력-출력 특성:**
    - **1층:** 입력 채널: 32, 출력 채널: 32, 커널 크기: 8, 스트라이드: 4
    - **2층:** 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1
    - **3층:** 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1
- **3단계: 최종 예측**
  - **구조**: Linear projection

***

## V Results and Analysis

**비교 대상:**

- **시뮬레이션:** 여러 기준 모델(Table II)
- **현실 환경:** 제조사 제공 A1 컨트롤러(Figure 3)
- **다양한 야외 지형:** 다양한 환경에서 RMA 테스트(Figure 1)

***

### 1. Baselines

1. **A1 컨트롤러 (A1 Controller)**
    - 제조사 기본 컨트롤러
    - 힘 기반 제어 및 모델 예측 제어(MPC) 사용
2. **Robustness through Domain Randomization**
    - Extrinsics vector($$z_t$$) 없이 학습된 기본 정책
    - 훈련 범위 내 변화를 견디도록 설계
3. **Expert Adaptation Policy**
    - 시뮬레이션에서 true extrinsics vector($$z_t$$) 사용
    - RMA의 이론적 상한 성능 제공
4. **RMA w/o Adaptation**
    - Adaptation module 없이 base policy 단독 평가
    - Adaptation module의 중요성 분석
5. **System Identification**
    - Extrinsics vector($$\hat{z_t}$$) 대신 system parameters $$\hat{e_t}$$ 직접 예측
6. **AWR (Advantage Weighted Regression for Domain Adaptation)**
    - 오프라인으로 Extrinsics vector($$\hat{z_t}$$) 최적화
    - 실제 환경 롤아웃을 기반으로 테스트 환경에 적응

**학습 조건 통일:**

- 동일한 네트워크 아키텍처 사용
- 동일한 보상 함수 사용
- 동일한 하이퍼파라미터 적용

***

### 2. Metrics

1. **Time-to-Fall (TTF):**
    - 최대 에피소드 길이로 나눈 **시간당 추락 비율**
    - 0~1 범위 정규화
2. **평균 전진 보상 (Average Forward Reward):**
3. **성공률 (Success Rate):**
4. **이동 거리 (Distance Covered):**
5. **적응에 필요한 탐색 샘플 수 (Exploration Samples Needed for Adaptation):**
6. **토크 사용량 (Torque Applied):**
7. **부드러움 (Smoothness):** 토크 미분 값
8. **지면 충격 (Ground Impact):** 발이 지면에 미치는 충격 수준

***

### 3. Indoor Experiments

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/fig3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

- **비교 대상:**
  - **RMA (Rapid Motor Adaptation)**
  - **A1 기본 컨트롤러**
  - **적응 모듈 제거된 RMA (RMA w/o Adaptation)**
- **비교 목적:** 로봇 하드웨어 손상 최소화
- **실험 조건:**
  - 각 방법당 5회 실험
  - 심각한 실패 발생 시 2회만 진행 후 실패로 기록
- **평가 지표:**
  - 성공률 (Success Rate)
  - 낙하 시간 (TTF: Time-to-Fall)
  - 이동 거리 (Distance Covered)

***

#### 3.1 Setups

1. **n-kg 페이로드 (n-kg Payload)**
    - **목표:** n-kg 하중을 싣고 300cm 걷기
2. **StepUp-n**
    - **목표:** n-cm 높이의 계단 오르기
    - **평가 지표:** 성공률만 기록
3. **Uneven Foam**
    - **목표:** 중앙이 솟아있는 스펀지 위 180cm 걷기
4. **Mattress**
    - **목표:** 메모리폼 매트리스 위 60cm 걷기
5. **StepDown-n**
    - **목표:** n-cm 높이의 계단 내려오기
    - **평가 지표:** 성공률만 기록
6. **Incline**
    - **목표:** 6도 경사로 오르기
7. **Oily Surface**
    - **목표:** 미끄러운 오일이 뿌려진 표면 건너기

***

#### 3.2 Results

- **RMA의 성능:**
  - **모든 환경에서 높은 성공률 달성**
  - **A1 컨트롤러 대비 월등한 성능 발휘**
- **적응 모듈의 중요성:**
  - 적응 모듈 비활성화 시 성능 크게 저하
  - 많은 과제에서 문제 해결 불가
- **A1 컨트롤러의 한계:**
  - **Uneven Foam:** 불안정한 지지대에서 불안정함 발생
  - **StepUp/StepDown:** 높은 단차에서 실패 빈번
  - **Payload:** 5kg 이상의 하중에서는 처짐 발생 및 낙하
- **RMA의 강점:**
  - 최대 12kg (로봇 체중의 100%) 하중 운반 성공
  - 높이를 유지하며 안정적 보행 가능
- **RMA w/o 적응 모듈:**
  - 대부분 낙하하지 않음
  - 전진 움직임은 거의 없음
- **Oily Surface**
  - **RMA:** 성공적으로 미끄러운 지형 통과
  - **RMA w/o Adaptation:** 나무 바닥에서는 별도 미세 조정 없이도 성공적 보행 가능

***

### 4. Outdoor Experiments

- **목표:** RMA의 성능을 다양한 야외 환경에서 검증
- **테스트 환경:**
  - 모래 (Sand)
  - 진흙 (Mud)
  - 흙길 (Dirt)
  - 높은 식물 지대 (Tall Vegetation)
  - 덤불 (Bush)
  - 계단 (Stairs)
  - 건설 폐기물 (Construction Debris)

***

#### 4.1 Results

1. **모래, 진흙, 흙길 (Sand, Mud, Dirt)**
    - **성공률:** 100%
    - **도전 과제:**
        - 발이 빠지거나 달라붙는 문제 발생
        - 동적 발판 조정 필요
2. **높은 식물 지대 및 덤불 (Tall Vegetation, Bush)**
    - **성공률:** 100%
    - **도전 과제:**
        - 발이 장애물에 얽혀 불안정 발생
        - 주기적 발판 불안정성 해결 필요
        - 장애물에 맞서 강력한 추진력 필요
3. **하이킹 계단 (Stairs on Hiking Trail)**
    - **성공률:** 70%
    - **도전 과제:**
        - 훈련 중 계단을 경험하지 못함
        - 불규칙한 발판과 높낮이 조정 필요
4. **건설 폐기물 (Construction Debris)**
    - **하위 실험:**
        - 진흙 더미 (Mud Pile): 성공률 100%
        - 시멘트 더미 (Cement Pile): 성공률 80%
        - 자갈 더미 (Pebble Pile): 성공률 80%
    - **도전 과제:**
        - 급경사 및 측면 경사면
        - 불규칙한 발판과 무게 균형 필요

***

### 5. Simulation Results

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/table2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

- **비교 대상:** 여러 기준 방법 (Table II)
- **훈련 및 테스트 설정:**
  - **훈련 및 테스트 매개변수:** Table I에 따라 샘플링
  - **재샘플링 확률:**
    - 훈련: 스텝당 0.004
    - 테스트: 스텝당 0.01
- **평가 방법:**
  - **정책 초기화:** 무작위로 3회 초기화된 정책 사용
  - **에피소드 수:** 초기화당 1000회 에피소드 실행
- **성능 평가:** 평균값 도출

***

#### 5.1 Results

1. **AWR (Advantage Weighted Regression)**
   - **적응 속도 저하:** 변화하는 환경에 느린 적응
   - **성능 저하:** 지속적인 환경 변화에 취약
2. **Robust (도메인 무작위화 기반 강건성)**
   - **환경 특성 무시:** Extrinsics vector($$z_t$$) 사용하지 않음
   - **보수적 정책:** 성능 저하 발생
3. **System Identification (SysID)**
   - **환경 매개변수($$e_t$$) 추정의 어려움:** 명시적 매개변수 추정 어려움
   - **불필요성:** 높은 성능 달성을 위해 필수적이지 않음
4. **RMA w/o Adaptation (적응 모듈 미사용 RMA)**
   - **성능 급감:** 적응 모듈 없이는 성능 저하 심각

***

### 6. Adaptation Analysis

<div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/rma/fig4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

- **목표:** 미끄러운 표면에서 RMA의 보행 패턴, 토크 프로파일, Extrinsics vector($$\hat{z_t}$$) 분석
- **실험 조건:**
  - **표면:** 플라스틱 바닥에 오일 도포
  - **로봇 발:** 플라스틱으로 덮음

***

#### 6.1 Results

- **성공률:** 90%
- **분석 항목:**
  - 무릎 토크 프로파일 (Torque Profile of Knee)
  - 보행 패턴 (Gait Pattern)
  - Extrinsics vector($$\hat{z_t}$$) 분석: 1번째 및 5번째 성분의 중위수 필터링된 값
