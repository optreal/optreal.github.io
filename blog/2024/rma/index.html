<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RMA: Rapid Motor Adaptation for Legged Robots | Optreal </title> <meta name="author" content="Optreal "> <meta name="description" content="RMA: Rapid Motor Adaptation for Legged Robots"> <meta name="keywords" content="optreal, jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://optreal.github.io/blog/2024/rma/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Optreal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/">tutorials </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">RMA: Rapid Motor Adaptation for Legged Robots</h1> <p class="post-meta"> Created in December 30, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> Robotics,</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement-Learning</a>   ·   <a href="/blog/category/robotics"> <i class="fa-solid fa-tag fa-sm"></i> Robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/title-480.webp 480w,/assets/img/rma/title-800.webp 800w,/assets/img/rma/title-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/title.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="i-introduction">I Introduction</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig1-480.webp 480w,/assets/img/rma/fig1-800.webp 800w,/assets/img/rma/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig2-480.webp 480w,/assets/img/rma/fig2-800.webp 800w,/assets/img/rma/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="1-legged-robotics의-발전">1. Legged Robotics의 발전</h3> <ul> <li>물리적 역학 모델링 및 제어 이론 도구 활용</li> <li>인간 설계자의 전문성 요구</li> <li>강화 학습 및 모방 학습 기법 적용 <ul> <li>설계 부담 완화</li> <li>성능 향상 가능성</li> </ul> </li> </ul> <hr> <h3 id="2-강화-학습-기반-컨트롤러의-표준-패러다임">2. 강화 학습 기반 컨트롤러의 표준 패러다임</h3> <ul> <li>물리 시뮬레이션 환경에서 RL 기반 컨트롤러 훈련</li> <li>시뮬레이션-현실 간 전이 (sim-to-real) 기술 적용 <ul> <li>물리 로봇과 시뮬레이터 모델의 차이</li> <li>현실 세계 지형의 다양성</li> <li>시뮬레이터의 물리적 한계 <ul> <li>접촉력, 변형 가능한 표면 등 복잡한 물리 현상</li> </ul> </li> </ul> </li> </ul> <hr> <h3 id="3-quadruped-locomotion-challenge-solving-approach">3. Quadruped Locomotion Challenge Solving Approach</h3> <ul> <li><strong>A1 로봇 (Unitree)의 실험 플랫폼 사용</strong></li> <li> <strong>실제 인간 보행의 특징</strong> <ul> <li>다양한 지형 적응 (흙, 언덕, 하중 등)</li> <li>피로 및 부상 대응 능력</li> </ul> </li> <li> <strong>RMA (Rapid Motor Adaptation)</strong> <ul> <li>실시간 온라인 적응 필요 (초 단위 적응)</li> <li>물리 세계에서 다중 실험 및 최적화 불가능</li> <li>실제 환경에서의 데이터 수집 어려움 <ul> <li>3-5분 데이터 수집조차 비현실적</li> </ul> </li> </ul> </li> </ul> <hr> <h3 id="4-전략">4. 전략</h3> <ul> <li>기본 보행 정책 및 RMA를 시뮬레이션에서 훈련</li> <li>직접 현실 세계에 배포</li> </ul> <hr> <h3 id="5-rma-rapid-motor-adaptation">5. RMA (Rapid Motor Adaptation)</h3> <ul> <li> <strong>두 가지 하위 시스템</strong> <ul> <li>기본 정책 (Base Policy, \(\pi\))</li> <li>적응 모듈 (Adaptation Module, \(\phi\))</li> </ul> </li> <li> <strong>실시간 온라인 적응 지원</strong> <ul> <li>다양한 환경 구성에 적응</li> </ul> </li> </ul> <hr> <h4 id="51-base-policy-pi">5.1 Base Policy, \(\pi\)</h4> <ul> <li> <strong>훈련 방식</strong> <ul> <li>강화 학습(RL)을 통한 시뮬레이션 훈련</li> <li>환경 구성 벡터(\(e_t\))의 privileged information 활용 (마찰, 하중 등)</li> </ul> </li> <li> <strong>환경 구성 벡터(\(e_t\)) 활용 과정</strong> <ul> <li>인코더 네트워크(\(\mu\))를 통해 잠재 특징 공간(\(z_t\))으로 인코딩</li> <li>잠재 벡터(\(z_t\)) → 기본 정책(\(π\)) 입력</li> <li>로봇의 현재 상태(\(x_t\)) 및 이전 행동(\(a_{t-1}\))과 함께 사용</li> <li>목표 관절 위치(\(a_t\)) 예측</li> </ul> </li> <li> <strong>End-to-End Training</strong> <ul> <li>정책(\(π\))과 인코더(\(\mu\))의 End-to-End 강화 학습</li> </ul> </li> </ul> <hr> <h4 id="52-adaptation-module-phi">5.2 Adaptation Module, \(\phi\)</h4> <ul> <li> <strong>목표</strong> <ul> <li>실시간으로 잠재 벡터(\(z_t\)) 추정</li> <li>Privileged information(\(e_t\)) 없이 상태 및 행동 history를 통해 추정</li> </ul> </li> <li> <strong>원리</strong> <ul> <li>로봇 관절의 명령된 움직임과 실제 움직임의 차이를 기반으로 환경 특성 추정</li> <li>칼만 필터와 유사한 접근법</li> </ul> </li> <li> <strong>훈련 과정</strong> <ul> <li>시뮬레이션에서 감독 학습으로 훈련</li> <li>상태 이력과 잠재 벡터(\(z_t\)) 계산 가능</li> </ul> </li> <li> <strong>실행 시 동작</strong> <ul> <li>실시간 추정으로 정책(\(π\))에 환경 특성 벡터(\(z_t\)) 제공</li> <li>비동기 병렬 실행 <ul> <li>Base policy(\(π\)): 100Hz 실행</li> <li>Adaptation module(\(φ\)): 10Hz 실행</li> <li>중앙 클럭 없이 독립 실행</li> </ul> </li> <li>잠재 벡터(\(z_t\))를 기본 정책에 전달하여 행동(\(a_t\)) 예측 지원</li> </ul> </li> </ul> <hr> <h3 id="6-기존-접근법과의-비교">6. 기존 접근법과의 비교</h3> <ul> <li> <strong>기존 접근법</strong> <ul> <li>새로운 환경에서 소규모 데이터셋 수집 후 정책 적응</li> <li>물리적 매개변수 (마찰 등) 또는 잠재 인코딩 활용</li> </ul> </li> <li> <strong>기존 접근법의 문제점</strong> <ul> <li>초기 데이터셋 수집 중 낙상 및 로봇 손상 위험</li> </ul> </li> <li> <strong>RMA 접근법의 장점</strong> <ul> <li>잠재 벡터(\(z_t\))의 빠른 추정</li> <li>즉각적 정책 적응을 통해 낙상 방지</li> </ul> </li> </ul> <hr> <h3 id="7-rma의-주요-특징-및-핵심-원리">7. RMA의 주요 특징 및 핵심 원리</h3> <hr> <h4 id="71-base-policy">7.1 Base Policy</h4> <ul> <li> <strong>기존 접근법과의 유사점</strong> <ul> <li>환경 매개변수를 추가 인자로 활용한 강화 학습(RL) 훈련</li> </ul> </li> <li> <strong>새로운 측면</strong> <ul> <li>다양한 지형 생성기 사용</li> <li>생체에너지학에서 영감을 받은 자연스러운 보상 함수 사용</li> <li>Reference demonstrations 없이 보행 정책 학습 가능</li> </ul> </li> </ul> <hr> <h4 id="72-adaptation-module의-작동-원리">7.2 Adaptation Module의 작동 원리</h4> <ul> <li> <strong>System Identification의 원리 활용</strong> <ul> <li>최적화 문제로서의 시스템 식별 접근</li> <li>신경망을 사용한 입력-출력 관계 근사</li> </ul> </li> <li> <strong>완벽한 시스템 식별의 불필요성</strong> <ul> <li>Extrinsics Vector(\(z_t\)): 환경 매개변수의 저차원 비선형 투영</li> <li>정확한 ‘정답’ 벡터가 아닌 ‘올바른 행동’을 유도하는 벡터로 최적화</li> </ul> </li> <li> <strong>다양한 훈련 상황 제공</strong> <ul> <li>프랙탈 지형 생성기 사용</li> <li>질량, 마찰 등 매개변수 무작위화</li> <li>다양한 물리적 맥락에서의 보행 반응 훈련</li> </ul> </li> </ul> <hr> <h3 id="8-실제-환경에서의-성능-평가">8. 실제 환경에서의 성능 평가</h3> <ul> <li> <strong>다양한 지형에서의 검증</strong> <ul> <li>미끄러운 표면</li> <li>불규칙한 지형</li> <li>변형 가능한 표면 (스펀지, 매트리스 등)</li> <li>자연 지형 (잔디, 긴 식물, 콘크리트, 자갈, 바위, 모래 등)</li> </ul> </li> </ul> <hr> <h3 id="9-rma의-주요-기여">9. RMA의 주요 기여</h3> <ul> <li>실시간 적응 모듈을 통한 환경 변화 대응</li> <li>데이터셋 의존 없이 즉각적인 정책 조정</li> <li>강건한 보행 정책의 다양한 환경 적용 가능성</li> </ul> <hr> <h2 id="ii-related-work">II Related Work</h2> <hr> <h3 id="1-전통적인-제어-기반-접근법-control-based-methods">1. 전통적인 제어 기반 접근법 (Control-Based Methods)</h3> <ul> <li> <strong>주요 로봇 사례</strong> <ul> <li>MIT Cheetah 3 <ul> <li>정규화된 <strong>모델 예측 제어 (MPC)</strong> 사용</li> <li>단순화된 동역학 활용</li> <li>고속 이동 및 장애물 점프 가능</li> </ul> </li> <li>ANYmal 로봇 <ul> <li>매개변수화된 제어기 최적화</li> <li>역진자 모델 기반 계획 수행</li> </ul> </li> </ul> </li> <li> <strong>한계점</strong> <ul> <li>정확한 실제 동역학 모델 요구</li> <li>로봇에 대한 사전 지식 필요</li> <li>보행 및 행동 수동 튜닝 요구</li> </ul> </li> <li> <strong>개선 방안</strong> <ul> <li>제어기 최적화 및 MPC 결합 <ul> <li>문제 일부 완화</li> <li>여전히 과제별 특징 엔지니어링 필요</li> </ul> </li> </ul> </li> </ul> <hr> <h3 id="2-학습-기반-보행-접근법-learning-for-legged-locomotion">2. 학습 기반 보행 접근법 (Learning for Legged Locomotion)</h3> <ul> <li> <strong>초기 시도</strong> <ul> <li>DARPA Learning Locomotion Program</li> </ul> </li> <li> <strong>최근 동향</strong> <ul> <li>심층 강화 학습(RL)의 도입 <ul> <li>인간 전문성 의존도 감소</li> <li>시뮬레이션에서 우수한 결과 도출</li> </ul> </li> </ul> </li> <li> <strong>한계점</strong> <ul> <li>정책의 현실 세계 전이 어려움</li> </ul> </li> <li> <strong>해결책</strong> <ul> <li>현실 세계에서 직접 훈련 <ul> <li>단순한 환경에 한정됨</li> <li>복잡한 환경에서는 비효율적 및 안전 문제 발생</li> </ul> </li> </ul> </li> </ul> <hr> <h3 id="3-sim-to-real-강화-학습-sim-to-real-reinforcement-learning">3. Sim-to-Real 강화 학습 (Sim-to-Real Reinforcement Learning)</h3> <ul> <li> <strong>Domain Randomization</strong> <ul> <li>다양한 환경 매개변수 및 센서 노이즈 사용</li> <li>강건한 정책 학습 가능</li> <li>최적성(Optimality) 희생 → 과도하게 보수적인 정책 생성</li> </ul> </li> <li> <strong>Simulation Accuracy Improvement</strong> <ul> <li>모터 모델 개선 <ul> <li>Tan et al.: 선형 함수로 모터 데이터 피팅</li> <li>Hwangbo et al.: 신경망으로 액추에이터 모델 매개변수화</li> </ul> </li> </ul> </li> <li> <strong>Limitations</strong> <ul> <li>초기 데이터 수집 필요</li> <li>새로운 환경마다 재조정 필요</li> </ul> </li> </ul> <hr> <h3 id="4-시스템-식별-및-적응-system-identification-and-adaptation">4. 시스템 식별 및 적응 (System Identification and Adaptation)</h3> <ul> <li> <strong>Online System Identification</strong> <ul> <li>시뮬레이션에서 훈련된 모듈을 통해 매개변수 추정</li> <li>진화 알고리즘을 사용한 직접 최적화</li> </ul> </li> <li> <strong>Latent Embedding 활용</strong> <ul> <li>저차원 잠재 임베딩을 통해 시스템 매개변수 표현</li> <li>실제 환경 롤아웃 기반 최적화 <ul> <li>정책 그래디언트 방법 사용</li> <li>베이지안 최적화 적용</li> <li>무작위 탐색 활용</li> </ul> </li> </ul> </li> <li> <strong>Meta-Learning</strong> <ul> <li>빠른 온라인 적응을 위한 정책 네트워크 초기화 학습</li> </ul> </li> <li> <strong>Limitations</strong> <ul> <li>여러 실제 환경 롤아웃 필요</li> <li>현실 세계에서의 최적화 비용 및 시간 문제</li> </ul> </li> </ul> <hr> <h3 id="5-요약-및-비교">5. 요약 및 비교</h3> <ul> <li> <strong>전통적 제어 접근법:</strong> 정확한 모델과 수작업 튜닝 요구</li> <li> <strong>학습 기반 접근법:</strong> 시뮬레이션 성능 우수, 현실 전이 어려움</li> <li> <strong>Sim-to-Real 접근법:</strong> 도메인 무작위화와 시뮬레이션 정확도 향상</li> <li> <strong>시스템 식별 및 적응:</strong> 저차원 잠재 임베딩과 메타 학습 활용</li> </ul> <p><strong>결론:</strong> RMA는 기존 접근법의 한계를 극복하며 다양한 환경에서 강건한 보행 정책을 실시간으로 적응 가능하게 함.</p> <hr> <h2 id="iii-rapid-motor-adaptation">III Rapid Motor Adaptation</h2> <hr> <h3 id="1-base-policy">1. Base Policy</h3> <hr> <h4 id="11-base-policy">1.1 Base Policy</h4> \[a_t = \pi(x_t, a_{t-1}, z_t)\] <ul> <li> <strong>Input</strong> <ul> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> <li>extrinsics vector: \(z_t \in \mathbb{R}^8\) <ul> <li>form: \(z_t=\mu(e_t)\)</li> <li>environment vector \(e_t \in \mathbb{R}^{17}\)</li> <li>environment factor endoder: \(\mu\)</li> </ul> </li> </ul> </li> <li> <strong>Ouput</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> <li>\(a_t\)는 12개의 로봇 관절이 원하는 위치로 설정된 값으로, PD 제어기를 사용하여 토크로 변환</li> </ul> </li> </ul> <hr> <h4 id="12-정책-학습">1.2 정책 학습</h4> <ul> <li> <strong>네트워크 구조:</strong> <ul> <li>정책 네트워크(\(\pi\)) 및 인코더(\(\mu\)): 다층 퍼셉트론(MLP)으로 구현</li> </ul> </li> <li> <strong>End-to-End 훈련:</strong> Model-free RL</li> <li> <strong>목표:</strong> 정책의 기대 반환(\(J(\pi)\)) 최대화</li> </ul> \[J(\pi) = \mathbb{E}_{\tau \sim p(\tau | \pi)} \left[\sum_{t=0}^{T-1} \gamma^t r_t \right]\] <ul> <li> <strong>경로(\(\tau\)):</strong> 정책(\(\pi\))을 따르는 에이전트의 상태, 행동, 보상 시퀀스</li> </ul> <hr> <h4 id="13-자연적-제약을-통한-안정적인-보행-stable-gait-through-natural-constraints">1.3 자연적 제약을 통한 안정적인 보행 (Stable Gait through Natural Constraints)</h4> <ul> <li> <strong>자연적 제약 활용:</strong> <ul> <li> <strong>생체에너지학 기반 보상 함수:</strong> 작업 최소화, 지면 충격 감소 목표</li> <li> <strong>고르지 않은 지형에서 훈련:</strong> 추가 보상 없이 보행 강건성 확보</li> </ul> </li> <li> <strong>학습 환경:</strong> <ul> <li>인공적인 시뮬레이션 노이즈 대신 자연적 제약 적용</li> </ul> </li> <li> <strong>정책 전이:</strong> <ul> <li>단순 환경(콘크리트, 나무 바닥)으로 자연스럽게 전이</li> <li>추가적인 미세 조정(finetuning) 불필요</li> </ul> </li> </ul> <hr> <h4 id="14-기존-sim-to-real-접근법과의-차이점">1.4 기존 Sim-to-Real 접근법과의 차이점</h4> <ul> <li> <strong>기존 접근법:</strong> <ul> <li>시뮬레이션과 현실 간 보정(calibration) 수행 [51, 23]</li> <li>현실 세계에서 정책 미세 조정(finetuning) 필요 [41]</li> </ul> </li> <li> <strong>RMA 접근법:</strong> <ul> <li>적응 모듈(Adaptation Module)을 통해 단순 환경에서 복잡한 지형으로 확장 가능</li> </ul> </li> </ul> <hr> <h4 id="15-강화-학습-보상-rl-rewards">1.5 강화 학습 보상 (RL Rewards)</h4> <p>보상 함수는 에이전트가 최대 0.35 m/s 속도로 전진하도록 장려 및 불규칙하고 비효율적인 움직임에 패널티 부여.</p> <ul> <li><strong>선형 속도: \(\mathbf{v}\)</strong></li> <li><strong>자세: \(\mathbf{\theta}\)</strong></li> <li><strong>각속도: \(\omega\)</strong></li> <li><strong>관절 각도: \(\mathbf{q}\)</strong></li> <li><strong>관절 속도: \(\dot{\mathbf{q}}\)</strong></li> <li><strong>관절 토크: \(\tau\)</strong></li> <li><strong>발의 지면 반력: \(\mathbf{f}\)</strong></li> <li><strong>발의 속도: \(\mathbf{v}_\mathbf{f}\)</strong></li> <li><strong>발 접촉 이진 벡터: \(\mathbf{g}\)</strong></li> </ul> <p>시간 \(t\)에서의 보상은 다음 요소들의 합으로 정의됩니다:</p> <ol> <li> <strong>전진:</strong> \(\operatorname{min}(v^t_x, 0.35)\)</li> <li> <strong>측면 이동 및 회전:</strong> \(- \| v_{y}^{t} \|^2 - \| \omega_{\text{yaw}}^{t} \|^2\)</li> <li> <strong>작업:</strong> \(- \| \boldsymbol{\tau}^{T} \cdot (\mathbf{q}^{t} - \mathbf{q}^{t-1}) \|\)</li> <li> <strong>지면 충격:</strong> \(- \| \mathbf{f}^{t} - \mathbf{f}^{t-1} \|^2\)</li> <li> <strong>부드러움:</strong> \(- \| \mathbf{\tau}^{t} - \mathbf{\tau}^{t-1} \|^2\)</li> <li> <strong>행동 크기:</strong> \(- \| \mathbf{a}^{t} \|^2\)</li> <li> <strong>관절 속도:</strong> \(- \| \dot{\mathbf{q}}^{t} \|^2\)</li> <li> <strong>자세 안정성:</strong> \(- \| \boldsymbol{\theta}^{t}_{\text{roll, pitch}} \|^2\)</li> <li> <strong>Z축 가속도:</strong> \(- \| v^t_z \|^2\)</li> <li> <strong>발 미끄러짐:</strong> \(- \| \operatorname{diag}(\mathbf{g}^{t}) \cdot \mathbf{v}_{\mathbf{f}}^{t} \|^2\)</li> </ol> <p>각 보상 항목의 스케일링 계수: 20, 21, 0.002, 0.02, 0.001, 0.07, 0.002, 1.5, 2.0, 0.8.</p> <hr> <h4 id="16-training-curriculum">1.6 Training Curriculum</h4> <p>위 보상 함수로 학습 시, 움직직에 대한 패널티로 인해 제자리에서 머무르는 현상 발생 가능. 이를 방지하기 위해 다음 전략 사용:</p> <ol> <li> <strong>패널티 계수:</strong> 초기에는 매우 작은 패널티 계수로 시작, 훈련이 진행됨에 따라 점진적으로 계수 증가.</li> <li> <strong>환경 변화:</strong> 질량, 마찰, 모터 힘 등의 난이도를 선형적으로 증가.</li> <li> <strong>지형 난이도:</strong> 지형에 대한 별도의 커리큘럼 없음, 고정된 난이도에서 무작위 지형 프로필 샘플링.</li> </ol> <hr> <h3 id="2-adaptation-module">2. Adaptation Module</h3> <hr> <h4 id="21-adaptation-module">2.1 Adaptation Module</h4> \[\hat{z_t} = \phi(x_{t-k:t-1}, a_{t-k:t-1})\] <ul> <li> <strong>역할</strong> <ul> <li>실제 환경에서 \(z_t\)를 온라인 추정</li> <li>privileged environment configuration, <strong>\(e_t\)</strong> 없이 작동</li> </ul> </li> <li> <strong>Input</strong> <ul> <li>history of robot’s states: \(x_{t-k:t-1}\)</li> <li>history of robot’s actions(\(a_{t-k:t-1}\))</li> </ul> </li> <li> <strong>Output</strong> <ul> <li>predicted extrinsics vector: \(\hat{z_t}\)</li> </ul> </li> <li> <strong>Hyperparameter</strong> <ul> <li>k=50 사용 (약 0.5초에 해당)</li> </ul> </li> </ul> <hr> <h4 id="22-모델-학습">2.2 모델 학습</h4> \[\operatorname{MSE}(\hat{z}_{t}, z_{t}) = \| \hat{z}_{t} - z_{t} \|^2\] <ul> <li> <strong>네트워크 구조</strong>: <ul> <li> <strong>1D CNN</strong> 사용: 시간적 상관관계 포착</li> </ul> </li> </ul> <hr> <h4 id="23-데이터-수집-방식">2.3 데이터 수집 방식</h4> <ul> <li> <strong>문제점</strong> <ul> <li>Base policy(\(\pi\))을 기반으로 한 데이터셋은 최적 경로만 포함</li> <li>실제 배포 시 발생할 편차를 충분히 다루지 못함</li> </ul> </li> <li> <strong>해결책: On-Policy 데이터 사용</strong> <ul> <li>무작위로 초기화된 <strong>Adaptation module(\(\phi\))</strong> 사용하여 데이터 수집</li> <li>상태-행동 이력과 목표 잠재 벡터(\(z_t\)) 쌍으로 학습 진행</li> </ul> </li> <li> <strong>훈련 절차</strong> <ul> <li>Base policy(\(\pi\))을 \(\hat{z_t}\)로 롤아웃</li> <li>state action history과 ground truth \(z_t\) 생성</li> <li>이 과정을 반복하여 수렴 유도</li> </ul> </li> <li> <strong>강건성 확보 메커니즘</strong> <ul> <li>랜덤 초기화된 <strong>Adaptation module</strong>(\(\phi\)) 사용</li> <li> <strong>imperfect prediction</strong>(\(\hat{z_t}\)) 수용</li> <li>훈련 중 충분한 탐색 경로 확보</li> </ul> </li> </ul> <hr> <h3 id="3-asynchronous-deployment">3. Asynchronous Deployment</h3> <hr> <h4 id="31-비동기식-배포">3.1 비동기식 배포</h4> <ul> <li> <strong>훈련 및 배포</strong> <ul> <li>완전한 시뮬레이션 기반 훈련</li> <li>현실 세계로 직접 배포 (수정 및 미세 조정 불필요)</li> </ul> </li> <li> <strong>비동기식 실행 구조</strong> <ul> <li>두 하위 시스템의 비동기적 실행</li> <li>서로 다른 주기로 작동 → 온보드 컴퓨팅 부담 최소화</li> </ul> </li> </ul> <hr> <h4 id="32-adaptation-module">3.2 Adaptation Module</h4> <ul> <li> <strong>작동 주기:</strong> 10Hz</li> <li> <strong>입력:</strong> 최근 50 타임 스텝의 상태 및 행동 이력</li> <li> <strong>출력:</strong> Extrinsics Vector, \(\hat{z_t}\)</li> <li> <strong>특징:</strong> 비교적 느리게 업데이트되나 성능에 영향 없음</li> </ul> <hr> <h4 id="33-base-policy">3.3 Base Policy</h4> <ul> <li> <strong>작동 주기:</strong> 100Hz</li> <li> <strong>입력:</strong> <ul> <li>the most recent \(\hat{z_t}\) generated by the adaptation module</li> <li>current state: \(x_t \in \mathbb{R}^{30}\)</li> <li>previous action: \(a_{t-1} \in \mathbb{R}^{12}\)</li> </ul> </li> <li> <strong>Output</strong> <ul> <li>next action: \(a_{t} \in \mathbb{R}^{12}\)</li> </ul> </li> <li> <strong>특징:</strong> <ul> <li>빠른 주기로 작동</li> <li>적응 모듈의 비동기적 업데이트 수용 가능</li> </ul> </li> </ul> <hr> <h4 id="34-단일-정책-설계의-한계점">3.4 단일 정책 설계의 한계점</h4> <ul> <li> <strong>상태 및 행동 이력을 직접 입력받는 단일 정책 접근법의 문제점</strong> <ul> <li><strong>비자연스러운 보행 패턴 발생</strong></li> <li><strong>시뮬레이션 성능 저하</strong></li> <li> <strong>온보드 컴퓨팅 한계:</strong> 10Hz에서만 작동 가능</li> <li> <strong>비동기적 설계 불가능:</strong> 하위 시스템 간 동기화 및 보정 필요</li> </ul> </li> </ul> <hr> <h3 id="35-비동기-설계의-장점">3.5 비동기 설계의 장점</h3> <ul> <li>상대적으로 느리게 변하는 Extrinsics Vector(\(\hat{z_t}\))와 빠르게 변하는 로봇 상태(\(x_t\))의 분리</li> <li>효율적인 온보드 컴퓨팅 자원 활용 가능</li> <li>동기화 및 추가 보정 불필요 → 원활한 Real-world 배포 가능</li> </ul> <hr> <h2 id="iv-experimental-setup">IV Experimental Setup</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table1-480.webp 480w,/assets/img/rma/table1-800.webp 800w,/assets/img/rma/table1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/table1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <hr> <h3 id="1-environment-details">1. Environment Details</h3> <hr> <h4 id="11-hardware-details">1.1 Hardware Details</h4> <ul> <li> <strong>로봇 플랫폼:</strong> Unitree의 A1 로봇</li> <li> <strong>특징:</strong> <ul> <li>중형 크기, 저비용 사족 보행 로봇</li> <li> <strong>자유도:</strong> 총 18자유도 (12자유도는 액추에이터 작동, 다리당 3개 모터)</li> <li> <strong>무게:</strong> 약 12kg</li> </ul> </li> <li> <strong>센서 입력:</strong> <ul> <li> <strong>모터 인코더:</strong> 관절 위치 및 속도 측정</li> <li> <strong>IMU 센서:</strong> 롤(Roll), 피치(Pitch) 측정</li> <li> <strong>발 센서:</strong> 이진화된 발 접촉 상태</li> </ul> </li> <li> <strong>제어 방식:</strong> <ul> <li><strong>관절 위치 제어 사용</strong></li> <li> <strong>PD 컨트롤러로 토크 변환</strong> <ul> <li>Gain: \(K_p = 55, K_d = 0.8\)</li> </ul> </li> </ul> </li> </ul> <hr> <h4 id="12-simulation-setup">1.2 Simulation Setup</h4> <ul> <li> <strong>시뮬레이터:</strong> RaiSim 사용</li> <li> <strong>로봇 모델:</strong> Unitree A1 URDF 파일 사용</li> <li> <strong>지형 생성기:</strong> 프랙탈 지형 생성기 <ul> <li>fractal octaves: 2</li> <li>fractal lacunarity: 2.0</li> <li>fractal gain: 0.25</li> <li>Z-scale: 0.27</li> </ul> </li> <li> <strong>에피소드 길이:</strong> 최대 1000 steps <ul> <li>조기 종료 조건: <ul> <li>높이 &lt; 0.28m</li> <li>롤 각도 &gt; 0.4 radians</li> <li>피치 각도 &gt; 0.2 radians</li> </ul> </li> </ul> </li> <li> <strong>제어 주파수:</strong> 100 Hz</li> <li> <strong>시뮬레이션 시간 간격:</strong> 0.025s = 1/40</li> </ul> <hr> <h4 id="13-state-action-space">1.3 State-Action Space</h4> <ul> <li> <strong>상태 벡터(</strong>\(x_t \in \mathbb{R}^{30}\)<strong>)</strong> <ul> <li> <strong>관절 위치:</strong> 12개 값</li> <li> <strong>관절 속도:</strong> 12개 값</li> <li> <strong>몸체 롤 및 피치:</strong> 2개 값</li> <li> <strong>발 접촉 상태:</strong> 4개 값</li> </ul> </li> <li> <strong>행동 벡터(</strong>\(a \in \mathbb{R}^{12}\)<strong>)</strong> <ul> <li> <strong>관절 목표 위치 예측:</strong> \(a = \hat{q} \in \mathbb{R}^{12}\)</li> <li> <strong>토크 변환:</strong> PD 컨트롤러 사용 <ul> <li>수식: \(\tau = K_p (\hat{q} - q) + K_d (\hat{\dot{q}} - \dot{q})\)</li> </ul> </li> <li> <strong>이득 값:</strong> \(K_p\) 및 \(K_d\) (수동 설정)</li> <li> <strong>목표 관절 속도:</strong> \(\hat{\dot{q}} = 0\)</li> </ul> </li> </ul> <hr> <h4 id="14-environmental-variations">1.4 Environmental Variations</h4> <ul> <li> <strong>환경 벡터(\(e_t \in \mathbb{R}^{17}\))</strong> <ul> <li> <strong>질량 및 로봇 내 위치:</strong> 3차원</li> <li> <strong>모터 강도:</strong> 12차원</li> <li> <strong>마찰 계수:</strong> 스칼라 값</li> <li> <strong>지형 높이:</strong> 스칼라 값</li> </ul> </li> <li> <strong>지형 높이 측정 방법:</strong> <ul> <li>각 발 아래 지형 높이 값을 소수점 첫째 자리로 이산화</li> <li>네 발 중 최대값 사용</li> </ul> </li> <li> <strong>지형 프로파일 특성:</strong> <ul> <li>고정된 난이도</li> <li>로컬 지형 높이의 동적 변화</li> </ul> </li> </ul> <hr> <h3 id="2-training-details">2. Training Details</h3> <hr> <h4 id="21-base-policy-and-environment-factor-encoder-architecture">2.1 Base Policy and Environment Factor Encoder Architecture</h4> <ul> <li> <strong>Base Policy</strong> <ul> <li> <strong>구조:</strong> 3-layer MLP</li> <li> <strong>Hidden layer sizes:</strong> 128</li> </ul> </li> <li> <strong>Environment Factor Encoder</strong> <ul> <li> <strong>구조:</strong> 3층 다층 퍼셉트론(MLP)</li> <li> <strong>Hidden layer sizes:</strong> 256, 128</li> </ul> </li> </ul> <hr> <h4 id="22-adaptation-module-architecture">2.2 Adaptation Module Architecture</h4> <ul> <li> <strong>1단계: 상태 및 행동 임베딩</strong> <ul> <li> <strong>구조:</strong> 2-layer MLP</li> </ul> </li> <li> <strong>2단계: 시간적 상관관계 학습</strong> <ul> <li> <strong>구조:</strong> 3-layer 1D CNN</li> <li> <strong>입력-출력 특성:</strong> <ul> <li> <strong>1층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 8, 스트라이드: 4</li> <li> <strong>2층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1</li> <li> <strong>3층:</strong> 입력 채널: 32, 출력 채널: 32, 커널 크기: 5, 스트라이드: 1</li> </ul> </li> </ul> </li> <li> <strong>3단계: 최종 예측</strong> <ul> <li> <strong>구조</strong>: Linear projection</li> </ul> </li> </ul> <hr> <h2 id="v-results-and-analysis">V Results and Analysis</h2> <p><strong>비교 대상:</strong></p> <ul> <li> <strong>시뮬레이션:</strong> 여러 기준 모델(Table II)</li> <li> <strong>현실 환경:</strong> 제조사 제공 A1 컨트롤러(Figure 3)</li> <li> <strong>다양한 야외 지형:</strong> 다양한 환경에서 RMA 테스트(Figure 1)</li> </ul> <hr> <h3 id="1-baselines">1. Baselines</h3> <ol> <li> <strong>A1 컨트롤러 (A1 Controller)</strong> <ul> <li>제조사 기본 컨트롤러</li> <li>힘 기반 제어 및 모델 예측 제어(MPC) 사용</li> </ul> </li> <li> <strong>Robustness through Domain Randomization</strong> <ul> <li>Extrinsics vector(\(z_t\)) 없이 학습된 기본 정책</li> <li>훈련 범위 내 변화를 견디도록 설계</li> </ul> </li> <li> <strong>Expert Adaptation Policy</strong> <ul> <li>시뮬레이션에서 true extrinsics vector(\(z_t\)) 사용</li> <li>RMA의 이론적 상한 성능 제공</li> </ul> </li> <li> <strong>RMA w/o Adaptation</strong> <ul> <li>Adaptation module 없이 base policy 단독 평가</li> <li>Adaptation module의 중요성 분석</li> </ul> </li> <li> <strong>System Identification</strong> <ul> <li>Extrinsics vector(\(\hat{z_t}\)) 대신 system parameters \(\hat{e_t}\) 직접 예측</li> </ul> </li> <li> <strong>AWR (Advantage Weighted Regression for Domain Adaptation)</strong> <ul> <li>오프라인으로 Extrinsics vector(\(\hat{z_t}\)) 최적화</li> <li>실제 환경 롤아웃을 기반으로 테스트 환경에 적응</li> </ul> </li> </ol> <p><strong>학습 조건 통일:</strong></p> <ul> <li>동일한 네트워크 아키텍처 사용</li> <li>동일한 보상 함수 사용</li> <li>동일한 하이퍼파라미터 적용</li> </ul> <hr> <h3 id="2-metrics">2. Metrics</h3> <ol> <li> <strong>Time-to-Fall (TTF):</strong> <ul> <li>최대 에피소드 길이로 나눈 <strong>시간당 추락 비율</strong> </li> <li>0~1 범위 정규화</li> </ul> </li> <li><strong>평균 전진 보상 (Average Forward Reward):</strong></li> <li><strong>성공률 (Success Rate):</strong></li> <li><strong>이동 거리 (Distance Covered):</strong></li> <li><strong>적응에 필요한 탐색 샘플 수 (Exploration Samples Needed for Adaptation):</strong></li> <li><strong>토크 사용량 (Torque Applied):</strong></li> <li> <strong>부드러움 (Smoothness):</strong> 토크 미분 값</li> <li> <strong>지면 충격 (Ground Impact):</strong> 발이 지면에 미치는 충격 수준</li> </ol> <hr> <h3 id="3-indoor-experiments">3. Indoor Experiments</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig3-480.webp 480w,/assets/img/rma/fig3-800.webp 800w,/assets/img/rma/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>비교 대상:</strong> <ul> <li><strong>RMA (Rapid Motor Adaptation)</strong></li> <li><strong>A1 기본 컨트롤러</strong></li> <li><strong>적응 모듈 제거된 RMA (RMA w/o Adaptation)</strong></li> </ul> </li> <li> <strong>비교 목적:</strong> 로봇 하드웨어 손상 최소화</li> <li> <strong>실험 조건:</strong> <ul> <li>각 방법당 5회 실험</li> <li>심각한 실패 발생 시 2회만 진행 후 실패로 기록</li> </ul> </li> <li> <strong>평가 지표:</strong> <ul> <li>성공률 (Success Rate)</li> <li>낙하 시간 (TTF: Time-to-Fall)</li> <li>이동 거리 (Distance Covered)</li> </ul> </li> </ul> <hr> <h4 id="31-setups">3.1 Setups</h4> <ol> <li> <strong>n-kg 페이로드 (n-kg Payload)</strong> <ul> <li> <strong>목표:</strong> n-kg 하중을 싣고 300cm 걷기</li> </ul> </li> <li> <strong>StepUp-n</strong> <ul> <li> <strong>목표:</strong> n-cm 높이의 계단 오르기</li> <li> <strong>평가 지표:</strong> 성공률만 기록</li> </ul> </li> <li> <strong>Uneven Foam</strong> <ul> <li> <strong>목표:</strong> 중앙이 솟아있는 스펀지 위 180cm 걷기</li> </ul> </li> <li> <strong>Mattress</strong> <ul> <li> <strong>목표:</strong> 메모리폼 매트리스 위 60cm 걷기</li> </ul> </li> <li> <strong>StepDown-n</strong> <ul> <li> <strong>목표:</strong> n-cm 높이의 계단 내려오기</li> <li> <strong>평가 지표:</strong> 성공률만 기록</li> </ul> </li> <li> <strong>Incline</strong> <ul> <li> <strong>목표:</strong> 6도 경사로 오르기</li> </ul> </li> <li> <strong>Oily Surface</strong> <ul> <li> <strong>목표:</strong> 미끄러운 오일이 뿌려진 표면 건너기</li> </ul> </li> </ol> <hr> <h4 id="32-results">3.2 Results</h4> <ul> <li> <strong>RMA의 성능:</strong> <ul> <li><strong>모든 환경에서 높은 성공률 달성</strong></li> <li><strong>A1 컨트롤러 대비 월등한 성능 발휘</strong></li> </ul> </li> <li> <strong>적응 모듈의 중요성:</strong> <ul> <li>적응 모듈 비활성화 시 성능 크게 저하</li> <li>많은 과제에서 문제 해결 불가</li> </ul> </li> <li> <strong>A1 컨트롤러의 한계:</strong> <ul> <li> <strong>Uneven Foam:</strong> 불안정한 지지대에서 불안정함 발생</li> <li> <strong>StepUp/StepDown:</strong> 높은 단차에서 실패 빈번</li> <li> <strong>Payload:</strong> 5kg 이상의 하중에서는 처짐 발생 및 낙하</li> </ul> </li> <li> <strong>RMA의 강점:</strong> <ul> <li>최대 12kg (로봇 체중의 100%) 하중 운반 성공</li> <li>높이를 유지하며 안정적 보행 가능</li> </ul> </li> <li> <strong>RMA w/o 적응 모듈:</strong> <ul> <li>대부분 낙하하지 않음</li> <li>전진 움직임은 거의 없음</li> </ul> </li> <li> <strong>Oily Surface</strong> <ul> <li> <strong>RMA:</strong> 성공적으로 미끄러운 지형 통과</li> <li> <strong>RMA w/o Adaptation:</strong> 나무 바닥에서는 별도 미세 조정 없이도 성공적 보행 가능</li> </ul> </li> </ul> <hr> <h3 id="4-outdoor-experiments">4. Outdoor Experiments</h3> <ul> <li> <strong>목표:</strong> RMA의 성능을 다양한 야외 환경에서 검증</li> <li> <strong>테스트 환경:</strong> <ul> <li>모래 (Sand)</li> <li>진흙 (Mud)</li> <li>흙길 (Dirt)</li> <li>높은 식물 지대 (Tall Vegetation)</li> <li>덤불 (Bush)</li> <li>계단 (Stairs)</li> <li>건설 폐기물 (Construction Debris)</li> </ul> </li> </ul> <hr> <h4 id="41-results">4.1 Results</h4> <ol> <li> <strong>모래, 진흙, 흙길 (Sand, Mud, Dirt)</strong> <ul> <li> <strong>성공률:</strong> 100%</li> <li> <strong>도전 과제:</strong> <ul> <li>발이 빠지거나 달라붙는 문제 발생</li> <li>동적 발판 조정 필요</li> </ul> </li> </ul> </li> <li> <strong>높은 식물 지대 및 덤불 (Tall Vegetation, Bush)</strong> <ul> <li> <strong>성공률:</strong> 100%</li> <li> <strong>도전 과제:</strong> <ul> <li>발이 장애물에 얽혀 불안정 발생</li> <li>주기적 발판 불안정성 해결 필요</li> <li>장애물에 맞서 강력한 추진력 필요</li> </ul> </li> </ul> </li> <li> <strong>하이킹 계단 (Stairs on Hiking Trail)</strong> <ul> <li> <strong>성공률:</strong> 70%</li> <li> <strong>도전 과제:</strong> <ul> <li>훈련 중 계단을 경험하지 못함</li> <li>불규칙한 발판과 높낮이 조정 필요</li> </ul> </li> </ul> </li> <li> <strong>건설 폐기물 (Construction Debris)</strong> <ul> <li> <strong>하위 실험:</strong> <ul> <li>진흙 더미 (Mud Pile): 성공률 100%</li> <li>시멘트 더미 (Cement Pile): 성공률 80%</li> <li>자갈 더미 (Pebble Pile): 성공률 80%</li> </ul> </li> <li> <strong>도전 과제:</strong> <ul> <li>급경사 및 측면 경사면</li> <li>불규칙한 발판과 무게 균형 필요</li> </ul> </li> </ul> </li> </ol> <hr> <h3 id="5-simulation-results">5. Simulation Results</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/table2-480.webp 480w,/assets/img/rma/table2-800.webp 800w,/assets/img/rma/table2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/table2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>비교 대상:</strong> 여러 기준 방법 (Table II)</li> <li> <strong>훈련 및 테스트 설정:</strong> <ul> <li> <strong>훈련 및 테스트 매개변수:</strong> Table I에 따라 샘플링</li> <li> <strong>재샘플링 확률:</strong> <ul> <li>훈련: 스텝당 0.004</li> <li>테스트: 스텝당 0.01</li> </ul> </li> </ul> </li> <li> <strong>평가 방법:</strong> <ul> <li> <strong>정책 초기화:</strong> 무작위로 3회 초기화된 정책 사용</li> <li> <strong>에피소드 수:</strong> 초기화당 1000회 에피소드 실행</li> </ul> </li> <li> <strong>성능 평가:</strong> 평균값 도출</li> </ul> <hr> <h4 id="51-results">5.1 Results</h4> <ol> <li> <strong>AWR (Advantage Weighted Regression)</strong> <ul> <li> <strong>적응 속도 저하:</strong> 변화하는 환경에 느린 적응</li> <li> <strong>성능 저하:</strong> 지속적인 환경 변화에 취약</li> </ul> </li> <li> <strong>Robust (도메인 무작위화 기반 강건성)</strong> <ul> <li> <strong>환경 특성 무시:</strong> Extrinsics vector(\(z_t\)) 사용하지 않음</li> <li> <strong>보수적 정책:</strong> 성능 저하 발생</li> </ul> </li> <li> <strong>System Identification (SysID)</strong> <ul> <li> <strong>환경 매개변수(\(e_t\)) 추정의 어려움:</strong> 명시적 매개변수 추정 어려움</li> <li> <strong>불필요성:</strong> 높은 성능 달성을 위해 필수적이지 않음</li> </ul> </li> <li> <strong>RMA w/o Adaptation (적응 모듈 미사용 RMA)</strong> <ul> <li> <strong>성능 급감:</strong> 적응 모듈 없이는 성능 저하 심각</li> </ul> </li> </ol> <hr> <h3 id="6-adaptation-analysis">6. Adaptation Analysis</h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rma/fig4-480.webp 480w,/assets/img/rma/fig4-800.webp 800w,/assets/img/rma/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rma/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>목표:</strong> 미끄러운 표면에서 RMA의 보행 패턴, 토크 프로파일, Extrinsics vector(\(\hat{z_t}\)) 분석</li> <li> <strong>실험 조건:</strong> <ul> <li> <strong>표면:</strong> 플라스틱 바닥에 오일 도포</li> <li> <strong>로봇 발:</strong> 플라스틱으로 덮음</li> </ul> </li> </ul> <hr> <h4 id="61-results">6.1 Results</h4> <ul> <li> <strong>성공률:</strong> 90%</li> <li> <strong>분석 항목:</strong> <ul> <li>무릎 토크 프로파일 (Torque Profile of Knee)</li> <li>보행 패턴 (Gait Pattern)</li> <li>Extrinsics vector(\(\hat{z_t}\)) 분석: 1번째 및 5번째 성분의 중위수 필터링된 값</li> </ul> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/batch_RL_vs_offline_RL/">Batch RL vs. Offline RL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/offline_RL_IL/">Offline RL vs. Offline IL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/imitation_learning/">Imitation Learning (모방 학습)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/copycat/">Copycat 문제</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/traj_planning/">Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/quantum_vl/">Quantum Virtual Link Generation via Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs-ttt/">UDC</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/udc/">A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs/">a post with tabs2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs/">a post with tabs</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Optreal . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-tutorials",title:"tutorials",description:"A growing collection of tutorials.",section:"Navigation",handler:()=>{window.location.href="/tutorials/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-batch-rl-vs-offline-rl",title:"Batch RL vs. Offline RL",description:"Batch RL vs. Offline RL",section:"Posts",handler:()=>{window.location.href="/blog/2025/batch_RL_vs_offline_RL/"}},{id:"post-offline-rl-vs-offline-il",title:"Offline RL vs. Offline IL",description:"Offline RL vs. Offline IL",section:"Posts",handler:()=>{window.location.href="/blog/2025/offline_RL_IL/"}},{id:"post-imitation-learning-\ubaa8\ubc29-\ud559\uc2b5",title:"Imitation Learning (\ubaa8\ubc29 \ud559\uc2b5)",description:"Imitation Learning (\ubaa8\ubc29 \ud559\uc2b5)",section:"Posts",handler:()=>{window.location.href="/blog/2025/imitation_learning/"}},{id:"post-copycat-\ubb38\uc81c",title:"Copycat \ubb38\uc81c",description:"Copycat \ubb38\uc81c",section:"Posts",handler:()=>{window.location.href="/blog/2025/copycat/"}},{id:"post-enhancing-safety-via-deep-reinforcement-learning-in-trajectory-planning-for-agile-flights-in-unknown-environments",title:"Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in...",description:"Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments",section:"Posts",handler:()=>{window.location.href="/blog/2025/traj_planning/"}},{id:"post-rma-rapid-motor-adaptation-for-legged-robots",title:"RMA: Rapid Motor Adaptation for Legged Robots",description:"RMA: Rapid Motor Adaptation for Legged Robots",section:"Posts",handler:()=>{window.location.href="/blog/2024/rma/"}},{id:"post-quantum-virtual-link-generation-via-reinforcement-learning",title:"Quantum Virtual Link Generation via Reinforcement Learning",description:"Quantum Virtual Link Generation via Reinforcement Learning",section:"Posts",handler:()=>{window.location.href="/blog/2024/quantum_vl/"}},{id:"post-udc",title:"UDC",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs-ttt/"}},{id:"post-a-unified-neural-divide-and-conquer-framework-for-large-scale-combinatorial-optimization-problems",title:"A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",description:"A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",section:"Posts",handler:()=>{window.location.href="/blog/2024/udc/"}},{id:"post-a-post-with-tabs2",title:"a post with tabs2",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"tutorials-torchrl-environment",title:"TorchRL Environment",description:"Pytorch TorchRL Environment",section:"Tutorials",handler:()=>{window.location.href="/tutorials/1_torchrl_env/"}},{id:"tutorials-torchrl-modules",title:"TorchRL Modules",description:"Pytorch TorchRL Modules",section:"Tutorials",handler:()=>{window.location.href="/tutorials/2_torchrl_module/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%73%74%65%72@%6F%70%74%72%65%61%6C.%69%6F","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>