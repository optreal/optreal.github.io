<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> TorchRL Modules | Optreal </title> <meta name="author" content="Optreal "> <meta name="description" content="Pytorch TorchRL Modules"> <meta name="keywords" content="optreal, jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://optreal.github.io/tutorials/2_torchrl_module/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Optreal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/tutorials/">tutorials <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">TorchRL Modules</h1> <p class="post-description">Pytorch TorchRL Modules</p> </header> <article> <h3 id="1-torchrl-modules-소개">1. TorchRL Modules 소개</h3> <ul> <li>TorchRL에서 바라보는 강화학습(Reinforcement Learning) <ul> <li>특정 과제를 효과적으로 해결할 수 있는 <code class="language-plaintext highlighter-rouge">정책(policy)</code>을 설계하는 것이 목표</li> <li>정책은 다양한 형태를 가질 수 있음: <ul> <li>관측 공간에서 행동 공간으로의 미분 가능한 매핑</li> <li>가능한 행동에 대해 계산된 <code class="language-plaintext highlighter-rouge">Value</code> 리스트에서 <code class="language-plaintext highlighter-rouge">argmax</code>를 선택하는 방식 등</li> </ul> </li> </ul> </li> <li>TorchRL에서 바라보는 정책의 특징 <ul> <li>결정적(deterministic) 또는 확률적(stochastic)</li> <li>RNN(Recurrent Neural Network), Transformer 등 복잡한 요소를 포함할 수 있음</li> <li>복잡한 시나리오 대응 - 다양한 정책 유형을 수용하는 것은 복잡할 수 있음 - TorchRL은 이러한 정책 설계를 간단히 처리할 수 있는 기능을 제공</li> </ul> </li> <li>이 튜토리얼의 초점 <ul> <li>정책 설계의 핵심 기능을 간략히 설명</li> <li>확률적(stochastic) 및 Q-Value 정책에 초점</li> <li>MLP 및 CNN을 백본(backbone)으로 사용하는 두 가지 일반적인 시나리오 소개</li> </ul> </li> </ul> <h3 id="2-tensordictmodules">2. <code class="language-plaintext highlighter-rouge">TensorDictModules</code> </h3> <ul> <li> <code class="language-plaintext highlighter-rouge">TensorDict</code>를 입력과 출력으로 사용하는 PyTorch 모듈을 정의하는 클래스. <ul> <li>표준 모듈(Module)이나 함수를 <code class="language-plaintext highlighter-rouge">TensorDictModules</code>로 감싸서, 필요한 항목을 읽고 모듈에 전달한 뒤 결과를 정해진 항목에 기록</li> </ul> </li> <li>환경이 <code class="language-plaintext highlighter-rouge">TensorDict</code>와 상호작용하는 방식처럼, 정책(<code class="language-plaintext highlighter-rouge">policy</code>)과 가치 함수(<code class="language-plaintext highlighter-rouge">value function</code>)를 나타내는 모듈도 동일하게 <code class="language-plaintext highlighter-rouge">TensorDict</code>와 상호작용할 수 있도록 만들기 위해 사용하는 모듈</li> <li>사용 예시 <ul> <li>예제 정책: 결정적 맵(deterministic map) <ul> <li>관측 공간에서 행동 공간으로의 결정론적 맵을 사용한 가장 간단한 정책</li> <li>범용성을 위해 <code class="language-plaintext highlighter-rouge">LazyLinear</code> 모듈을 활용</li> </ul> </li> <li>아래 예시만으로도 <code class="language-plaintext highlighter-rouge">정책(policy)</code>을 실행하는 데 필요한 모든 준비가 완료.</li> <li>Lazy 모듈을 사용하면 관측 공간(observation space)의 크기를 직접 가져올 필요가 없으며, 모듈이 이를 자동으로 결정</li> <li>이제 이 정책은 환경과 상호작용할 준비가 됨</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">tensordict.nn</span> <span class="kn">import</span> <span class="n">TensorDictModule</span>
<span class="kn">from</span> <span class="n">torchrl.envs</span> <span class="kn">import</span> <span class="n">GymEnv</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GymEnv</span><span class="p">(</span><span class="sh">"</span><span class="s">Pendulum-v1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LazyLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="nc">TensorDictModule</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">observation</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">action</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>실행 결과</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td> <td class="rouge-code"><pre>TensorDict(
    fields={
        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        next: TensorDict(
            fields={
                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)
            },
            batch_size=torch.Size([10]),
            device=None,
            is_shared=False
        ),
        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)
    },
    batch_size=torch.Size([10]),
    device=None,
    is_shared=False
)
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="3-특별한-tensordictmodules">3. 특별한 <code class="language-plaintext highlighter-rouge">TensorDictModules</code> </h3> <ul> <li>특별한 <code class="language-plaintext highlighter-rouge">TensorDictModules</code> <ul> <li> <code class="language-plaintext highlighter-rouge">Actor</code> <ul> <li> <code class="language-plaintext highlighter-rouge">Actor</code>는 <code class="language-plaintext highlighter-rouge">in_keys</code>와 <code class="language-plaintext highlighter-rouge">out_keys</code>에 대한 기본값을 제공하여, 많은 일반적인 환경과 간단히 통합 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">ProbabilisticActor</code></li> <li><code class="language-plaintext highlighter-rouge">ActorValueOperator</code></li> <li><code class="language-plaintext highlighter-rouge">ActorCriticOperator</code></li> </ul> </li> <li>사용 예시</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">tensordict.nn</span> <span class="kn">import</span> <span class="n">TensorDictModule</span>
<span class="kn">from</span> <span class="n">torchrl.envs</span> <span class="kn">import</span> <span class="n">GymEnv</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">GymEnv</span><span class="p">(</span><span class="sh">"</span><span class="s">Pendulum-v1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LazyLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="4-네트워크">4. 네트워크</h3> <ul> <li>TorchRL은 <code class="language-plaintext highlighter-rouge">TensorDict</code> 기능을 사용하지 않고도 사용할 수 있는 일반적인 모듈을 제공</li> <li>가장 흔히 사용되는 네트워크는 <code class="language-plaintext highlighter-rouge">MLP (Multi-Layer Perceptron)</code>와 <code class="language-plaintext highlighter-rouge">ConvNet (CNN)</code> 모듈</li> <li>사용 예시</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torchrl.modules</span> <span class="kn">import</span> <span class="n">MLP</span>

<span class="n">module</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span>
    <span class="n">out_features</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
    <span class="n">activation_class</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="5-확률적-정책-probabilistic-policies">5. 확률적 정책 (Probabilistic policies)</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">PPO(Policy-Optimization Algorithms)</code>와 같은 정책 최적화 알고리즘은 확률적 정책을 사용</li> <li>이 때, 정책 모듈은 관측 공간에서 가능한 행동에 대한 분포를 나타내는 파라미터 공간으로의 매핑이 필요</li> <li>TorchRL은 다음과 같은 작업들을 하나의 클래스에서 처리하여 이러한 모듈 설계를 간소화함 <ul> <li>파라미터에서 분포 생성</li> <li>해당 분포에서 샘플링</li> <li>로그 확률(log-probability) 계산 및 반환</li> </ul> </li> <li>사용 예시 <ul> <li>아래 예시는 <code class="language-plaintext highlighter-rouge">정규 분포(Normal Distribution)</code>를 사용하는 <code class="language-plaintext highlighter-rouge">액터(actor)</code>를 구축하는 과정 <ul> <li>즉, 확률적 정책 설계의 기본 원리를 보여줌</li> </ul> </li> <li>MLP 백본(backbone) <ul> <li>크기가 3인 관측값을 입력받아 크기가 2인 텐서 출력</li> </ul> </li> <li>NormalParamExtractor 모듈 <ul> <li>MLP의 출력을 두 개의 부분, 평균(mean)과 표준편차(standard deviation), 으로 나눔</li> <li>각각 크기는 1</li> <li>이 값은 <code class="language-plaintext highlighter-rouge">loc</code>와 <code class="language-plaintext highlighter-rouge">scale</code> 항목으로 반환</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">ProbabilisticActor</code> <ul> <li>평균과 표준편차를 <code class="language-plaintext highlighter-rouge">in_keys</code>로 받아 분포를 생성.</li> <li>샘플 및 로그 확률(log-probability)을 계산하고 <code class="language-plaintext highlighter-rouge">TensorDict</code>에 저장</li> </ul> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">tensordict.nn.distributions</span> <span class="kn">import</span> <span class="n">NormalParamExtractor</span>
<span class="kn">from</span> <span class="n">torch.distributions</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="n">torchrl.modules</span> <span class="kn">import</span> <span class="n">ProbabilisticActor</span>

<span class="n">backbone</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">extractor</span> <span class="o">=</span> <span class="nc">NormalParamExtractor</span><span class="p">()</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">extractor</span><span class="p">)</span>
<span class="n">td_module</span> <span class="o">=</span> <span class="nc">TensorDictModule</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span> 
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">observation</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">loc</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">scale</span><span class="sh">"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="nc">ProbabilisticActor</span><span class="p">(</span>
    <span class="n">td_module</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">loc</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">scale</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">action</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">distribution_class</span><span class="o">=</span><span class="n">Normal</span><span class="p">,</span>
    <span class="n">return_log_prob</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>실행 결과</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td> <td class="rouge-code"><pre>TensorDict(
    fields={
        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        loc: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        next: TensorDict(
            fields={
                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)
            },
            batch_size=torch.Size([10]),
            device=None,
            is_shared=False
        ),
        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)
    },
    batch_size=torch.Size([10]),
    device=None,
    is_shared=False
)
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>set_exploration_type() 함수 <ul> <li> <code class="language-plaintext highlighter-rouge">행동(action)</code>의 샘플링 방식을 제어하여, 랜덤 샘플 대신 분포의 기댓값(expected value)이나 다른 특성을 사용 가능</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torchrl.envs.utils</span> <span class="kn">import</span> <span class="n">ExplorationType</span><span class="p">,</span> <span class="n">set_exploration_type</span>

<span class="k">with</span> <span class="nf">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="p">.</span><span class="n">DETERMINISTIC</span><span class="p">):</span>
    <span class="c1"># takes the mean as action
</span>    <span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="p">.</span><span class="n">RANDOM</span><span class="p">):</span>
    <span class="c1"># Samples actions according to the dist
</span>    <span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="6-탐험-exploration">6. 탐험 (Exploration)</h3> <ul> <li>TorchRL은 <strong>탐험 모듈(exploration modules)</strong> 제공 <ul> <li>EGreedyModule</li> <li>AdditiveGaussianModule</li> <li>OrnsteinUhlenbeckProcessModule</li> </ul> </li> <li>사용 예시 <ul> <li>탐험 모듈이 어떻게 작동하는지 보기 위해, 다시 결정론적 정책(deterministic policy)으로 전환</li> <li>\(\epsilon\)-greedy 탐험 모듈 <ul> <li>annealing frames(탐험 감소 프레임 수)와 \(\epsilon\) 초기값으로 커스터마이즈</li> <li>\(\epsilon=1\)인 경우: 모든 행동이 랜덤으로 선택</li> <li>\(\epsilon=0\)인 경우: 탐험 없이 완전히 결정론적 행동만 수행</li> <li>탐험 요소(\(\epsilon\) 값)를 점진적으로 감소시키기 위해 <code class="language-plaintext highlighter-rouge">step()</code> 호출 필요</li> </ul> </li> <li>TensorDictSequential 모듈 사용 <ul> <li>탐험적 정책(explorative policy)을 구축하기 위해, 결정론적 정책 모듈과 탐험 모듈을 연결</li> <li> <code class="language-plaintext highlighter-rouge">TensorDictSequential</code>은 <code class="language-plaintext highlighter-rouge">TensorDict</code> 환경에서의 <code class="language-plaintext highlighter-rouge">Sequential</code>과 유사한 역할을 수행</li> </ul> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">tensordict.nn</span> <span class="kn">import</span> <span class="n">TensorDictSequential</span>
<span class="kn">from</span> <span class="n">torchrl.modules</span> <span class="kn">import</span> <span class="n">EGreedyModule</span>

<span class="n">policy</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">(</span><span class="nc">MLP</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]))</span>
<span class="n">exploration_module</span> <span class="o">=</span> <span class="nc">EGreedyModule</span><span class="p">(</span>
    <span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">,</span> <span class="n">annealing_num_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">eps_init</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>

<span class="n">exploration_policy</span> <span class="o">=</span> <span class="nc">TensorDictSequential</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">exploration_module</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="p">.</span><span class="n">DETERMINISTIC</span><span class="p">):</span>
    <span class="c1"># Turns off exploration
</span>    <span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">exploration_policy</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="p">.</span><span class="n">RANDOM</span><span class="p">):</span>
    <span class="c1"># Turns on exploration
</span>    <span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">exploration_policy</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="7-q-value-actors">7. <code class="language-plaintext highlighter-rouge">Q-Value Actors</code> </h3> <ul> <li>일부 강화 학습 알고리즘에서는 <code class="language-plaintext highlighter-rouge">정책(policy)</code>이 독립된 모듈이 아니라, 다른 모듈을 사용하여 간접적으로 구축됨</li> <li> <code class="language-plaintext highlighter-rouge">Q-Value actors</code>가 이에 해당</li> <li> <code class="language-plaintext highlighter-rouge">Q-Value actors</code>는 <code class="language-plaintext highlighter-rouge">행동(action)</code> 값에 대한 추정(대부분의 경우 이산(discrete) 값)을 요구하며, 가장 높은 값을 가진 행동을 탐욕적으로 선택</li> <li>특정 설정(유한 이산 행동 공간 및 유한 이산 상태 공간)에서는 상태-행동(state-action) 쌍을 2D 테이블에 저장</li> <li> <strong>DQN(Deep Q-Network)</strong>의 혁신은 신경망을 활용해 연속 상태 공간(continuous state spaces)에서도 Q(s, a) 값 맵을 인코딩할 수 있도록 확장</li> <li>사용 예시 <ul> <li>이산 행동 공간을 가진 환경만 활용 가능</li> <li>value_net: 환경으로 부터 상태를 받으면 각 개별 행동당 Q-value를 산출</li> <li>Q-Value actor: <code class="language-plaintext highlighter-rouge">QValueModule</code> 활용</li> <li>해당 <code class="language-plaintext highlighter-rouge">actor</code>을 적용한 rollout 결과에 다음 두 개의 값에 주목 <ul> <li><code class="language-plaintext highlighter-rouge">action_value</code></li> <li><code class="language-plaintext highlighter-rouge">chosen_action_value</code></li> </ul> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td> <td class="rouge-code"><pre><span class="n">env</span> <span class="o">=</span> <span class="nc">GymEnv</span><span class="p">(</span><span class="sh">"</span><span class="s">CartPole-v1</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>실행 결과</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="rouge-code"><pre>OneHot(
    shape=torch.Size([2]),
    space=CategoricalBox(n=2),
    device=cpu,
    dtype=torch.int64,
    domain=discrete
)
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>사용 예시</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torchrl.modules</span> <span class="kn">import</span> <span class="n">QValueModule</span>

<span class="n">num_actions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">value_net</span> <span class="o">=</span> <span class="nc">TensorDictModule</span><span class="p">(</span>
    <span class="nc">MLP</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">observation</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">action_value</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="nc">TensorDictSequential</span><span class="p">(</span>
    <span class="n">value_net</span><span class="p">,</span>  <span class="c1"># writes action values in our tensordict
</span>    <span class="nc">QValueModule</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">),</span>  <span class="c1"># Reads the "action_value" entry by default
</span><span class="p">)</span>
<span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>실행 결과</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td> <td class="rouge-code"><pre>TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),
        action_value: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),
        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        next: TensorDict(
            fields={
                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)
            },
            batch_size=torch.Size([3]),
            device=None,
            is_shared=False
        ),
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False
)
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>탐험 적용 사용 예시 <ul> <li> <code class="language-plaintext highlighter-rouge">EGreedyModule</code> 사용</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torchrl.modules</span> <span class="kn">import</span> <span class="n">QValueModule</span><span class="p">,</span> <span class="n">EGreedyModule</span>

<span class="n">num_actions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">value_net</span> <span class="o">=</span> <span class="nc">TensorDictModule</span><span class="p">(</span>
    <span class="nc">MLP</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">observation</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">action_value</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">policy_explore</span> <span class="o">=</span> <span class="nc">TensorDictSequential</span><span class="p">(</span>
    <span class="n">value_net</span><span class="p">,</span>  <span class="c1"># writes action values in our tensordict
</span>    <span class="nc">QValueModule</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">),</span>  <span class="c1"># Reads the "action_value" entry by default
</span>    <span class="nc">EGreedyModule</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_spec</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">with</span> <span class="nf">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="p">.</span><span class="n">RANDOM</span><span class="p">):</span>
    <span class="n">rollout_explore</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">rollout</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy_explore</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>실행 결과</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td> <td class="rouge-code"><pre>TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),
        action_value: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),
        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        next: TensorDict(
            fields={
                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)
            },
            batch_size=torch.Size([3]),
            device=None,
            is_shared=False
        ),
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)
    },
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False
)
</pre></td> </tr></tbody></table></code></pre></div></div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Optreal . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-tutorials",title:"tutorials",description:"A growing collection of tutorials.",section:"Navigation",handler:()=>{window.location.href="/tutorials/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-enhancing-safety-via-deep-reinforcement-learning-in-trajectory-planning-for-agile-flights-in-unknown-environments",title:"Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in...",description:"Enhancing Safety via Deep Reinforcement Learning in Trajectory Planning for Agile Flights in Unknown Environments",section:"Posts",handler:()=>{window.location.href="/blog/2025/traj_planning/"}},{id:"post-rma-rapid-motor-adaptation-for-legged-robots",title:"RMA: Rapid Motor Adaptation for Legged Robots",description:"RMA: Rapid Motor Adaptation for Legged Robots",section:"Posts",handler:()=>{window.location.href="/blog/2024/rma/"}},{id:"post-quantum-virtual-link-generation-via-reinforcement-learning",title:"Quantum Virtual Link Generation via Reinforcement Learning",description:"Quantum Virtual Link Generation via Reinforcement Learning",section:"Posts",handler:()=>{window.location.href="/blog/2024/quantum_vl/"}},{id:"post-udc",title:"UDC",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs-ttt/"}},{id:"post-a-unified-neural-divide-and-conquer-framework-for-large-scale-combinatorial-optimization-problems",title:"A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",description:"A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",section:"Posts",handler:()=>{window.location.href="/blog/2024/udc/"}},{id:"post-a-post-with-tabs2",title:"a post with tabs2",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"tutorials-torchrl-environment",title:"TorchRL Environment",description:"Pytorch TorchRL Environment",section:"Tutorials",handler:()=>{window.location.href="/tutorials/1_torchrl_env/"}},{id:"tutorials-torchrl-modules",title:"TorchRL Modules",description:"Pytorch TorchRL Modules",section:"Tutorials",handler:()=>{window.location.href="/tutorials/2_torchrl_module/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%73%74%65%72@%6F%70%74%72%65%61%6C.%69%6F","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>